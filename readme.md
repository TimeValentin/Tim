# Учебное пособие: Настройка сетевого стенда по ДЭ ССА за 2025 год

**Введение:**
Это учебное пособие представляет собой **детальное решение образца задания Демонстрационного Экзамена по компетенции 'Сетевое и системное администрирование' для специальности 09.02.06 за 2025 год**, фокусируясь на **Модуле 1: Настройка сетевой инфраструктуры** и на **Модуле 2: Организация сетевого администрирования операционных систем**. На выполнение **Модуля 1** отводится **1 час**, на **Модуль 2** — **1 час 30 минут**.

Пособие служит пошаговым руководством для студентов и начинающих ИТ-специалистов по настройке сетевой инфраструктуры тестового стенда, описанного в топологии (**Рисунок 1**) и таблицах адресации. Оно охватывает базовую настройку устройств, включая присвоение имён, IP-адресацию, настройку VLAN, GRE-туннелей, динамической маршрутизации (OSPF), NAT, DHCP, DNS и безопасного удалённого доступа (SSH), а также настройку доменного контроллера, файлового хранилища, службы времени, Ansible и развёртывание веб-приложения в Docker.

---

**Важное замечание о вариативности заданий!**

Демонстрационный экзамен может проводиться по **нескольким вариантам заданий**. Это означает, что **конкретные значения некоторых параметров**, таких как:
*   **VLAN ID**.
*   Размеры IP-подсетей (маски).
*   Идентификаторы пользователей (UID).
*   Порты служб (SSH и др.).
*   Уровни RAID.
*   Пароли (хотя в данном пособии они зафиксированы, теоретически могут меняться).
*   Детали конфигурации DNS или Reverse Proxy.
**могут отличаться** от тех, что приведены в данном учебном пособии.

**Основное правило:** **всегда внимательно читайте ваш вариант задания на экзамене и используйте указанные в нём значения!** В данном пособии вариативные моменты будут отмечены комментариями вида `// ВАРИАТИВНО: ...`.

      
**Критическое исключение: Настройка VLAN ID для HQ-SRV и HQ-CLI, и вариативность для сети Управления**

Из-за особенностей преднастроенного **экзаменационного стенда** (виртуальных коммутаторов), которые могут быть **жёстко настроены на определённые VLAN ID для сетей HQ-SRV и HQ-CLI**, существует **важное исключение** из основного правила:

*   **Для сетей HQ-SRV и HQ-CLI:** **Независимо от того, какие VLAN ID указаны в вашем варианте задания для этих двух сетей**, при настройке виртуального коммутатора на `HQ-RTR` (Шаг 4 Модуля 1) **всегда используйте следующие фиксированные ID из-за особенностей преднастроенного экзаменационного стенда:**
    *   `VLAN ID 100` для сети HQ-SRV.
    *   `VLAN ID 200` для сети HQ-CLI.
    *   **Игнорируйте** требования к VLAN ID в вашем задании для сетей **HQ-SRV и HQ-CLI**, если они отличаются от 100 и 200. Использование других VLAN ID для этих двух сетей приведёт к **неработоспособности сети** на экзаменационном стенде.
*   **Для сети Управления (Management):**
    *   **Всегда используйте VLAN ID, указанный в вашем варианте задания для сети Управления.**
    *   Если в вашем варианте задания VLAN ID для сети Управления не указан или вы не уверены, можно использовать `VLAN ID 999` как пример, приведённый в данном пособии. Это значение **является вариативным** и должно соответствовать вашему заданию.
*   Данное исключение (использование фиксированных ID 100 и 200) касается **ТОЛЬКО VLAN ID для сетей HQ-SRV и HQ-CLI**. По всем остальным параметрам (IP-адреса, маски, UID, порты, **VLAN ID для сети Управления** и т.д.) вы **должны** следовать требованиям **вашего варианта задания**.

---

**Требования к отчётам:**
*   **Модуль 1:** По итогам выполнения необходимо предоставить **один итоговый файл отчёта**. Согласно заданию, этот файл должен включать **одну таблицу** (таблица адресации из Шага 1) и **четыре отчёта о ходе работы**. Анализ задания показывает, что **таблица адресации считается одним из этих пяти отчётов**. Остальные четыре отчёта требуются по шагам: **Шаг 4 (VLAN), Шаг 6 (GRE-туннель), Шаг 7 (OSPF), Шаг 9 (DHCP)**.
*   **Модуль 2:** По итогам выполнения необходимо предоставить **пять отчётов** (можно отдельными файлами или разделами одного файла). Явно отчёты требуются по шагам: **Шаг 2 (NFS/RAID), Шаг 7 (Moodle), Шаг 9 (Браузер)**. Для выполнения требования о пяти отчётах, наиболее вероятно, также требуются отчёты по наиболее значимым шагам: **Шаг 1 (Samba DC) и Шаг 4 (Ansible)**.
*   **В данном пособии:** Примеры отчётов приведены для всех указанных выше шагов. Ищите маркер **`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`** для шагов, требующих отчётности. Обратите внимание, что пример отчёта для шага 8 Модуля 2 предоставлен для полноты картины, но может не входить в обязательные пять отчётов на экзамене.

      
**Рекомендуемый порядок выполнения заданий Модуля 1 (стратегия экзамена):**
Для максимально эффективного использования времени на экзамене, учитывая необходимость доступа в Интернет для установки пакетов и зависимости между шагами, рекомендуется следующий порядок **практических действий**:
1.  Выполнить **Шаг 1:** Базовая настройка устройств (Имена, IP). *Примечание: В рамках Шага 1.2 также выполняется временная настройка DNS-клиентов на `HQ-RTR`, `BR-RTR` и `HQ-SRV` для обеспечения возможности скачивания пакетов.*
2.  Выполнить **Шаг 2:** Настройка ISP (Интернет-канал).
3.  Выполнить **Шаг 8:** Настройка NAT на HQ-RTR/BR-RTR и MSS Clamping (Обеспечение доступа машин в Интернет!).
4.  Выполнить **Шаг 3:** Создание локальных учётных записей (Быстрые баллы).
5.  Выполнить **Шаг 5:** Настройка безопасного удалённого доступа (SSH) (Быстрые баллы).
6.  Выполнить **Шаг 4:** Настройка VLAN на `HQ-RTR` (Настройка LAN). **Помните про исключение для VLAN ID!**
7.  **(Параллельно!) Запустить процесс установки пакетов:** Начать установку пакетов, требующих Интернет, **на соответствующих машинах в фоновом режиме** (добавив `&` в конце команды `apt-get install`) и **немедленно перейти** к следующему пункту, пока они скачиваются:
    *   На **ISP**: `apt-get update && apt-get install -y tzdata &`
    *   На **HQ-RTR**: `apt-get update && apt-get install -y frr dnsmasq &`
    *   На **BR-RTR**: `apt-get update && apt-get install -y frr &`
    *   На **HQ-SRV**: `apt-get update && apt-get install -y dnsmasq &`
8.  Выполнить **Шаг 6:** Настройка IP-туннеля (GRE) (Связь роутеров).
9.  **(Конфигурация после установки):** По мере завершения фоновых установок пакетов, настроить соответствующие службы:
    *   Выполнить **Шаг 11:** Настройка часового пояса (Требует `tzdata` на ISP).
    *   Выполнить **Шаг 10:** Настройка DNS на HQ-SRV (Требует `dnsmasq` на HQ-SRV).
    *   Выполнить **Шаг 9:** Настройка DHCP на HQ-RTR (Требует `dnsmasq` на HQ-RTR и настроенного DNS).
    *   Выполнить **Шаг 7:** Настройка динамической маршрутизации (OSPF) (Требует `frr` и GRE).
**Важно:** Данное пособие **структурировано по шагам 1-11** согласно заданию. Рекомендуемый порядок выше относится к **практической последовательности действий** на экзамене.

**Используемый стенд:** Все настройки выполняются на виртуальных машинах согласно спецификациям, приведённым в **Таблице 2**. Предполагается, что виртуальные машины развёрнуты и базовые операционные системы установлены.

**Важно:** При выполнении заданий внимательно сверяйтесь с **Рисунком 1 (Топология сети)** и **Таблицей 1 (Итоговая таблица адресации Модуля 1)** для корректного применения IP-адресов, масок, VLAN ID и других сетевых параметров. **Помните про исключение для VLAN ID!**

**Ключевые концепции и инструменты:**

*   **Выполнение команд:** Все команды в данном руководстве предполагаются к выполнению от имени суперпользователя (`root`), если явно не указано иное (например, через `su -`).
*   **Маркеры контекста:** Перед каждым блоком команд будет указан маркер вида `# [root@hostname ~]# --- Описание ---`, явно показывающий, на какой машине и от какого пользователя выполняется команда. Сами команды вставляются без этого маркера.
*   **Установка пакетов (ALT Linux):** Используется команда `apt-get update && apt-get install -y имя_пакета`. Она сначала обновляет списки доступных пакетов из репозиториев, а затем (`&&`) устанавливает указанный пакет без интерактивных вопросов (`-y`). **Помните:** Для экономии времени на экзамене можно запускать долгие установки в фоне, добавив `&` в конце команды, и продолжать другие шаги.
*   **Управление службами (systemd):**
    *   `systemctl enable --now имя_службы`: Включает автозапуск службы при загрузке системы и немедленно запускает её. Используется для первого запуска и включения.
    *   `systemctl restart имя_службы`: Перезапускает службу, например, для применения новой конфигурации.
    *   `systemctl status имя_службы`: Показывает текущий статус службы (активна, неактивна, ошибки).
    *   `systemctl is-active имя_службы`: Кратко проверяет, активна ли служба (`active` или `inactive`).
*   **Редактирование файлов:**
    *   **`sed -i 's/шаблон/замена/g' файл`**: Используется для автоматической замены текста внутри файла "на месте" (`-i`). Шаблон и замена разделяются символом (часто `/`, но может быть `|` или `#` для удобства).
    *   **`cat <<'EOF' > файл ... EOF` (Here Document):** Позволяет записать многострочный текст в файл. Разделитель `EOF` (может быть любым словом) указывает начало и конец текста. **Одинарные кавычки** вокруг `'EOF'` в первой строке **предотвращают интерпретацию** таких символов, как `$`и `\` внутри текста со стороны *внешней* оболочки, что безопасно для записи конфигурационных файлов. Закрывающий `EOF` должен стоять в самом начале строки.
    *   **`tee -a файл > /dev/null <<'EOF' ... EOF`**: Используется для **добавления** (`-a`) многострочного текста в конец файла, не выводя текст на экран (`> /dev/null`). Кавычки у `'EOF'` также используются для безопасности.
*   **Проверочные команды:** Команды, предназначенные *только* для проверки состояния или просмотра конфигурации (например, `cat`, `grep`, `ip addr`, `systemctl status`, `ls`), как правило, вынесены в раздел `### Проверка шага...`.
*   **Опциональные проверки (в коде):** Комментарии вида `# [Проверка (опционально): <что>]` перед командами указывают на необязательные проверочные действия внутри основного потока команд. Они могут быть полезны для отладки, но их выполнение не строго обязательно для конечного результата.
*   **Объяснения:** При первом появлении важного инструмента или концепции даётся более подробное объяснение. При последующих использованиях предполагается, что базовый принцип понятен. Если появляется новый важный параметр или опция, даётся краткое пояснение.
*   **Комментарии о вариативности:** Комментарии вида `// ВАРИАТИВНО: ...` указывают на параметры, которые могут отличаться в вашем варианте задания.

---

# Общие сведения о топологии и тестовом стенде

Здесь представлены рисунок 1 и таблица 1, которые являются частью задания модуля 1. Для Модуля 2 будет представлена отдельная таблица адресации, так как имена интерфейсов на преднастроенном стенде Модуля 2 отличаются.

**Рисунок 1. Топология сети**
(Схема остаётся визуально прежней, но логическая адресация может меняться в зависимости от модуля)

```text
                +------------+
                |  Internet  |
                +------------+
                      |
                +------------+
                |    ISP     |----------------(  ISP-BR  )----------------+
                +------------+                                            |
                      |                                                   |
                 (  ISP-HQ  )                                             |
                      |                                                   |
                +------------+                                      +------------+
                |   HQ-RTR   |===========(  GRE Tunnel  )===========|   BR-RTR   |
                +------------+                                      +------------+
                      |                                                   |
                 (  HQ-Net  )                                             |
                  /   |    \                                              |
  ( VLAN 100 )---+    |     +--( VLAN 200 )                          (  BR-Net  )
      |               |             |                                     |
+------------+  ( VLAN 999 )  +------------+                        +------------+
|   HQ-SRV   | ( Management ) |   HQ-CLI   |                        |   BR-SRV   |
+------------+                +------------+                        +------------+
```

**Таблица 1. Итоговая таблица адресации (Модуль 1)**

| Устройство | Интерфейс           | IP-адрес            | Маска             | VLAN  | Подсеть           | Шлюз          | Примечание                  |
| :--------- | :------------------ | :------------------ | :---------------- | :---- | :---------------- | :------------ | :-------------------------- |
| ISP        | ens18 (к Интернету) | DHCP                | DHCP              | -     | DHCP              | DHCP          | Внешний канал               |
|            | ens19 (к HQ-RTR)    | 172.16.4.1          | 255.255.255.240   | -     | 172.16.4.0/28     | -             | WAN к HQ-RTR                |
|            | ens20 (к BR-RTR)    | 172.16.5.1          | 255.255.255.240   | -     | 172.16.5.0/28     | -             | WAN к BR-RTR                |
| HQ-RTR     | ens18 (к ISP)       | 172.16.4.4          | 255.255.255.240   | -     | 172.16.4.0/28     | 172.16.4.1    | WAN к ISP                   |
|            | ens19 (Trunk)       | -                   | -                 | Trunk | -                 | -             | Trunk порт для LAN/DMZ      |
|            | vlan100 (на ens19)  | 192.168.1.1         | 255.255.255.192   | 100   | 192.168.1.0/26    | -             | LAN 1 (Сеть HQ-SRV) // ВАРИАТИВНО: Размер сети HQ-SRV (влияет на маску). VLAN ID см. ниже. |
|            | vlan200 (на ens19)  | 192.168.2.1         | 255.255.255.240   | 200   | 192.168.2.0/28    | -             | LAN 2 (Сеть HQ-CLI) // ВАРИАТИВНО: Размер сети HQ-CLI (влияет на маску). VLAN ID см. ниже. |
|            | vlan999 (на ens19)  | 192.168.99.1        | 255.255.255.248   | 999   | 192.168.99.0/29   | -             | Сеть управления (DMZ) // ВАРИАТИВНО: Размер сети Управления (влияет на маску). **VLAN ID для этой сети (здесь пример 999) должен быть взят из вашего варианта задания.** |
|            | gre1 (IP туннель)   | 192.168.5.1         | 255.255.255.252   | -     | 192.168.5.0/30    | -             | GRE туннель к BR-RTR        |
| HQ-SRV     | ens18 (к HQ-RTR)    | 192.168.1.10        | 255.255.255.192   | 100*  | 192.168.1.0/26    | 192.168.1.1   | LAN 1 (Подключён к VLAN100) // ВАРИАТИВНО: Размер сети HQ-SRV (влияет на маску). |
| HQ-CLI     | ens18 (к HQ-RTR)    | DHCP (192.168.2.10) | DHCP (255.255.255.240)   | 200*  | 192.168.2.0/28    | 192.168.2.1   | LAN 2 (Подключён к VLAN200) // ВАРИАТИВНО: Размер сети HQ-CLI (влияет на маску). |
| BR-RTR     | ens18 (к ISP)       | 172.16.5.5          | 255.255.255.240   | -     | 172.16.5.0/28     | 172.16.5.1    | WAN к ISP                   |
|            | ens19 (к BR-SRV)    | 192.168.3.1         | 255.255.255.224   | -     | 192.168.3.0/27    | -             | LAN 3 (Сеть BR-SRV) // ВАРИАТИВНО: Размер сети BR-SRV (влияет на маску). |
|            | gre1 (IP туннель)   | 192.168.5.2         | 255.255.255.252   | -     | 192.168.5.0/30    | -             | GRE туннель к HQ-RTR        |
| BR-SRV     | ens18 (к BR-RTR)    | 192.168.3.10        | 255.255.255.224   | -     | 192.168.3.0/27    | 192.168.3.1   | LAN 3 (Сеть BR) // ВАРИАТИВНО: Размер сети BR-SRV (влияет на маску). |
    
**Примечание:** Символ `*` в колонке VLAN означает, что тегирование настроено на уровне гипервизора. Для `HQ-SRV` и `HQ-CLI` интерфейс `ens18` подключается к транковому порту `ens19` на `HQ-RTR`. IP-адрес/DHCP настроены непосредственно на `ens18` этих машин. **VLAN ID**:
*   **Для HQ-SRV и HQ-CLI:** Независимо от вашего варианта задания, **всегда используйте `ID 100` (для HQ-SRV) и `ID 200` (для HQ-CLI)** из-за особенностей стенда.
*   **Для сети Управления:** **Всегда используйте VLAN ID (например, `999` в данном пособии), указанный в вашем варианте задания.**

**Таблица 2. Спецификации оборудования**

| Машина | RAM, ГБ | CPU | HDD/SDD, ГБ | OS                             |
| :----- | :------ | :-: | :---------: | :----------------------------- |
| ISP    | 1       |  1  |     10      | ОС Альт JeOS/Linux             |
| HQ-RTR | 1       |  1  |     10      | ОС Альт Сервер                 |
| BR-RTR | 1       |  1  |     10      | ОС Альт Сервер                 |
| HQ-SRV | 2       |  1  |     10      | ОС Альт Сервер                 |
| BR-SRV | 2       |  1  |     10      | ОС Альт Сервер                 |
| HQ-CLI | 3       |  2  |     15      | ОС Альт Рабочая Станция        |
| Итого  | 10      |  7  |     65      | -                              |

---

# **Модуль 1:** Настройка сетевой инфраструктуры

**Введение:** Этот модуль посвящён базовой настройке сетевых устройств согласно предложенной топологии (**Рисунок 1**) и таблице адресации (**Таблица 1**). Мы последовательно настроим имена хостов, IP-адреса, обеспечим доступ в Интернет, настроим VLAN и создадим необходимые учётные записи, туннели и маршрутизацию.

**(Используйте рекомендуемый порядок выполнения из Введения для оптимальной работы)**

## Шаг 1: Базовая настройка устройств

**Цель:** Присвоить каждому устройству в сети уникальное имя и настроить базовую IP-адресацию согласно плану (**Таблица 1**). Это основа для дальнейшей конфигурации и идентификации устройств в сети.

**Практическое назначение:** Уникальные имена (FQDN) необходимы для разрешения имён через DNS и удобства администрирования. Статическая IP-адресация критична для серверов и маршрутизаторов, обеспечивая стабильность сетевых служб. Включение IP-форвардинга на маршрутизаторах необходимо для передачи трафика между сетями.

**`Студентам НЕОБХОДИМО внести таблицу адресации в итоговый отчёт по этому шагу.`**

### Шаг 1.1: Настройка имён хостов (FQDN)

Каждому устройству необходимо присвоить **полное доменное имя (FQDN - Fully Qualified Domain Name)**. FQDN включает имя хоста и доменное имя (например, `hq-rtr.au-team.irpo`), что обеспечивает уникальную идентификацию в сети и необходимо для корректной работы некоторых сетевых служб, таких как DNS. Настройка выполняется на каждом устройстве стенда с помощью команды `hostnamectl set-hostname`.

#### ISP

```bash
# [root@localhost ~]# --- Устанавливаем FQDN для ISP ---
# FQDN состоит из имени хоста (isp) и доменного имени (au-team.irpo).
hostnamectl set-hostname isp.au-team.irpo
# [root@isp ~]# --- Перезапускаем оболочку bash ---
# Перезапускаем оболочку bash, чтобы изменения имени хоста немедленно отразились в командной строке.
exec bash
```

#### HQ-RTR

```bash
# [root@localhost ~]# --- Устанавливаем FQDN для HQ-RTR ---
hostnamectl set-hostname hq-rtr.au-team.irpo
# [root@hq-rtr ~]# --- Перезапускаем оболочку bash ---
exec bash
```

#### BR-RTR

```bash
# [root@localhost ~]# --- Устанавливаем FQDN для BR-RTR ---
hostnamectl set-hostname br-rtr.au-team.irpo
# [root@br-rtr ~]# --- Перезапускаем оболочку bash ---
exec bash
```

#### HQ-SRV

```bash
# [root@localhost ~]# --- Устанавливаем FQDN для HQ-SRV ---
hostnamectl set-hostname hq-srv.au-team.irpo
# [root@hq-srv ~]# --- Перезапускаем оболочку bash ---
exec bash
```

#### BR-SRV

```bash
# [root@localhost ~]# --- Устанавливаем FQDN для BR-SRV ---
hostnamectl set-hostname br-srv.au-team.irpo
# [root@br-srv ~]# --- Перезапускаем оболочку bash ---
exec bash
```

#### HQ-CLI

```bash
# [root@localhost ~]# --- Устанавливаем FQDN для HQ-CLI ---
hostnamectl set-hostname hq-cli.au-team.irpo
# [root@hq-cli ~]# --- Перезапускаем оболочку bash ---
exec bash
```

---

      
### Шаг 1.2: Настройка статической IP-адресации (IPv4)

На этом шаге мы настроим статические IP-адреса на сетевых интерфейсах маршрутизаторов и серверов согласно **Таблице 1**. Статическая адресация важна для устройств с фиксированной ролью в сети (маршрутизаторы, серверы). Клиентская машина `HQ-CLI` пока получит временный статический адрес; позже для неё будет настроен DHCP.

**Используемая система конфигурации сети:** В данном руководстве используется система настройки сети через файлы в директориях `/etc/net/ifaces/имя_интерфейса/`. Это стандартный метод для используемых дистрибутивов ОС ALT Linux.

*   **Подготовка директорий:** Команды `mkdir -p` создают директории для конфигурации интерфейсов, если они не существуют. Команда `find ... -delete` очищает предыдущие конфигурационные файлы для данного интерфейса, чтобы избежать конфликтов.
*   **IP-адресация:** Используются приватные IP-адреса согласно **RFC1918** (`10.x.x.x`, `172.16-31.x.x`, `192.168.x.x`). Размеры подсетей определены в задании (например, `/26` для `HQ-SRV`, `/28` для `HQ-CLI` и т.д.). **// ВАРИАТИВНО: Размеры подсетей (маски) для HQ-SRV, HQ-CLI, Управления, BR-SRV могут отличаться. Используйте маску из вашего варианта задания.**
*   **Временная настройка DNS-клиента:** Для машин, которым потребуется установка пакетов из интернета до настройки основного DNS-сервера (`HQ-RTR`, `BR-RTR`, `HQ-SRV`), будет выполнена временная настройка DNS-клиента. В файл `/etc/net/ifaces/имя_WAN_интерфейса/resolv.conf` будут добавлены публичные DNS-серверы (например, `8.8.8.8`). Эта настройка будет действовать до момента конфигурации DNS в Шаге 10.
*   **IP-форвардинг:** На маршрутизаторах (`HQ-RTR`, `BR-RTR`, `ISP`) необходимо включить **IP-форвардинг (пересылку пакетов)**, чтобы они могли маршрутизировать трафик между разными сетями. Это делается изменением параметра ядра `net.ipv4.ip_forward` с помощью команды `sed` (для постоянного изменения в `/etc/net/sysctl.conf`) и `sysctl -w` (для немедленного применения без перезагрузки).
*   **Подключение ВМ к VLAN:** Для `HQ-SRV` и `HQ-CLI` тегирование VLAN выполняется **на уровне гипервизора**. Это означает, что внутри ВМ мы настраиваем интерфейс `ens18` (согласно Таблице 1) как обычный порт доступа (untagged), присваивая ему IP из соответствующей подсети VLAN. Сама ОС ВМ не настраивает VLAN-теги.

**(Важно для отчётности):** Согласно заданию, сведения об IP-адресации необходимо занести в отчёт в виде таблицы (см. формат в Приложении Б КОДа). Эта таблица является **первым из пяти требуемых элементов отчётности** для итогового отчёта Модуля 1.
**`Студентам НЕОБХОДИМО подготовить таблицу адресации по этому шагу.`**

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Конфигурация интерфейса ens18 (к ISP) ---
# Создаём директорию для настроек интерфейса ens18, если она отсутствует.
mkdir -p /etc/net/ifaces/ens18
# Удаляем все существующие файлы конфигурации внутри /etc/net/ifaces/ens18.
find /etc/net/ifaces/ens18 -mindepth 1 -delete
# Указываем тип интерфейса Ethernet. BOOTPROTO=static подразумевается.
echo 'TYPE=eth' > /etc/net/ifaces/ens18/options
# Настраиваем статический IPv4-адрес и маску (172.16.4.4/28).
echo '172.16.4.4/28' > /etc/net/ifaces/ens18/ipv4address
# Настраиваем шлюз по умолчанию (IP-адрес ISP на этом сегменте).
echo 'default via 172.16.4.1' > /etc/net/ifaces/ens18/ipv4route
# [root@hq-rtr ~]# --- Временная настройка DNS-клиента для ens18 ---
# Это необходимо для скачивания пакетов (frr, dnsmasq) до настройки основного DNS.
cat <<'EOF' > /etc/net/ifaces/ens18/resolv.conf
nameserver 8.8.8.8
nameserver 1.1.1.1
EOF

# [root@hq-rtr ~]# --- Конфигурация интерфейса ens19 (Trunk к LAN HQ) ---
# Создаём директорию для настроек интерфейса ens19.
mkdir -p /etc/net/ifaces/ens19
# Очищаем предыдущие настройки для ens19.
find /etc/net/ifaces/ens19 -mindepth 1 -delete
# Указываем тип интерфейса Ethernet. IP-адрес не настраивается на транковом порту.
echo 'TYPE=eth' > /etc/net/ifaces/ens19/options

# [root@hq-rtr ~]# --- Включение IP-форвардинга ---
# Изменяем параметр ядра для разрешения пересылки пакетов (постоянно).
sed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/net/sysctl.conf
# [Проверка (опционально): изменение в файле]
# grep 'net.ipv4.ip_forward = 1' /etc/net/sysctl.conf
# Применяем изменение параметра ядра немедленно.
sysctl -w net.ipv4.ip_forward=1

# [root@hq-rtr ~]# --- Перезапуск сетевой службы ---
# Применяем все сделанные изменения в конфигурации интерфейсов.
systemctl restart network
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Конфигурация интерфейса ens18 (к ISP) ---
mkdir -p /etc/net/ifaces/ens18
find /etc/net/ifaces/ens18 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens18/options
# Настраиваем статический IPv4-адрес и маску (172.16.5.5/28).
echo '172.16.5.5/28' > /etc/net/ifaces/ens18/ipv4address
# Настраиваем шлюз по умолчанию (IP-адрес ISP).
echo 'default via 172.16.5.1' > /etc/net/ifaces/ens18/ipv4route
# [root@br-rtr ~]# --- Временная настройка DNS-клиента для ens18 ---
# Это необходимо для скачивания пакетов (frr) до настройки основного DNS.
cat <<'EOF' > /etc/net/ifaces/ens18/resolv.conf
nameserver 8.8.8.8
nameserver 1.1.1.1
EOF

# [root@br-rtr ~]# --- Конфигурация интерфейса ens19 (к LAN BR) ---
mkdir -p /etc/net/ifaces/ens19
find /etc/net/ifaces/ens19 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens19/options
# Настраиваем статический IPv4-адрес и маску (шлюз для сети BR-SRV).
# // ВАРИАТИВНО: Размер сети BR-SRV (влияет на маску /27) может отличаться. Используйте маску из вашего варианта задания.
echo '192.168.3.1/27' > /etc/net/ifaces/ens19/ipv4address

# [root@br-rtr ~]# --- Включение IP-форвардинга ---
sed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/net/sysctl.conf
# [Проверка (опционально): изменение в файле]
# grep 'net.ipv4.ip_forward = 1' /etc/net/sysctl.conf
sysctl -w net.ipv4.ip_forward=1

# [root@br-rtr ~]# --- Перезапуск сетевой службы ---
systemctl restart network
```

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Конфигурация интерфейса ens18 (VLAN 100 через гипервизор) ---
mkdir -p /etc/net/ifaces/ens18
find /etc/net/ifaces/ens18 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens18/options
# Настраиваем статический IPv4-адрес и маску (192.168.1.10/26).
# // ВАРИАТИВНО: Размер сети HQ-SRV (влияет на маску /26) может отличаться. Используйте маску из вашего варианта задания.
echo '192.168.1.10/26' > /etc/net/ifaces/ens18/ipv4address
# Настраиваем шлюз по умолчанию (IP-адрес интерфейса vlan100 на HQ-RTR).
echo 'default via 192.168.1.1' > /etc/net/ifaces/ens18/ipv4route
# [root@hq-srv ~]# --- Временная настройка DNS-клиента для ens18 ---
# Это необходимо для скачивания пакетов (dnsmasq) до настройки основного DNS.
# Доступ в интернет будет осуществляться через HQ-RTR (NAT) после его полной настройки.
cat <<'EOF' > /etc/net/ifaces/ens18/resolv.conf
nameserver 8.8.8.8
nameserver 1.1.1.1
EOF

# [root@hq-srv ~]# --- Перезапуск сетевой службы ---
systemctl restart network
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Конфигурация интерфейса ens18 (к BR-RTR) ---
mkdir -p /etc/net/ifaces/ens18
find /etc/net/ifaces/ens18 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens18/options
# Настраиваем статический IPv4-адрес и маску (192.168.3.10/27).
# // ВАРИАТИВНО: Размер сети BR-SRV (влияет на маску /27) может отличаться. Используйте маску из вашего варианта задания.
echo '192.168.3.10/27' > /etc/net/ifaces/ens18/ipv4address
# Настраиваем шлюз по умолчанию (IP-адрес интерфейса ens19 на BR-RTR).
echo 'default via 192.168.3.1' > /etc/net/ifaces/ens18/ipv4route

# [root@br-srv ~]# --- Перезапуск сетевой службы ---
systemctl restart network
```

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Временная статическая конфигурация ens18 (VLAN 200 через гипервизор) ---
# Позже будет настроен DHCP (Шаг 9).
mkdir -p /etc/net/ifaces/ens18
find /etc/net/ifaces/ens18 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens18/options
# Временно настраиваем статический IPv4-адрес (192.168.2.10/28).
# // ВАРИАТИВНО: Размер сети HQ-CLI (влияет на маску /28) может отличаться. Используйте маску из вашего варианта задания.
echo '192.168.2.10/28' > /etc/net/ifaces/ens18/ipv4address
# Временно настраиваем шлюз по умолчанию (IP-адрес интерфейса vlan200 на HQ-RTR).
echo 'default via 192.168.2.1' > /etc/net/ifaces/ens18/ipv4route

# [root@hq-cli ~]# --- Перезапуск сетевой службы ---
systemctl restart network
# Перезагружаем ВМ, т.к. IP/шлюз могут сбрасываться после `systemctl restart network`; перезагрузка гарантирует их применение.
reboot
```

**Примечание о временной конфигурации DNS:**
Временные файлы `/etc/net/ifaces/имя_интерфейса/resolv.conf`, созданные на HQ-RTR, BR-RTR и HQ-SRV, будут автоматически заменены или перезаписаны при выполнении Шага 10 (Настройка DNS), где клиенты будут настроены на использование корпоративного DNS-сервера 192.168.1.10.

### Проверка шага 1: Базовая настройка устройств

**Цель:** Проверить, что имена хостов (FQDN) установлены корректно на всех машинах, статические IP-адреса присвоены правильно, и IP-форвардинг включён на маршрутизаторах.

#### Все виртуальные машины

```bash
# [user@hostname ~]# --- Проверка имени хоста (FQDN) ---
hostname
# Ожидаемый результат: Соответствующее FQDN (например, `hq-rtr.au-team.irpo`).
```

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Проверка IP ens18 ---
ip -br addr show ens18
# Ожидаемый результат: Строка вида `ens18 UP 172.16.4.4/28 ...`.
# [root@hq-rtr ~]# --- Проверка шлюза по умолчанию ---
ip route show default
# Ожидаемый результат: Строка вида `default via 172.16.4.1 dev ens18 ...`.
# [root@hq-rtr ~]# --- Проверка IP-форвардинга ---
sysctl net.ipv4.ip_forward
# Ожидаемый результат: `net.ipv4.ip_forward = 1`.
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Проверка IP ens18 ---
ip -br addr show ens18
# Ожидаемый результат: Строка вида `ens18 UP 172.16.5.5/28 ...`.
# [root@br-rtr ~]# --- Проверка IP ens19 ---
ip -br addr show ens19
# Ожидаемый результат: Строка вида `ens19 UP 192.168.3.1/27 ...`. // ВАРИАТИВНО: Маска может отличаться.
# [root@br-rtr ~]# --- Проверка шлюза по умолчанию ---
ip route show default
# Ожидаемый результат: Строка вида `default via 172.16.5.1 dev ens18 ...`.
# [root@br-rtr ~]# --- Проверка IP-форвардинга ---
sysctl net.ipv4.ip_forward
# Ожидаемый результат: `net.ipv4.ip_forward = 1`.
```

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Проверка IP ens18 ---
ip -br addr show ens18
# Ожидаемый результат: Строка вида `ens18 UP 192.168.1.10/26 ...`. // ВАРИАТИВНО: Маска может отличаться.
# [root@hq-srv ~]# --- Проверка шлюза по умолчанию ---
ip route show default
# Ожидаемый результат: Строка вида `default via 192.168.1.1 dev ens18 ...`.
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка IP ens18 ---
ip -br addr show ens18
# Ожидаемый результат: Строка вида `ens18 UP 192.168.3.10/27 ...`. // ВАРИАТИВНО: Маска может отличаться.
# [root@br-srv ~]# --- Проверка шлюза по умолчанию ---
ip route show default
# Ожидаемый результат: Строка вида `default via 192.168.3.1 dev ens18 ...`.
```

#### HQ-CLI (временная статика)

```bash
# [root@hq-cli ~]# --- Проверка IP ens18 ---
ip -br addr show ens18
# Ожидаемый результат: Строка вида `ens18 UP 192.168.2.10/28 ...`. // ВАРИАТИВНО: Маска может отличаться.
# [root@hq-cli ~]# --- Проверка шлюза по умолчанию ---
ip route show default
# Ожидаемый результат: Строка вида `default via 192.168.2.1 dev ens18 ...`.
```

#### Проверка базовой связности (выполнять на соответствующих машинах)

```bash
# [root@hq-rtr ~]# --- Проверка с HQ-RTR (заработает после Шага 2) ---
# Пингуем шлюз провайдера (ISP) с маршрутизатора HQ-RTR.
# ping -c 3 172.16.4.1
# Ожидаемый результат: Успешные ответы.

# [root@br-rtr ~]# --- Проверка с BR-RTR (заработает после Шага 2) ---
# Пингуем шлюз провайдера (ISP) с маршрутизатора BR-RTR.
# ping -c 3 172.16.5.1
# Ожидаемый результат: Успешные ответы.

# [root@hq-srv ~]# --- Проверка с HQ-SRV (заработает после Шага 4) ---
# Пингуем IP-адрес интерфейса VLAN 100 на HQ-RTR, который является шлюзом для HQ-SRV.
# ping -c 3 192.168.1.1
# Ожидаемый результат: Успешные ответы (после настройки VLAN на HQ-RTR).

# [root@br-srv ~]# --- Проверка с BR-SRV ---
# Пингуем IP-адрес интерфейса ens19 на BR-RTR, который является шлюзом для BR-SRV.
ping -c 3 192.168.3.1
# Ожидаемый результат: Успешные ответы.

# [root@hq-cli ~]# --- Проверка с HQ-CLI (заработает после Шага 4) ---
# Пингуем IP-адрес интерфейса VLAN 200 на HQ-RTR, который является шлюзом для HQ-CLI.
# ping -c 3 192.168.2.1
# Ожидаемый результат: Успешные ответы (после настройки VLAN на HQ-RTR).
```

**Примечание:** Пинги на шлюзы VLAN (192.168.1.1 с HQ-SRV и 192.168.2.1 с HQ-CLI) заработают только после выполнения Шага 4. Аналогично, пинги на шлюзы провайдера (ISP) с HQ-RTR (на 172.16.4.1) и BR-RTR (на 172.16.5.1) заработают только после выполнения Шага 2.

---

## Шаг 2: Настройка ISP (Интернет-провайдера)

**Цель:** Настроить маршрутизатор ISP так, чтобы он обеспечивал подключение к "Интернету" (через DHCP на внешнем интерфейсе `ens18`), предоставлял доступ к своим внутренним сетям для `HQ-RTR` и `BR-RTR` (статические IP на `ens19` и `ens20`) и выполнял трансляцию сетевых адресов (NAT) для выхода внутренних сетей HQ и BR в "Интернет".

**Практическое назначение:** Имитация работы реального интернет-провайдера. NAT (в данном случае `MASQUERADE`) необходим, чтобы позволить устройствам из внутренних сетей (с приватными IP-адресами) получать доступ к ресурсам во внешней сети, используя публичный IP-адрес маршрутизатора ISP.

#### ISP

```bash
# [root@isp ~]# --- Конфигурация интерфейса ens18 (к "Интернету", DHCP) ---
mkdir -p /etc/net/ifaces/ens18
find /etc/net/ifaces/ens18 -mindepth 1 -delete
# Указываем получение настроек по DHCP
cat <<'EOF' > /etc/net/ifaces/ens18/options
BOOTPROTO=dhcp
TYPE=eth
EOF

# [root@isp ~]# --- Конфигурация интерфейса ens19 (к HQ-RTR, статика) ---
mkdir -p /etc/net/ifaces/ens19
find /etc/net/ifaces/ens19 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens19/options
# Настраиваем статический IP и маску для связи с HQ-RTR.
echo '172.16.4.1/28' > /etc/net/ifaces/ens19/ipv4address

# [root@isp ~]# --- Конфигурация интерфейса ens20 (к BR-RTR, статика) ---
mkdir -p /etc/net/ifaces/ens20
find /etc/net/ifaces/ens20 -mindepth 1 -delete
echo 'TYPE=eth' > /etc/net/ifaces/ens20/options
# Настраиваем статический IP и маску для связи с BR-RTR.
echo '172.16.5.1/28' > /etc/net/ifaces/ens20/ipv4address

# [root@isp ~]# --- Включение IP-форвардинга ---
sed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/net/sysctl.conf
sysctl -w net.ipv4.ip_forward=1

# [root@isp ~]# --- Перезапуск сетевой службы ---
systemctl restart network

# [root@isp ~]# --- Установка iptables ---
apt-get update && apt-get install -y iptables

# [root@isp ~]# --- Настройка NAT (Network Address Translation) ---
# iptables - стандартный инструмент для firewall и NAT в Linux.
# -t nat: Работаем с таблицей NAT.
# -F POSTROUTING: Очищаем цепочку POSTROUTING (важно перед добавлением правил).
iptables -t nat -F POSTROUTING
# -A POSTROUTING: Добавляем правило в конец цепочки POSTROUTING (после маршрутизации).
# -o ens18: Правило для пакетов, уходящих через интерфейс ens18.
# -j MASQUERADE: Действие - подменять исходящий IP на IP интерфейса ens18.
iptables -t nat -A POSTROUTING -o ens18 -j MASQUERADE

# [root@isp ~]# --- Установка разрешающих политик firewall (упрощённая) ---
# -P INPUT/FORWARD/OUTPUT ACCEPT: Устанавливает политику по умолчанию "Разрешить".
# Для учебного стенда это допустимо, в реальных системах требуются более строгие правила.
iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -P OUTPUT ACCEPT

# [root@isp ~]# --- Сохранение правил iptables ---
# iptables-save: Выводит текущие правила в формате, понятном iptables-restore.
# > /etc/sysconfig/iptables: Перенаправляет вывод в файл для автозагрузки службой.
iptables-save > /etc/sysconfig/iptables
# [Проверка (опционально): сохранённые правила]
# cat /etc/sysconfig/iptables | grep MASQUERADE

# [root@isp ~]# --- Включение и запуск службы iptables ---
# enable --now: Включает автозапуск и сразу запускает службу.
# Служба iptables загрузит правила из /etc/sysconfig/iptables.
systemctl enable --now iptables
```

### Проверка шага 2: Настройка ISP (Интернет-провайдера)

**Цель:** Проверить корректность настройки IP-адресов, IP-форвардинга и NAT на маршрутизаторе ISP.

#### ISP

```bash
# [root@isp ~]# --- Проверка IP-адресов ---
ip -br addr show ens18
# Ожидаемый результат: `ens18 UP <IP-адрес_от_DHCP>/<маска> ...`.
ip -br addr show ens19
# Ожидаемый результат: `ens19 UP 172.16.4.1/28 ...`.
ip -br addr show ens20
# Ожидаемый результат: `ens20 UP 172.16.5.1/28 ...`.

# [root@isp ~]# --- Проверка IP-форвардинга ---
sysctl net.ipv4.ip_forward
# Ожидаемый результат: `net.ipv4.ip_forward = 1`.

# [root@isp ~]# --- Проверка шлюза по умолчанию (от DHCP) ---
ip route show default
# Ожидаемый результат: Строка вида `default via <IP-адрес_шлюза_от_DHCP> dev ens18 ...`.

# [root@isp ~]# --- Проверка правила NAT (Masquerade) ---
iptables -t nat -L POSTROUTING -nv
# Ожидаемый результат: Наличие строки с target `MASQUERADE` для out-interface `ens18`.

# [root@isp ~]# --- Проверка базовой связности ---
# Внешний ресурс (проверка выхода в "Интернет")
ping -c 3 8.8.8.8 
# Ожидаемый результат: Успешные ответы.
 # Пингуем IP-адрес интерфейса ens18 на HQ-RTR
ping -c 3 172.16.4.4
# Ожидаемый результат: Успешные ответы (если HQ-RTR настроен).
 # Пингуем IP-адрес интерфейса ens18 на BR-RTR
ping -c 3 172.16.5.5
# Ожидаемый результат: Успешные ответы (если BR-RTR настроен).
```

---

## Шаг 3: Создание локальных учётных записей

**Цель:** Создать необходимых пользователей (`sshuser` на серверах, `net_admin` на маршрутизаторах) с заданными паролями, UID и правами `sudo` без пароля для выполнения административных задач и удалённого доступа.

**Практическое назначение:** Использование выделенных учётных записей для разных ролей повышает безопасность и управляемость. `sudo` позволяет выполнять команды с повышенными привилегиями без необходимости входа под `root`. Отсутствие запроса пароля (`NOPASSWD`) упрощает работу в рамках учебного стенда, но **не рекомендуется** в производственной среде.

**Метод создания:**
*   `useradd пользователь -u UID -U`: Создаёт пользователя с указанным UID и одноимённой основной группой.
*   `echo 'пользователь:пароль' | chpasswd`: Устанавливает пароль пользователя (**небезопасный метод в реальной среде!**).
*   `usermod -aG wheel пользователь`: Добавляет пользователя в группу `wheel`, что обычно даёт право использовать `sudo`.
*   `tee -a /etc/sudoers > /dev/null <<'EOF' ... EOF`: Добавляет правило `NOPASSWD` в `/etc/sudoers` для указанного пользователя, разрешая выполнение всех команд без пароля.

### Шаг 3.1: Создание пользователя `sshuser` на серверах HQ-SRV и BR-SRV

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Создание пользователя sshuser (UID 1010) ---
# // ВАРИАТИВНО: UID пользователя 'sshuser' (здесь 1010) может отличаться. Используйте UID из вашего варианта задания.
useradd sshuser -u 1010 -U
# Устанавливаем пароль 'P@ssw0rd'.
echo 'sshuser:P@ssw0rd' | chpasswd
# Добавляем в группу wheel.
usermod -aG wheel sshuser
# Добавляем правило sudo NOPASSWD.
tee -a /etc/sudoers > /dev/null <<'EOF'

sshuser ALL=(ALL) NOPASSWD: ALL
EOF
# [Проверка (опционально): правило sudo]
# tail -n 2 /etc/sudoers | grep sshuser
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Создание пользователя sshuser (UID 1010) ---
# // ВАРИАТИВНО: UID пользователя 'sshuser' (здесь 1010) может отличаться. Используйте UID из вашего варианта задания.
useradd sshuser -u 1010 -U
# Устанавливаем пароль 'P@ssw0rd'.
echo 'sshuser:P@ssw0rd' | chpasswd
# Добавляем в группу wheel.
usermod -aG wheel sshuser
# Добавляем правило sudo NOPASSWD.
tee -a /etc/sudoers > /dev/null <<'EOF'

sshuser ALL=(ALL) NOPASSWD: ALL
EOF
# [Проверка (опционально): правило sudo]
# tail -n 2 /etc/sudoers | grep sshuser
```

### Шаг 3.2: Создание пользователя `net_admin` на маршрутизаторах HQ-RTR и BR-RTR

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Создание пользователя net_admin (UID 1010) ---
# UID 1010 может совпадать с UID sshuser на серверах, но это разные машины.
# // ВАРИАТИВНО: UID пользователя 'net_admin' (здесь 1010) может отличаться. Используйте UID из вашего варианта задания.
useradd net_admin -u 1010 -U
# Устанавливаем пароль 'P@$$word'.
echo 'net_admin:P@$$word' | chpasswd
# Добавляем в группу wheel.
usermod -aG wheel net_admin
# Добавляем правило sudo NOPASSWD.
tee -a /etc/sudoers > /dev/null <<'EOF'

net_admin ALL=(ALL) NOPASSWD: ALL
EOF
# [Проверка (опционально): правило sudo]
# tail -n 2 /etc/sudoers | grep net_admin
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Создание пользователя net_admin (UID 1010) ---
# // ВАРИАТИВНО: UID пользователя 'net_admin' (здесь 1010) может отличаться. Используйте UID из вашего варианта задания.
useradd net_admin -u 1010 -U
# Устанавливаем пароль 'P@$$word'.
echo 'net_admin:P@$$word' | chpasswd
# Добавляем в группу wheel.
usermod -aG wheel net_admin
# Добавляем правило sudo NOPASSWD.
tee -a /etc/sudoers > /dev/null <<'EOF'

net_admin ALL=(ALL) NOPASSWD: ALL
EOF
# [Проверка (опционально): правило sudo]
# tail -n 2 /etc/sudoers | grep net_admin
```

### Проверка шага 3: Создание локальных учётных записей

**Цель:** Проверить, что пользователи `sshuser` (на серверах) и `net_admin` (на маршрутизаторах) созданы корректно с нужными UID, группами и правами `sudo`.

#### HQ-SRV и BR-SRV

```bash
# [root@hostname ~]# --- Проверка пользователя sshuser (ID, группы) ---
id sshuser
# Ожидаемый результат: `uid=1010(sshuser) gid=1010(sshuser) groups=1010(sshuser),10(wheel)`. // ВАРИАТИВНО: UID может отличаться.
# [root@hostname ~]# --- Проверка прав sudo для sshuser ---
sudo -l -U sshuser
# Ожидаемый результат: Вывод содержит `(ALL) NOPASSWD: ALL`.
```

#### HQ-RTR и BR-RTR

```bash
# [root@hostname ~]# --- Проверка пользователя net_admin (ID, группы) ---
id net_admin
# Ожидаемый результат: `uid=1010(net_admin) gid=1010(net_admin) groups=1010(net_admin),10(wheel)`. // ВАРИАТИВНО: UID может отличаться.
# [root@hostname ~]# --- Проверка прав sudo для net_admin ---
sudo -l -U net_admin
# Ожидаемый результат: Вывод содержит `(ALL) NOPASSWD: ALL`.
```

---

## Шаг 4: Настройка VLAN на HQ-RTR

**Цель:** Разделить локальную сеть главного офиса (HQ) на три логически изолированных сегмента с помощью **VLAN (Virtual Local Area Network)**:
*   VLAN 100: для сервера `HQ-SRV` (сеть `192.168.1.0/26`)
*   VLAN 200: для клиента `HQ-CLI` (сеть `192.168.2.0/28`)
*   VLAN 999: для управления (сеть `192.168.99.0/29`)

**Реализация:** Настройка выполняется на маршрутизаторе `HQ-RTR`. Физический интерфейс `ens19` (согласно Таблице 1), смотрящий в локальную сеть HQ, будет настроен как **Trunk Port** (транковый порт). Для каждого VLAN будет создан виртуальный подынтерфейс (`vlan100`, `vlan200`, `vlan999`) поверх `ens19`, на которых будут настроены IP-адреса, служащие шлюзами для соответствующих VLAN.

**Практическое назначение:** Использование VLAN повышает безопасность (изоляция трафика), управляемость (логическое разделение сети) и производительность (уменьшение широковещательных доменов).

---

**Внимание: Критический важный момент с настройкой VLAN ID**

На экзамене вам может попасться вариант задания, где для сетей HQ-SRV, HQ-CLI и Управления требуются **другие VLAN ID** (например, 10, 20, 99), отличающиеся от тех (100, 200, 999), что используются в данном пособии.

**Проблема:** Преднастроенный стенд (виртуальные или физические коммутаторы), с которым вы будете работать, скорее всего, жёстко настроен на использование "стандартных" VLAN ID: **100 (HQ-SRV), 200 (HQ-CLI), 999 (Управление).** Если вы настроите VLAN ID согласно вашему варианту задания (например, 10, 20, 99), сетевое взаимодействие между устройствами **работать не будет!**

**Инструкция:**
*   **Для сетей HQ-SRV и HQ-CLI:** **Независимо от требований вашего варианта задания к VLAN ID для этих двух сетей,** при настройке интерфейсов на `HQ-RTR` (и других связанных настроек VLAN) **всегда используйте следующие фиксированные ID:**
    *   `VLAN ID 100` для сети HQ-SRV.
    *   `VLAN ID 200` для сети HQ-CLI.
    *   **Игнорируйте** требования к VLAN ID в вашем задании для сетей **HQ-SRV и HQ-CLI**, если они отличаются от 100 и 200.
*   **Для сети Управления (Management): Всегда используйте VLAN ID, указанный в вашем варианте задания.** Если в вашем варианте ID не указан, можно использовать `VLAN ID 999`, как в данном пособии (это значение вариативно).
*   **На экзамене:** Если возможно, уточните этот момент у эксперта. Если эксперт при проверке укажет на несоответствие настроенных вами VLAN ID 100/200 заданию, объясните, что вы действовали согласно этой инструкции для обеспечения работоспособности стенда.

**ВАЖНО:** Данное исключение касается **только VLAN ID**. По всем остальным параметрам (IP-адреса, маски подсетей, UID пользователей, порты служб, уровни RAID и т.д.) вы **должны** следовать требованиям вашего варианта задания!

---

**Метод:** Используется стандартная настройка VLAN в ALT Linux через файлы в `/etc/net/ifaces/`.
*   Файл `options` в директории `vlanXXX` содержит `TYPE=vlan`, `HOST=ens19` (родительский интерфейс) и `VID=XXX` (идентификатор VLAN).
*   Файл `ipv4address` содержит IP-адрес шлюза и префикс маски для данного VLAN.

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Конфигурация VLAN100 (для HQ-SRV) ---
# Создаём директорию для настроек виртуального интерфейса vlan100.
mkdir -p /etc/net/ifaces/vlan100
# Создаём файл 'options' для vlan100.
# // ВАРИАТИВНО: VLAN ID (здесь 100) может отличаться в задании. Но! Используйте 100 из-за стенда!
cat <<'EOF' > /etc/net/ifaces/vlan100/options
TYPE=vlan
HOST=ens19
VID=100
EOF
# Настраиваем IP-адрес и маску для интерфейса vlan100 (шлюз для сети HQ-SRV).
# // ВАРИАТИВНО: Размер сети HQ-SRV (влияет на маску /26) может отличаться.
echo '192.168.1.1/26' > /etc/net/ifaces/vlan100/ipv4address

# [root@hq-rtr ~]# --- Конфигурация VLAN200 (для HQ-CLI) ---
# Создаём директорию для настроек vlan200.
mkdir -p /etc/net/ifaces/vlan200
# Создаём файл 'options' для vlan200.
# // ВАРИАТИВНО: VLAN ID (здесь 200) может отличаться в задании. Но! Используйте 200 из-за стенда!
cat <<'EOF' > /etc/net/ifaces/vlan200/options
TYPE=vlan
HOST=ens19
VID=200
EOF
# Настраиваем IP-адрес и маску для vlan200 (шлюз для сети HQ-CLI).
# // ВАРИАТИВНО: Размер сети HQ-CLI (влияет на маску /28) может отличаться.
echo '192.168.2.1/28' > /etc/net/ifaces/vlan200/ipv4address

# [root@hq-rtr ~]# --- Конфигурация VLAN999 (управление) ---
# Создаём директорию для настроек vlan999.
mkdir -p /etc/net/ifaces/vlan999
# Создаём файл 'options' для vlan999.
# // ВАРИАТИВНО: VLAN ID для сети Управления (здесь пример 999). Используйте ID из вашего варианта задания.
cat <<'EOF' > /etc/net/ifaces/vlan999/options
TYPE=vlan
HOST=ens19
VID=999
EOF
# Настраиваем IP-адрес и маску для vlan999 (сеть управления).
# // ВАРИАТИВНО: Размер сети Управления (влияет на маску /29) может отличаться.
echo '192.168.99.1/29' > /etc/net/ifaces/vlan999/ipv4address

# [root@hq-rtr ~]# --- Перезапуск сетевой службы ---
# Применяем конфигурацию, создавая VLAN-подынтерфейсы и назначая им IP.
systemctl restart network
```

### Пример отчёта: настройка виртуального коммутатора (VLAN) на HQ-RTR

**Цель:**
Разделение трафика в локальной сети HQ на изолированные логические сегменты (`VLAN`) для сервера (`HQ-SRV`), клиента (`HQ-CLI`) и управления. Это необходимо для повышения безопасности и управляемости сети. Разделение реализовано на маршрутизаторе `HQ-RTR` с использованием его интерфейса `ens19`, подключённого к локальной сети HQ.

**Выбор реализации:**
Для реализации разделения на `VLAN` была выбрана технология виртуальных локальных сетей стандарта IEEE 802.1Q. В операционной системе Linux (используемой на `HQ-RTR`) эта технология поддерживается на уровне ядра через создание **VLAN-подынтерфейсов** (`vlan<ID>`) на базе физического сетевого интерфейса.

Физический интерфейс `ens19` на `HQ-RTR` был настроен как **транковый порт** (trunk port), способный передавать тегированный трафик нескольких `VLAN`. На этом интерфейсе были созданы следующие виртуальные подынтерфейсы, каждый из которых соответствует своему `VLAN ID` и представляет шлюз в соответствующую подсеть:

*   `vlan100` (`VLAN ID 100`): Для сети сервера `HQ-SRV`.
*   `vlan200` (`VLAN ID 200`): Для сети клиента `HQ-CLI`.
*   `vlan999` (`VLAN ID 999`, **пример**): Для сети управления.
    *   **Примечание:** Согласно специальным инструкциям для экзамена, для сетей HQ-SRV и HQ-CLI были использованы **фиксированные VLAN ID 100 и 200 соответственно**, независимо от значений, указанных в конкретном варианте задания, для обеспечения совместимости с преднастроенным стендом. VLAN ID для сети Управления (в данном примере 999) **должен быть взят из вашего варианта задания**.
**Основные шаги конфигурации:**

1.  **Создание директорий конфигурации:** Для каждого VLAN-подынтерфейса (`vlan100`, `vlan200`, `vlan999`) была создана соответствующая директория в `/etc/net/ifaces/`.
2.  **Определение параметров VLAN:** В файле `options` для каждого подынтерфейса указан тип `vlan`, базовый физический интерфейс (`HOST=ens19`) и идентификатор VLAN (`VID=100`, `VID=200`, `VID=999`).
3.  **Назначение IP-адресов:** В файле `ipv4address` для каждого VLAN-подынтерфейса был настроен статический IP-адрес и маска подсети, соответствующая требованиям задания (или размерам подсетей из **Таблицы 1** данного пособия):
    *   `vlan100`: `192.168.1.1/26` // ВАРИАТИВНО: Маска может отличаться.
    *   `vlan200`: `192.168.2.1/28` // ВАРИАТИВНО: Маска может отличаться.
    *   `vlan999`: `192.168.99.1/29` // ВАРИАТИВНО: Маска может отличаться.
4.  **Применение конфигурации:** Сетевая служба была перезапущена (`systemctl restart network`) для активации созданных интерфейсов и применения настроек.

**Схема 1. Визуализация конфигурации VLAN на HQ-RTR**

```text
+--------------------------------------------+
| Маршрутизатор HQ-RTR                       |
|--------------------------------------------|
| Физический интерфейс ens19 (Trunk)         | ----> К коммутатору/устройствам HQ
|--------------------------------------------|
|   |       |       |                        |
|   |       |       +-- VLAN 999 (Упр.)      |
|   |       |           IP: 192.168.99.1/29  |
|   |       |                                |
|   |       +---------- VLAN 200 (Клиент)    | ----> Сеть HQ-CLI
|   |                   IP: 192.168.2.1/28   |
|   |                                        |
|   +------------------ VLAN 100 (Сервер)    | ----> Сеть HQ-SRV
|                       IP: 192.168.1.1/26   |
|                                            |
+--------------------------------------------+
```

**Итог:**
В результате выполненных настроек на маршрутизаторе `HQ-RTR` реализована виртуальная коммутация на базе `VLAN`. Интерфейс `ens19` функционирует как транковый порт, а созданные VLAN-подынтерфейсы (`vlan100`, `vlan200`, `vlan999`) обеспечивают маршрутизацию между соответствующими подсетями и остальной частью сети. Трафик между сервером, клиентом и сетью управления логически изолирован на канальном уровне. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 4: Настройка VLAN на HQ-RTR

**Цель:** Убедиться, что VLAN-подынтерфейсы на `HQ-RTR` созданы, активны и имеют правильные IP-адреса, а также проверить связность с устройствами в этих VLAN (теперь пинги из Шага 1.2 и проверка интернет-доступа из Шага 8 должны работать для `HQ-SRV`/`HQ-CLI`).

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Проверка статуса и IP VLAN-интерфейсов ---
ip -br addr show type vlan
# Ожидаемый результат: Строки для `vlan100@ens19`, `vlan200@ens19`, `vlan999@ens19` со статусом `UP` и IP-адресами `192.168.1.1/26`, `192.168.2.1/28`, `192.168.99.1/29`. // ВАРИАТИВНО: Маски могут отличаться.

# [root@hq-rtr ~]# --- Проверка связности с устройствами в VLAN ---
 # Пингуем IP-адрес интерфейса ens18 на HQ-SRV
ping -c 3 192.168.1.10
# Ожидаемый результат: Успешные ответы.
 # Пингуем IP-адрес интерфейса ens18 на HQ-CLI (с временным IP)
ping -c 3 192.168.2.10
# Ожидаемый результат: Успешные ответы.
```

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Проверка связности с шлюзом VLAN100 ---
ping -c 3 192.168.1.1
# Ожидаемый результат: Успешные ответы.
# [root@hq-srv ~]# --- Проверка доступа в Интернет (заработает после Шага 8) ---
# ping -c 3 8.8.8.8
# Ожидаемый результат: Успешные ответы.
```

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Проверка связности с шлюзом VLAN200 ---
ping -c 3 192.168.2.1
# Ожидаемый результат: Успешные ответы.
# [root@hq-cli ~]# --- Проверка доступа в Интернет (заработает после Шага 8) ---
# ping -c 3 8.8.8.8
# Ожидаемый результат: Успешные ответы.
```

---

## Шаг 5: Настройка безопасного удалённого доступа (SSH)

**Цель:** Настроить службу SSH (Secure Shell) на серверах `HQ-SRV` и `BR-SRV` для безопасного удалённого администрирования. Будет изменён стандартный порт, ограничен доступ по пользователю и количеству попыток входа, а также добавлен баннер.

**Практическое назначение:** Изменение стандартного порта SSH (22) снижает количество автоматических атак. Ограничение доступа по пользователям (`AllowUsers`) и количеству попыток (`MaxAuthTries`) повышает безопасность. Баннер (`Banner`) служит предупреждением для подключающихся пользователей.

**Метод:** Редактирование конфигурационного файла `/etc/openssh/sshd_config` с помощью `sed` и `tee`, создание файла баннера и перезапуск службы `sshd`.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Проверка/Установка openssh-server ---
# apt-get update && apt-get install -y openssh-server # openssh-server уже установлен

# [root@hq-srv ~]# --- Включение и запуск службы sshd ---
systemctl enable --now sshd

# [root@hq-srv ~]# --- Изменение порта SSH на 2024 ---
# // ВАРИАТИВНО: Порт SSH (здесь 2024) может отличаться (например, 3015). Используйте порт из вашего варианта задания.
# Находим строку 'Port 22' (даже закомментированную) и заменяем на 'Port 2024'.
sed -i 's/^#*[[:space:]]*Port[[:space:]]\+22/Port 2024/' /etc/openssh/sshd_config
# [Проверка (опционально): порт]
# grep '^Port ' /etc/openssh/sshd_config

# [root@hq-srv ~]# --- Ограничение доступа (только sshuser) ---
# Добавляем директиву AllowUsers в конец файла.
tee -a /etc/openssh/sshd_config > /dev/null <<'EOF'

AllowUsers sshuser
EOF
# [Проверка (опционально): правило AllowUsers]
# tail -n 3 /etc/openssh/sshd_config | grep AllowUsers

# [root@hq-srv ~]# --- Ограничение количества попыток аутентификации ---
# Находим строку 'MaxAuthTries' (даже закомментированную) и устанавливаем значение 2.
sed -i 's/^#*MaxAuthTries.*/MaxAuthTries 2/' /etc/openssh/sshd_config
# [Проверка (опционально): правило MaxAuthTries]
# grep '^MaxAuthTries' /etc/openssh/sshd_config

# [root@hq-srv ~]# --- Настройка баннера ---
# Создаём файл /etc/openssh/banner с текстом приветствия.
echo 'Authorized access only' > /etc/openssh/banner
# Находим строку 'Banner' (даже закомментированную) и указываем путь к файлу баннера.
sed -i 's|^#*Banner.*|Banner /etc/openssh/banner|' /etc/openssh/sshd_config
# [Проверка (опционально): правило Banner]
# grep '^Banner' /etc/openssh/sshd_config

# [root@hq-srv ~]# --- Перезапуск службы SSH ---
# Применяем все изменения.
systemctl restart sshd
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка/Установка openssh-server ---
# apt-get update && apt-get install -y openssh-server # openssh-server уже установлен

# [root@br-srv ~]# --- Включение и запуск службы sshd ---
systemctl enable --now sshd

# [root@br-srv ~]# --- Изменение порта SSH на 2024 ---
# // ВАРИАТИВНО: Порт SSH (здесь 2024) может отличаться (например, 3015). Используйте порт из вашего варианта задания.
sed -i 's/^#*[[:space:]]*Port[[:space:]]\+22/Port 2024/' /etc/openssh/sshd_config
# [Проверка (опционально): порт]
# grep '^Port ' /etc/openssh/sshd_config

# [root@br-srv ~]# --- Ограничение доступа (только sshuser) ---
tee -a /etc/openssh/sshd_config > /dev/null <<'EOF'

AllowUsers sshuser
EOF
# [Проверка (опционально): правило AllowUsers]
# tail -n 3 /etc/openssh/sshd_config | grep AllowUsers

# [root@br-srv ~]# --- Ограничение количества попыток аутентификации ---
sed -i 's/^#*MaxAuthTries.*/MaxAuthTries 2/' /etc/openssh/sshd_config
# [Проверка (опционально): правило MaxAuthTries]
# grep '^MaxAuthTries' /etc/openssh/sshd_config

# [root@br-srv ~]# --- Настройка баннера ---
echo 'Authorized access only' > /etc/openssh/banner
sed -i 's|^#*Banner.*|Banner /etc/openssh/banner|' /etc/openssh/sshd_config
# [Проверка (опционально): правило Banner]
# grep '^Banner' /etc/openssh/sshd_config

# [root@br-srv ~]# --- Перезапуск службы SSH ---
systemctl restart sshd
```

### Проверка шага 5: Настройка безопасного удалённого доступа (SSH)

**Цель:** Проверить, что служба SSH на `HQ-SRV` и `BR-SRV` запущена, слушает на порту 2024, и основные параметры безопасности (AllowUsers, MaxAuthTries, Banner) применены.

#### HQ-SRV и BR-SRV

```bash
# [root@hostname ~]# --- Проверка статуса sshd ---
systemctl is-active sshd
# Ожидаемый результат: `active`.
# [root@hostname ~]# --- Проверка порта 2024 ---
# // ВАРИАТИВНО: Проверяйте тот порт, который вы настроили (2024 или другой).
ss -tlpn | grep :2024
# Ожидаемый результат: `sshd` слушает порт `2024`.
# [root@hostname ~]# --- Проверка активной конфигурации ---
# Показывает параметры, с которыми sshd реально работает.
sshd -T | egrep '^(port|allowusers|maxauthtries|banner)'
# Ожидаемый результат: Отображение `port 2024`, `maxauthtries 2`, `banner /etc/openssh/banner`, `allowusers sshuser`. // ВАРИАТИВНО: Port может отличаться.
```

#### Тестирование подключения (с HQ-RTR или BR-RTR)

**(Выполнять после завершения настройки OSPF - Шаг 7)**

```bash
# [root@hq-rtr ~]# --- Тест подключения к HQ-SRV ---
# Используем опцию -o ConnectTimeout для ограничения времени ожидания.
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh sshuser@192.168.1.10 -p 2024 -o ConnectTimeout=5
# Ожидаемый результат: Появление баннера "Authorized access only",
# запрос пароля для sshuser, успешный вход после ввода 'P@ssw0rd'. (Введите 'exit' для выхода).
exit

# [root@br-rtr ~]# --- Тест подключения к BR-SRV ---
# OSPF (Шаг 7) обеспечивает маршрут.
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh sshuser@192.168.3.10 -p 2024 -o ConnectTimeout=5
# Ожидаемый результат: Появление баннера, запрос пароля 'P@ssw0rd', вход. (Введите 'exit').
exit

# [root@hq-rtr ~]# --- Тест запрещённого пользователя (root) ---
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh root@192.168.1.10 -p 2024 -o ConnectTimeout=5
# Ожидаемый результат: Быстрое закрытие соединения ("Permission denied" или "Connection closed").
exit

# [root@br-rtr ~]# --- Тест запрещённого пользователя (root) ---
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh sshuser@192.168.3.10 -p 2024 -o ConnectTimeout=5
# Ожидаемый результат: Быстрое закрытие соединения ("Permission denied" или "Connection closed").
exit

# [root@hq-rtr ~]# --- Тест MaxAuthTries ---
# Попробуйте подключиться к одному из серверов и введите неверный пароль дважды.
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh sshuser@192.168.1.10 -p 2024 -o ConnectTimeout=5
# Ожидаемый результат: После второй неудачной попытки соединение должно разорваться.
exit

# [root@br-rtr ~]# --- Тест MaxAuthTries ---
# Попробуйте подключиться к одному из серверов и введите неверный пароль дважды.
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh sshuser@192.168.3.10 -p 2024 -o ConnectTimeout=5
# Ожидаемый результат: После второй неудачной попытки соединение должно разорваться.
exit
```

---

## Шаг 6: Настройка IP-туннеля (GRE)

**Цель:** Создать защищённый канал связи (IP-туннель) между главным офисом (HQ) и филиалом (BR) поверх сети ISP, используя технологию **GRE (Generic Routing Encapsulation)**. Это позволит маршрутизировать трафик между локальными сетями офисов.

**Практическое назначение:** GRE является стандартным протоколом туннелирования (RFC 2784), который инкапсулирует пакеты IP (и других протоколов) внутри IP-пакетов. Он относительно прост в настройке и широко поддерживается.

**Метод:** Создание виртуального интерфейса `gre1` на каждом маршрутизаторе (`HQ-RTR`, `BR-RTR`) с настройкой локальной (`TUNLOCAL`) и удалённой (`TUNREMOTE`) точек туннеля (внешние IP-адреса согласно Таблице 1), типа туннеля (`TUNTYPE=gre`) и назначением IP-адреса (`192.168.5.0/30`) самому туннельному интерфейсу.

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Конфигурация GRE-туннеля gre1 ---
# Создаём директорию для настроек интерфейса gre1.
mkdir -p /etc/net/ifaces/gre1
# Настраиваем IP-адрес и маску для туннельного интерфейса gre1.
echo '192.168.5.1/30' > /etc/net/ifaces/gre1/ipv4address
# Создаём файл 'options' для gre1 с параметрами туннеля.
cat <<'EOF' > /etc/net/ifaces/gre1/options
# Тип интерфейса - IP туннель
TYPE=iptun
# Тип туннеля - GRE
TUNTYPE=gre
# Локальная точка туннеля (внешний IP HQ-RTR на ens18)
TUNLOCAL=172.16.4.4
# Удалённая точка туннеля (внешний IP BR-RTR на ens18)
TUNREMOTE=172.16.5.5
# Время жизни (TTL) пакетов туннеля
TUNTTL=64
# Дополнительные опции
TUNOPTIONS='ttl 64'
EOF

# [root@hq-rtr ~]# --- Перезапуск сетевой службы ---
# Применяем конфигурацию, создавая интерфейс gre1.
systemctl restart network
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Конфигурация GRE-туннеля gre1 ---
# Создаём директорию для настроек интерфейса gre1.
mkdir -p /etc/net/ifaces/gre1
# Настраиваем IP-адрес и маску для туннельного интерфейса gre1.
echo '192.168.5.2/30' > /etc/net/ifaces/gre1/ipv4address
# Создаём файл 'options' для gre1 с параметрами туннеля.
cat <<'EOF' > /etc/net/ifaces/gre1/options
# Тип интерфейса - IP туннель
TYPE=iptun
# Тип туннеля - GRE
TUNTYPE=gre
# Локальная точка туннеля (внешний IP BR-RTR на ens18)
TUNLOCAL=172.16.5.5
# Удалённая точка туннеля (внешний IP HQ-RTR на ens18)
TUNREMOTE=172.16.4.4
# Время жизни (TTL) пакетов туннеля
TUNTTL=64
# Дополнительные опции
TUNOPTIONS='ttl 64'
EOF

# [root@br-rtr ~]# --- Перезапуск сетевой службы ---
# Применяем конфигурацию, создавая интерфейс gre1.
systemctl restart network
```

### Пример отчёта: настройка IP-туннеля (GRE) между HQ-RTR и BR-RTR

**Цель:**
Обеспечить безопасное и надёжное соединение между локальными сетями главного офиса (HQ) и филиала (BR) через публичную сеть (Интернет/ISP), используя технологию IP-туннелирования. Это позволит маршрутизировать трафик между сетями `192.168.1.0/26`, `192.168.2.0/28`, `192.168.99.0/29` (HQ) и `192.168.3.0/27` (BR) так, как если бы они были напрямую соединены.

**Выбор реализации:**
Для создания туннеля между маршрутизаторами `HQ-RTR` и `BR-RTR` была выбрана технология **GRE (Generic Routing Encapsulation)**. GRE является стандартным протоколом туннелирования, который инкапсулирует пакеты различных сетевых протоколов внутри IP-пакетов. Он был выбран из-за его простоты, поддержки мультикаст-трафика и широкой совместимости. Туннель будет использовать IP-адреса внешних интерфейсов маршрутизаторов (`172.16.4.4` для HQ-RTR и `172.16.5.5` для BR-RTR, оба на интерфейсах `ens18`) в качестве конечных точек. Для самого туннельного интерфейса (`gre1`) выделена отдельная подсеть `192.168.5.0/30`.

**Основные шаги конфигурации:**

1.  **Создание директорий конфигурации:** На обоих маршрутизаторах (`HQ-RTR` и `BR-RTR`) была создана директория `/etc/net/ifaces/gre1`.
2.  **Назначение IP-адресов туннелю:** В файле `ipv4address` для интерфейса `gre1` были настроены статические IP-адреса из подсети `192.168.5.0/30`:
    *   `HQ-RTR`: `192.168.5.1/30`
    *   `BR-RTR`: `192.168.5.2/30`
3.  **Определение параметров GRE-туннеля:** В файле `options` для интерфейса `gre1` на каждом маршрутизаторе были указаны:
    *   Тип интерфейса: `TYPE=iptun`
    *   Тип туннеля: `TUNTYPE=gre`
    *   Локальный IP-адрес: `TUNLOCAL` (внешний IP маршрутизатора на `ens18`)
    *   Удалённый IP-адрес: `TUNREMOTE` (внешний IP другого маршрутизатора на `ens18`)
    *   TTL для туннельных пакетов: `TUNTTL=64`
4.  **Применение конфигурации:** Сетевая служба была перезапущена (`systemctl restart network`) на обоих маршрутизаторах.

**Схема 2. Визуализация конфигурации GRE-туннеля**

```text
          +--------------------+
          |        ISP         |  (Сеть провайдера)
          +--------------------+
          /                     \
(Сеть 172.16.4.0/28)  (Сеть 172.16.5.0/28)
        /                         \
+----------------+        +----------------+
|   HQ-RTR       |        |   BR-RTR       |
| IP: 172.16.4.4 |        | IP: 172.16.5.5 | (на ens18)
| (на ens18)     |        |                |
|----------------|========|----------------|  <-- GRE-Туннель (Сеть 192.168.5.0/30)
|  gre1:         |        |  gre1:         |      (через сеть ISP)
| 192.168.5.1    |        | 192.168.5.2    |
+----------------+        +----------------+
       |                         |
(К локальной              (К локальной
 сети HQ)                  сети BR)
```

**Итог:**
В результате настройки между `HQ-RTR` и `BR-RTR` создан GRE-туннель. Виртуальные интерфейсы `gre1` на обоих маршрутизаторах имеют IP-адреса и могут использоваться для маршрутизации трафика между локальными сетями HQ и BR через публичную сеть ISP. Это создаёт основу для дальнейшей настройки маршрутизации между офисами. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 6: Настройка IP-туннеля (GRE)

**Цель:** Проверить, что интерфейс GRE-туннеля `gre1` создан на обоих маршрутизаторах (`HQ-RTR`, `BR-RTR`), активен, имеет правильный IP-адрес и обеспечивает прямую связность между концами туннеля.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Проверка статуса и IP gre1 ---
ip -br addr show gre1
# Ожидаемый результат: Строка вида `gre1@NONE UP 192.168.5.1/30 ...`. Статус должен быть `UP`.
# [root@hq-rtr ~]# --- Пинг другого конца туннеля (BR-RTR) ---
ping -c 3 192.168.5.2
# Ожидаемый результат: Успешные ответы от 192.168.5.2. **Это ключевая проверка.**
# [root@hq-rtr ~]# --- Проверка параметров туннеля (опционально) ---
ip tunnel show gre1
# Ожидаемый результат: Вывод должен содержать `local 172.16.4.4 remote 172.16.5.5 ttl 64 ...`.
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Проверка статуса и IP gre1 ---
ip -br addr show gre1
# Ожидаемый результат: Строка вида `gre1@NONE UP 192.168.5.2/30 ...`. Статус должен быть `UP`.
# [root@br-rtr ~]# --- Пинг другого конца туннеля (HQ-RTR) ---
ping -c 3 192.168.5.1
# Ожидаемый результат: Успешные ответы от 192.168.5.1. **Это ключевая проверка.**
# [root@br-rtr ~]# --- Проверка параметров туннеля (опционально) ---
ip tunnel show gre1
# Ожидаемый результат: Вывод должен содержать `local 172.16.5.5 remote 172.16.4.4 ttl 64 ...`.
```

---

## Шаг 7: Настройка динамической маршрутизации (OSPF)

**Цель:** Автоматизировать обмен маршрутной информацией между `HQ-RTR` и `BR-RTR` через созданный GRE-туннель с помощью протокола **OSPF (Open Shortest Path First)**. Это избавит от необходимости ручной настройки статических маршрутов между локальными сетями офисов.

**Практическое назначение:** OSPF является стандартным протоколом динамической маршрутизации **состояния канала (link-state)**, который эффективно строит карту сети и быстро реагирует на изменения топологии. Использование `frr` (Free Range Routing) предоставляет современную и гибкую реализацию OSPF и других протоколов маршрутизации. Простая парольная аутентификация добавляет базовый уровень защиты протокола.

**Метод:** Включение демона `ospfd` в `frr`, настройка OSPF через файл `/etc/frr/frr.conf` с указанием интерфейсов, зоны, аутентификации, пассивных интерфейсов и Router ID.

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

### Шаг 7.1: Базовая настройка FRR

**Предпосылки:** Пакет `frr` установлен (установка запущена ранее).

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Установка frr (если не установлено) ---
apt-get update && apt-get install -y frr

# [root@hq-rtr ~]# --- Проверка статуса установки frr (если необходимо) ---
# rpm -q frr

# [root@hq-rtr ~]# --- Включение демона OSPF в FRR ---
# Редактируем файл /etc/frr/daemons, заменяя 'ospfd=no' на 'ospfd=yes'.
sed -i 's/ospfd=no/ospfd=yes/g' /etc/frr/daemons
# [Проверка (опционально): конфиг демонов]
# grep 'ospfd=yes' /etc/frr/daemons

# [root@hq-rtr ~]# --- Включение и запуск службы FRR ---
# Включаем автозапуск и запускаем службу FRR.
systemctl enable --now frr
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Установка frr (если не установлено) ---
apt-get update && apt-get install -y frr

# [root@br-rtr ~]# --- Проверка статуса установки frr (если необходимо) ---
# rpm -q frr

# [root@br-rtr ~]# --- Включение демона OSPF в FRR ---
sed -i 's/ospfd=no/ospfd=yes/g' /etc/frr/daemons
# [Проверка (опционально): конфиг демонов]
# grep 'ospfd=yes' /etc/frr/daemons

# [root@br-rtr ~]# --- Включение и запуск службы FRR ---
systemctl enable --now frr
```

### Шаг 7.2: Конфигурация OSPF в `/etc/frr/frr.conf`

**Метод:** Запись конфигурации OSPF через `cat <<'EOF' > /etc/frr/frr.conf`.
*   Включаем OSPF на нужных интерфейсах (`gre1`, `vlanXXX`, `ens19` на BR-RTR) в `area 0.0.0.0`.
*   Настраиваем простую парольную аутентификацию (`P@$$word`) на интерфейсе `gre1`.
*   Используем `passive-interface default` для безопасности (запрет Hello на LAN-интерфейсах).
*   Явно делаем `gre1` **непассивным** (`no ip ospf passive`) для установления соседства.
*   Задаём уникальные `router-id` (IP-адреса GRE-интерфейсов) для каждого маршрутизатора.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Настройка /etc/frr/frr.conf ---
# Создаём конфигурацию OSPF для HQ-RTR.
cat <<'EOF' > /etc/frr/frr.conf
! Конфигурация OSPF для HQ-RTR
hostname hq-rtr.au-team.irpo
log stdout
!
interface gre1
 description Tunnel to BR-RTR
 ip ospf area 0.0.0.0
 ip ospf authentication
 ip ospf authentication-key P@$$word
 no ip ospf passive
 exit
!
interface vlan100
 description LAN HQ-SRV
 ip ospf area 0.0.0.0
 exit
!
interface vlan200
 description LAN HQ-CLI
 ip ospf area 0.0.0.0
 exit
!
interface vlan999
 description Management LAN
 ip ospf area 0.0.0.0
 exit
!
router ospf
 passive-interface default
 router-id 192.168.5.1
 exit
!
line vty
!
EOF
# [Проверка (опционально): синтаксис конфига FRR]
# vtysh -c 'show running-config' || echo "Ошибка синтаксиса FRR"

# [root@hq-rtr ~]# --- Перезапуск службы FRR ---
# Применяем конфигурацию OSPF.
systemctl restart frr
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Настройка /etc/frr/frr.conf ---
# Создаём конфигурацию OSPF для BR-RTR.
cat <<'EOF' > /etc/frr/frr.conf
! Конфигурация OSPF для BR-RTR
hostname br-rtr.au-team.irpo
log stdout
!
interface gre1
 description Tunnel to HQ-RTR
 ip ospf area 0.0.0.0
 ip ospf authentication
 ip ospf authentication-key P@$$word
 no ip ospf passive
 exit
!
interface ens19
 description LAN BR-SRV
 ip ospf area 0.0.0.0
 exit
!
router ospf
 passive-interface default
 router-id 192.168.5.2
 exit
!
line vty
!
EOF
# [Проверка (опционально): синтаксис конфига FRR]
# vtysh -c 'show running-config' || echo "Ошибка синтаксиса FRR"

# [root@br-rtr ~]# --- Перезапуск службы FRR ---
# Применяем конфигурацию OSPF.
systemctl restart frr
```

### Пример отчёта: настройка динамической маршрутизации (OSPF) между HQ-RTR и BR-RTR

**Цель:**
Обеспечить автоматический обмен информацией о маршрутах между главным офисом (HQ) и филиалом (BR) для того, чтобы ресурсы (сети) одного офиса были доступны из другого. Это реализуется с помощью протокола динамической маршрутизации поверх ранее настроенного GRE-туннеля.

**Выбор реализации:**
Для динамической маршрутизации был выбран протокол **OSPF (Open Shortest Path First)**. OSPF является **link-state** протоколом, что соответствует требованию задания. Он широко используется, стандартизирован (RFC 2328) и эффективно строит карту сети для выбора оптимальных маршрутов. Маршрутизация будет осуществляться исключительно через GRE-туннель (`gre1`), связывающий `HQ-RTR` и `BR-RTR`. Для реализации используется пакет `frr`.

**Основные шаги конфигурации и защита:**

1.  **Установка FRR:** На оба маршрутизатора (`HQ-RTR` и `BR-RTR`) был установлен пакет Free Range Routing (`frr`).
2.  **Включение OSPF:** В конфигурации FRR (`/etc/frr/daemons`) был активирован демон `ospfd`.
3.  **Настройка OSPF (`/etc/frr/frr.conf`):**
    *   Заданы уникальные **Router ID** (`192.168.5.1` на `HQ-RTR`, `192.168.5.2` на `BR-RTR`).
    *   Интерфейсы `gre1` и локальные (`vlanXXX` на HQ, `ens19` на BR) включены в `Area 0.0.0.0` (`ip ospf area 0.0.0.0`).
    *   Установлена опция `passive-interface default` для запрета установления соседства на локальных интерфейсах по соображениям безопасности.
    *   Интерфейс `gre1` сделан активным (`no ip ospf passive`) для разрешения установления OSPF-соседства через туннель.
4.  **Защита протокола:** На интерфейсе `gre1` на **обоих** маршрутизаторах настроена **простая парольная аутентификация** (`ip ospf authentication`) с **одинаковым** ключом `P@$$word` (`ip ospf authentication-key P@$$word`). Это предотвращает установление неавторизованного соседства.
5.  **Применение конфигурации:** Служба `frr` перезапущена (`systemctl restart frr`).

**Схема 3. Визуализация конфигурации OSPF**

```text
+-----------------+         (OSPF Area 0.0.0.0)        +-----------------+
|    HQ-RTR       | <---- Auth: Simple, P@$$word ----> |    BR-RTR       |
| Router ID:      |                                    | Router ID:      |
|   192.168.5.1   |                                    |   192.168.5.2   |
|-----------------|            GRE-туннель             |-----------------|
| gre1: (Active)  |           (192.168.5.0/30)         | gre1: (Active)  |
| 192.168.5.1/30  |                                    | 192.168.5.2/30  |
+-----------------+                                    +-----------------+
         |                                                      |
         v                                                      v
 Анонсирует сети:                                        Анонсирует сети:
 (Интерфейсы Passive)                                     (Интерфейс Passive)
 - 192.168.1.0/26 (vlan100)                               - 192.168.3.0/27 (ens19)
 - 192.168.2.0/28 (vlan200)
 - 192.168.99.0/29 (vlan999)
 // ВАРИАТИВНО: Маски могут отличаться.
```

**Итог:**
На маршрутизаторах `HQ-RTR` и `BR-RTR` настроена динамическая маршрутизация OSPF, работающая через GRE-туннель с использованием парольной аутентификации. Маршрутизаторы должны обмениваться информацией о доступных локальных сетях, обеспечивая сквозную связность между офисами HQ и BR. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 7: Настройка динамической маршрутизации (OSPF)

**Цель:** Проверить, что служба FRR запущена, демон OSPF работает, соседство OSPF между `HQ-RTR` и `BR-RTR` установлено через `gre1` (состояние `Full`), и маршрутизаторы обменялись маршрутами к локальным сетям друг друга.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Проверка статуса FRR ---
systemctl is-active frr
# Ожидаемый результат: `active`.

# [root@hq-rtr ~]# --- Проверка OSPF через vtysh ---
# Входим в интерактивную оболочку FRR
vtysh
# Внутри vtysh: проверяем соседей OSPF
show ip ospf neighbor
# Ожидаемый результат: Строка с Neighbor ID `192.168.5.2`, State `Full/...`, Interface `gre1`.
# Внутри vtysh: проверяем маршруты, полученные по OSPF
show ip route ospf
# Ожидаемый результат: Маршрут к `192.168.3.0/27` через `192.168.5.2`. // ВАРИАТИВНО: Маска может отличаться.
# Выходим из vtysh
exit

# [root@hq-rtr ~]# --- Проверка системной таблицы маршрутизации ---
# Ищем маршруты, добавленные протоколом OSPF.
ip route show proto ospf
# Ожидаемый результат: Маршрут к `192.168.3.0/27` через `192.168.5.2`. // ВАРИАТИВНО: Маска может отличаться.
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Проверка статуса FRR ---
systemctl is-active frr
# Ожидаемый результат: `active`.

# [root@br-rtr ~]# --- Проверка OSPF через vtysh ---
vtysh
# Внутри vtysh: проверяем соседей OSPF
show ip ospf neighbor
# Ожидаемый результат: Строка с Neighbor ID `192.168.5.1`, State `Full/...`, Interface `gre1`.
# Внутри vtysh: проверяем маршруты, полученные по OSPF
show ip route ospf
# Ожидаемый результат: Маршруты к `192.168.1.0/26`, `192.168.2.0/28`, `192.168.99.0/29` через `192.168.5.1`. // ВАРИАТИВНО: Маски могут отличаться.
# Выходим из vtysh
exit

# [root@br-rtr ~]# --- Проверка системной таблицы маршрутизации ---
ip route show proto ospf
# Ожидаемый результат: Маршруты к сетям HQ через `192.168.5.1`.
```

#### Проверка сквозной связности между локальными сетями

```bash
# [root@hq-srv ~]# --- Проверка с HQ-SRV ---
 # Пингуем IP-адрес интерфейса ens18 на BR-SRV
ping -c 3 192.168.3.10
# Ожидаемый результат: Успешные ответы.

# [root@br-srv ~]# --- Проверка с BR-SRV ---
 # Пингуем IP-адрес интерфейса ens18 на HQ-SRV
ping -c 3 192.168.1.10
# Ожидаемый результат: Успешные ответы.
 # Пингуем IP-адрес интерфейса HQ-CLI
ping -c 3 192.168.2.10
# Ожидаемый результат: Успешные ответы.
```

#### Возможные проблемы и их решения

*   **OSPF соседство не устанавливается (не `Full`):**
    *   Проверьте связность `ping` между концами GRE-туннеля (`192.168.5.1`/`2`).
    *   Убедитесь в наличии `no ip ospf passive` на `gre1` и `passive-interface default` глобально на обоих маршрутизаторах.
    *   Проверьте совпадение пароля аутентификации (`P@$$word`).
    *   Проверьте отсутствие правил `iptables`, блокирующих GRE (протокол 47) или OSPF (протокол 89). Проверьте логи `frr` (`journalctl -u frr`).
*   **Маршруты не появляются:**
    *   Убедитесь, что соседство OSPF установлено (`show ip ospf neighbor`).
    *   Проверьте, что локальные интерфейсы включены в OSPF (`ip ospf area 0.0.0.0`) в конфигурации `frr`.

---

## Шаг 8: Настройка динамической трансляции адресов (NAT)

**Цель:** Обеспечить доступ к сети Интернет для всех устройств в локальных сетях обоих офисов (HQ и BR) через внешние IP-адреса маршрутизаторов `HQ-RTR` и `BR-RTR` соответственно. Дополнительно настроить **TCP MSS Clamping** на GRE-интерфейсах для предотвращения проблем с MTU при передаче данных через туннель.

**Практическое назначение:** NAT (Masquerade) необходим для выхода устройств с приватными IP в публичную сеть. TCP MSS Clamping решает распространённую проблему с производительностью TCP-соединений через туннели с уменьшенным MTU.

**Метод:** Использование `iptables` для настройки правил `MASQUERADE` в цепочке `POSTROUTING` таблицы `nat` и правил `TCPMSS --clamp-mss-to-pmtu` в цепочке `FORWARD` таблицы `mangle`. Правила сохраняются для автозагрузки.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Очистка правил NAT и Mangle, установка разрешающих политик ---
# Очищаем цепочку POSTROUTING таблицы nat.
iptables -t nat -F POSTROUTING
# Очищаем цепочку FORWARD таблицы mangle (где будет TCPMSS правило).
iptables -t mangle -F FORWARD
# Устанавливаем разрешающие политики по умолчанию (для простоты).
iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -P OUTPUT ACCEPT

# [root@hq-rtr ~]# --- Настройка NAT (Masquerade) для выхода в Интернет ---
# Трафик, уходящий через внешний интерфейс ens18, будет маскироваться.
iptables -t nat -A POSTROUTING -o ens18 -j MASQUERADE

# [root@hq-rtr ~]# --- Настройка TCP MSS Clamping для GRE туннеля ---
# -t mangle: Работаем с таблицей mangle для изменения пакетов.
# -A FORWARD: Правило для транзитных пакетов.
# -o gre1: Для пакетов, уходящих через gre1.
# -p tcp --tcp-flags SYN,RST SYN: Ловить только TCP SYN пакеты.
# -j TCPMSS --clamp-mss-to-pmtu: Действие - установить MSS равным Path MTU минус заголовки TCP/IP.
iptables -t mangle -A FORWARD -o gre1 -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu

# [root@hq-rtr ~]# --- Сохранение правил iptables ---
# Сохраняем текущие правила (NAT, Mangle, политики Filter) в файл.
# Важно: Это сохранит и правила DNAT из Шага 6 Модуля 2, если он был выполнен перед этим.
iptables-save > /etc/sysconfig/iptables
# [Проверка (опционально): сохранённые правила]
# cat /etc/sysconfig/iptables | grep -E 'MASQUERADE|TCPMSS'

# [root@hq-rtr ~]# --- Включение и перезапуск службы iptables ---
# Включаем автозапуск и перезапускаем службу для загрузки сохранённых правил.
systemctl enable --now iptables
systemctl restart iptables
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Очистка правил NAT и Mangle, установка разрешающих политик ---
iptables -t nat -F POSTROUTING
iptables -t mangle -F FORWARD
iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -P OUTPUT ACCEPT

# [root@br-rtr ~]# --- Настройка NAT (Masquerade) для выхода в Интернет ---
# Трафик, уходящий через внешний интерфейс ens18, будет маскироваться.
iptables -t nat -A POSTROUTING -o ens18 -j MASQUERADE

# [root@br-rtr ~]# --- Настройка TCP MSS Clamping для GRE туннеля ---
iptables -t mangle -A FORWARD -o gre1 -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu

# [root@br-rtr ~]# --- Сохранение правил iptables ---
# Важно: Это сохранит и правила DNAT из Шага 6 Модуля 2, если он был выполнен перед этим.
iptables-save > /etc/sysconfig/iptables
# [Проверка (опционально): сохранённые правила]
# cat /etc/sysconfig/iptables | grep -E 'MASQUERADE|TCPMSS'

# [root@br-rtr ~]# --- Включение и перезапуск службы iptables ---
systemctl enable --now iptables
systemctl restart iptables
```

### Проверка шага 8: Настройка динамической трансляции адресов (NAT)

**Цель:** Проверить, что правила NAT (Masquerade) и TCP MSS Clamping корректно настроены и сохранены на маршрутизаторах `HQ-RTR` и `BR-RTR`, а также убедиться, что устройства из локальных сетей могут выходить в Интернет.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Проверка правила NAT (Masquerade) ---
iptables -t nat -L POSTROUTING -nv | grep MASQUERADE
# Ожидаемый результат: Строка с target `MASQUERADE` для out-interface `ens18`.

# [root@hq-rtr ~]# --- Проверка правила TCP MSS Clamping ---
iptables -t mangle -L FORWARD -nv | grep TCPMSS
# Ожидаемый результат: Строка с target `TCPMSS clamp to pmtu` для out-interface `gre1`, протокол tcp, флаги SYN.
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Проверка правила NAT (Masquerade) ---
iptables -t nat -L POSTROUTING -nv | grep MASQUERADE
# Ожидаемый результат: Строка с target `MASQUERADE` для out-interface `ens18`.

# [root@br-rtr ~]# --- Проверка правила TCP MSS Clamping ---
iptables -t mangle -L FORWARD -nv | grep TCPMSS
# Ожидаемый результат: Строка с target `TCPMSS clamp to pmtu` для out-interface `gre1`, протокол tcp, флаги SYN.
```

#### Проверка доступа в Интернет из локальных сетей

```bash
# [root@hq-srv ~]# --- Проверка с HQ-SRV ---
ping -c 3 8.8.8.8
# Ожидаемый результат: Успешные ответы.
# Если DNS настроен (Шаг 10): curl -I http://example.com

# [root@hq-cli ~]# --- Проверка с HQ-CLI ---
ping -c 3 8.8.8.8
# Ожидаемый результат: Успешные ответы.
# Если DNS настроен: curl -I http://example.com

# [root@br-srv ~]# --- Проверка с BR-SRV ---
ping -c 3 8.8.8.8
# Ожидаемый результат: Успешные ответы.
# Если DNS настроен: curl -I http://example.com
```

---

## Шаг 9: Настройка протокола динамической конфигурации хостов (DHCP)

**Цель:** Автоматизировать выдачу IP-адресов и других сетевых настроек (шлюз, DNS-сервер, DNS-суффикс) для клиентской машины `HQ-CLI` в сети VLAN 200, используя **статическую аренду по Client ID** для гарантированного присвоения адреса `192.168.2.10`.

**Практическое назначение:** DHCP упрощает подключение клиентов. Статическая аренда по Client ID обеспечивает предсказуемость адреса для `HQ-CLI` независимо от MAC-адреса.

**Метод:** Настройка службы `dnsmasq` на `HQ-RTR` как DHCP-сервера для VLAN 200 с определением статической аренды (`dhcp-host=id:...`). Настройка клиента `HQ-CLI` на отправку заданного Client ID (`hq-cli-exam-id`).

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

### Шаг 9.1: Настройка DHCP-сервера `dnsmasq` на HQ-RTR

**Предпосылки:** Пакет `dnsmasq` установлен (установка запущена ранее). DNS-сервер (`192.168.1.10`) настроен и доступен (Шаг 10).

```bash
# [root@hq-rtr ~]# --- Установка dnsmasq (если не установлено) ---
apt-get update && apt-get install -y dnsmasq

# [root@hq-rtr ~]# --- Проверка статуса установки dnsmasq (если необходимо) ---
# rpm -q dnsmasq

# [root@hq-rtr ~]# --- Резервное копирование оригинального конфига ---
cp /etc/dnsmasq.conf /etc/dnsmasq.conf.bak

# [root@hq-rtr ~]# --- Создание конфигурационного файла dnsmasq ---
# Записываем конфигурацию DHCP-сервера.
cat <<'EOF' > /etc/dnsmasq.conf
# --- Основные настройки DHCP для dnsmasq ---
# Слушать DHCP запросы только на интерфейсе vlan200
interface=vlan200
# Явно указать IP-адрес интерфейса, на котором слушать
listen-address=192.168.2.1
# Отключить встроенный DNS-сервер dnsmasq (он не нужен на роутере)
port=0
# Не читать /etc/resolv.conf на сервере
no-resolv
# Объявить этот сервер авторитетным для данной подсети
dhcp-authoritative

# --- Пул адресов для VLAN200 (192.168.2.0/28) ---
# Диапазон: .2-.14; Маска: /28; Время аренды: 6 часов
# // ВАРИАТИВНО: Размер сети HQ-CLI (влияет на маску 255.255.255.240 и диапазон) может отличаться.
dhcp-range=interface:vlan200,192.168.2.2,192.168.2.14,255.255.255.240,6h

# --- Статическая аренда для HQ-CLI по Client ID ---
# Привязываем Client ID "hq-cli-exam-id" к IP 192.168.2.10 с именем hq-cli и бесконечной арендой.
dhcp-host=id:hq-cli-exam-id,hq-cli,192.168.2.10,infinite

# --- DHCP опции ---
# Опция 3: Шлюз (автоматически = listen-address)
# Опция 6: DNS-сервер (указываем IP HQ-SRV из Шага 10)
dhcp-option=6,192.168.1.10
# Опция 15: DNS-суффикс домена
dhcp-option=15,au-team.irpo
EOF
# [Проверка (опционально): конфиг]
# cat /etc/dnsmasq.conf

# [root@hq-rtr ~]# --- Настройка /etc/resolvconf.conf для корректной работы DNS ---
# Добавляем опции в конец /etc/resolvconf.conf, чтобы предотвратить конфликт с 127.0.0.1 от dnsmasq.
tee -a /etc/resolvconf.conf > /dev/null <<'EOF'

# Дополнительные настройки для resolvconf (из-за dnsmasq в роли DHCP-сервера)
resolv_conf_local_only=NO
deny_interfaces="lo.dnsmasq ens18"
EOF
# [Проверка (опционально): конфиг resolvconf]
# tail -n 3 /etc/resolvconf.conf

# [root@hq-rtr ~]# --- Обновляем /etc/resolv.conf немедленно ---
# Применяем настройки resolvconf (DNS клиентские настройки для самого роутера).
resolvconf -u

# [root@hq-rtr ~]# --- Включение и перезапуск службы dnsmasq ---
# Включаем автозапуск и перезапускаем службу для применения конфигурации DHCP.
systemctl enable --now dnsmasq
systemctl restart dnsmasq
```

### Шаг 9.2: Настройка DHCP-клиента `dhcpcd` на HQ-CLI

**Метод:** Перевод интерфейса `ens18` в режим DHCP (`BOOTPROTO=dhcp`) и настройка клиента `dhcpcd` на отправку специфического `Client ID` (`hq-cli-exam-id`).

```bash
# [root@hq-cli ~]# --- Перевод интерфейса ens18 в режим DHCP ---
# Создаём директорию (если её нет).
mkdir -p /etc/net/ifaces/ens18
# Очищаем предыдущую (временную статическую) конфигурацию.
find /etc/net/ifaces/ens18 -mindepth 1 -delete
# Указываем получение настроек по DHCP.
cat <<'EOF' > /etc/net/ifaces/ens18/options
BOOTPROTO=dhcp
TYPE=eth
EOF
# Удаляем файлы со статическими настройками, если остались.
rm -f /etc/net/ifaces/ens18/ipv4address /etc/net/ifaces/ens18/ipv4route /etc/net/ifaces/ens18/resolv.conf

# [root@hq-cli ~]# --- Настройка dhcpcd для отправки Client ID ---
# Редактируем /etc/dhcpcd.conf:
# Заменяем любую существующую строку 'clientid' (даже закомментированную) на активную 'clientid hq-cli-exam-id'.
sed -i 's/^[[:space:]]*#\?[[:space:]]*clientid.*/clientid hq-cli-exam-id/' /etc/dhcpcd.conf
# Комментируем строку 'duid', чтобы гарантированно использовался 'clientid'.
sed -i 's/^[[:space:]]*duid/#duid/' /etc/dhcpcd.conf
# [Проверка (опционально): конфиг dhcpcd]
# grep '^clientid' /etc/dhcpcd.conf
# grep '^#duid' /etc/dhcpcd.conf

# [root@hq-cli ~]# --- Перезапуск сетевой службы ---
# Применяет настройки интерфейса (DHCP) и заставляет dhcpcd запросить адрес с новым Client ID.
systemctl restart network
```

### Пример отчёта: настройка протокола динамической конфигурации хостов (DHCP) на HQ-RTR

**Цель:**
Автоматизировать процесс назначения сетевых настроек для клиентской машины `HQ-CLI` в сети VLAN 200, гарантировав при этом получение **конкретного IP-адреса (`192.168.2.10`)** независимо от MAC-адреса сетевой карты `HQ-CLI`. Это достигается с помощью привязки статической DHCP-аренды к заранее определённому **идентификатору клиента (Client ID)**.

**Выбор реализации:**
В качестве DHCP-сервера используется `dnsmasq`, установленный на маршрутизаторе `HQ-RTR`. Для обеспечения стабильного IP-адреса для `HQ-CLI` используется механизм **статической аренды по Client ID**. Клиентская машина `HQ-CLI` настраивается на отправку предопределённого Client ID (`hq-cli-exam-id`) при запросе адреса.

**Основные шаги конфигурации:**

1.  **Установка `dnsmasq`:** Пакет `dnsmasq` установлен на `HQ-RTR`.
2.  **Конфигурация `/etc/dnsmasq.conf` на `HQ-RTR`:**
    *   Настроено прослушивание только на интерфейсе `vlan200` (`listen-address=192.168.2.1`).
    *   Определён динамический пул адресов `192.168.2.2` - `192.168.2.14` (`dhcp-range`). *Примечание: Диапазон и маска могут отличаться в зависимости от варианта задания.*
    *   Добавлена **статическая привязка** (`dhcp-host`) для выдачи IP-адреса `192.168.2.10` клиенту с **Client ID `"hq-cli-exam-id"`**.
    *   Настроена передача DHCP-опций: DNS-сервер `192.168.1.10` (опция 6) и DNS-суффикс `au-team.irpo` (опция 15).
3.  **Настройка клиента (`HQ-CLI`):**
    *   Сетевой интерфейс `ens18` настроен на получение адреса по DHCP (`BOOTPROTO=dhcp`).
    *   В файле `/etc/dhcpcd.conf` активирована директива `clientid hq-cli-exam-id` (или другой ID из задания).
4.  **Запуск служб:** Служба `dnsmasq` на `HQ-RTR` включена и перезапущена. Сетевая служба на `HQ-CLI` перезапущена.

**Схема 4. Визуализация процесса DHCP**

```text
+---------------------+      (DHCP Discover + ClientID: "hq-cli-exam-id")        +---------------------+
|    HQ-RTR           | -------------------------------------------------------> |    HQ-CLI           |
| (vlan200)           |                                                          | (ens18, VLAN200)    |
| DHCP-сервер         |   (DHCP Offer - статическая аренда на основе Client ID)  | ClientID настроен   |
| IP: 192.168.2.1     | <------------------------------------------------------- | в /etc/dhcpcd.conf  |
+---------------------+      IP: 192.168.2.10/28 (Фиксированный)                 +---------------------+
                             Шлюз: 192.168.2.1
                             DNS: 192.168.1.10
                             DNS-суффикс: au-team.irpo
```

**Итог:**
На маршрутизаторе `HQ-RTR` настроен DHCP-сервер `dnsmasq`, выдающий IP-адреса в сети VLAN 200. Благодаря статической аренде по **Client ID** и соответствующей настройке DHCP-клиента на `HQ-CLI`, машина `HQ-CLI` гарантированно получает IP-адрес `192.168.2.10` и остальные необходимые сетевые параметры. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 9: Настройка протокола динамической конфигурации хостов (DHCP)

**Цель:** Проверить, что DHCP-сервер `dnsmasq` на `HQ-RTR` работает корректно и что `HQ-CLI` получает ожидаемый статический IP-адрес `192.168.2.10` и другие сетевые настройки (шлюз, DNS) по DHCP с использованием Client ID.

#### DHCP-сервер (HQ-RTR)

```bash
# [root@hq-rtr ~]# --- Проверка статуса dnsmasq ---
systemctl is-active dnsmasq
# Ожидаемый результат: `active`.
# [root@hq-rtr ~]# --- Проверка DHCP порта (UDP 67) ---
ss -ulnp | grep :67
# Ожидаемый результат: `dnsmasq` слушает `192.168.2.1:67`.
# [root@hq-rtr ~]# --- Проверка файла аренд ---
# Даём время клиенту получить адрес после перезапуска сети
sleep 5
systemctl --no-pager -l status dnsmasq.service
# Ожидаемый результат: (в выводе должна быть строка, содержащая следующий или похожий текст)
# `DHCPACK(vlan200) 192.168.2.10 bc:24:11:90:50:ac`
# ---
```

#### DHCP-клиент (HQ-CLI)

```bash
# [root@hq-cli ~]# --- Перезапуск сети (на всякий случай) ---
# systemctl restart network
# sleep 5

# [root@hq-cli ~]# --- Проверка полученного IP-адреса ---
ip -br addr show ens18
# Ожидаемый результат: Строка вида `ens18 UP 192.168.2.10/28 ...`. // ВАРИАТИВНО: Маска может отличаться.

# [root@hq-cli ~]# --- Проверка полученного шлюза ---
ip route show default
# Ожидаемый результат: Строка вида `default via 192.168.2.1 dev ens18 ...`.

# [root@hq-cli ~]# --- Проверка полученных DNS-серверов и домена поиска ---
cat /etc/resolv.conf
# Ожидаемый результат: Файл должен содержать `search au-team.irpo` и `nameserver 192.168.1.10`.
```

---

## Шаг 10: Настройка DNS

**Цель:** Настроить **систему доменных имён (DNS)** для разрешения имён хостов в IP-адреса (и обратно) внутри сети стенда, а также обеспечить разрешение внешних имён через пересылку запросов.

**Практическое назначение:** DNS необходим для удобной работы с устройствами по именам, а не по IP-адресам. `dnsmasq` является легковесным и простым в настройке решением для локального DNS и DHCP.

**Метод:** Настройка `dnsmasq` на `HQ-SRV` для обслуживания зоны `au-team.irpo` и пересылки остальных запросов. Настройка DNS-клиентов на всех остальных машинах для использования `HQ-SRV` в качестве DNS-сервера.

### Шаг 10.1: Настройка DNS-сервера `dnsmasq` на HQ-SRV

**Предпосылки:** Пакет `dnsmasq` установлен (установка запущена ранее). `HQ-SRV` имеет доступ в Интернет (Шаг 8 и Шаг 4).

```bash
# [root@hq-srv ~]# --- Установка dnsmasq (если не установлено) ---
apt-get update && apt-get install -y dnsmasq

# [root@hq-srv ~]# --- Проверка статуса установки dnsmasq (если необходимо) ---
# Убедитесь, что фоновый процесс установки завершён.
# rpm -q dnsmasq

# [root@hq-srv ~]# --- Резервное копирование оригинального конфига ---
cp /etc/dnsmasq.conf /etc/dnsmasq.conf.bak

# [root@hq-srv ~]# --- Создание конфигурационного файла dnsmasq ---
# Записываем конфигурацию DNS-сервера.
cat <<'EOF' > /etc/dnsmasq.conf
# --- Общие настройки DNS ---
# Локальный домен
domain=au-team.irpo
# Интерфейс для прослушивания DNS запросов
interface=ens18
# Адреса для прослушивания (внешний и локальный)
listen-address=127.0.0.1,192.168.1.10
# Не использовать системный resolv.conf для forwarders
no-resolv
# Не использовать системный /etc/hosts
no-hosts
# Размер кэша DNS
cache-size=1000

# --- Серверы пересылки (Upstream DNS) ---
# Общедоступные DNS-серверы для внешних запросов
server=8.8.8.8
server=1.1.1.1

# --- Локальные DNS-записи (A + PTR для HQ, A для BR, CNAME) ---
# Используем host-record для автоматического создания PTR записей для адресов в локальных сетях HQ-RTR
host-record=hq-rtr.au-team.irpo,192.168.1.1
host-record=hq-rtr.au-team.irpo,192.168.2.1
host-record=hq-rtr.au-team.irpo,192.168.99.1
host-record=hq-srv.au-team.irpo,192.168.1.10
host-record=hq-cli.au-team.irpo,192.168.2.10 # Указываем IP, который будет назначен по DHCP
# Для BR используем address, так как сети не локальны для HQ-SRV
# // ВАРИАТИВНО: Может потребоваться PTR запись для br-rtr. В этом случае используйте host-record вместо address.
address=/br-rtr.au-team.irpo/192.168.3.1
address=/br-srv.au-team.irpo/192.168.3.10
# CNAME записи (псевдонимы)
# // ВАРИАТИВНО: Цель и тип записей для moodle/wiki могут отличаться (например, A-записи). Сверьтесь с заданием.
cname=moodle.au-team.irpo,hq-rtr.au-team.irpo
cname=wiki.au-team.irpo,hq-rtr.au-team.irpo
EOF
# [Проверка (опционально): конфиг]
# cat /etc/dnsmasq.conf

# [root@hq-srv ~]# --- Настройка постоянного системного DNS на HQ-SRV ---
# Теперь, когда dnsmasq настроен как локальный DNS-сервер,
# системный DNS самого HQ-SRV будет перенастроен на использование 127.0.0.1.
# Эта настройка заменит временную конфигурацию DNS для ens18, сделанную в Шаге 1.2.
# Настройка выполняется через файл resolv.conf интерфейса ens18.
mkdir -p /etc/net/ifaces/ens18
cat <<'EOF' > /etc/net/ifaces/ens18/resolv.conf
# Домен поиска
search au-team.irpo
# Использовать локально запущенный dnsmasq
nameserver 127.0.0.1
EOF

# [root@hq-srv ~]# --- Включение и перезапуск служб ---
# Включаем автозапуск и перезапускаем dnsmasq.
systemctl enable --now dnsmasq
systemctl restart dnsmasq
# Обновляем системный /etc/resolv.conf через resolvconf и перезапускаем сеть.
resolvconf -u
systemctl restart network
```

### Шаг 10.2: Настройка DNS-клиентов

**Метод:** Настройка файла `/etc/net/ifaces/имя_интерфейса/resolv.conf` на `HQ-RTR`, `BR-RTR`, `BR-SRV` для использования `192.168.1.10` в качестве DNS-сервера. `HQ-CLI` получит настройки по DHCP (в Шаге 9).

**Важное замечание перед настройкой DNS-клиента на виртуальных машинах: Потенциальная проблема с перезаписью `/etc/resolv.conf`:** При выполнении начальной настройки сетевых интерфейсов (Шаг 1.2), для обеспечения доступа к репозиториям пакетов могла быть сконфигурирована временная пересылка DNS-запросов на публичные серверы (например, `8.8.8.8`, `1.1.1.1`) через WAN-интерфейс `HQ-RTR` (`ens18`). При текущей настройке DNS-клиента на `HQ-RTR` для использования внутреннего DNS-сервера (`192.168.1.10` через интерфейс `vlan100`), возникает ситуация, когда системный файл `/etc/resolv.conf` на `HQ-RTR` продолжает содержать или периодически перезаписываться нежелательными DNS-адресами.

Источников таких адресов может быть несколько:
*   **Локальный `dnsmasq` на `HQ-RTR`:** Несмотря на то, что в Шаге 9.1 `dnsmasq` на `HQ-RTR` настраивается как DHCP-сервер с отключенной DNS-функцией (`port=0` и `no-resolv` в `/etc/dnsmasq.conf`), некоторые конфигурации `resolvconf` могут по умолчанию пытаться добавить `127.0.0.1` в `/etc/resolv.conf`, если `dnsmasq` активен и слушает на `lo` (loopback) для каких-либо целей (даже если это не DNS-запросы). Это может привести к тому, что `HQ-RTR` будет пытаться резолвить DNS-имена через самого себя (что неверно, так как его DNS-функция отключена) или создаст конфликт с корректной настройкой.
*   **WAN-интерфейс (например, `ens18`):** Если он всё ещё пытается получить DNS от провайдера или для него осталась статическая конфигурация DNS.

**Решение:** Для принудительного и корректного использования внутреннего DNS-сервера на виртуальных машинах, необходимо явно указать службе `resolvconf` игнорировать информацию о DNS, поступающую от нежелательных источников. Это достигается путём модификации файла `/etc/resolvconf.conf` на `HQ-RTR`:
1.  Убедитесь, что в `/etc/dnsmasq.conf` на `HQ-RTR` присутствуют строки `port=0` и `no-resolv`. Для дополнительной изоляции `dnsmasq` от системного `/etc/resolv.conf` можно также добавить `resolv-file=/var/run/dnsmasq/dnsmasq.resolv.conf` (даже если этот файл не существует).
2.  В файле `/etc/resolvconf.conf` на виртуальных машинах строка `deny_interfaces` должна включать как `lo.dnsmasq` для HQ-RTR (для явного запрета `resolvconf` использовать информацию от `dnsmasq`, слушающего на `lo`), так и WAN-интерфейс `ens18` для всех маших.
3.  Убедитесь, что файлы `/etc/net/ifaces/имя_интерфейса/resolv.conf` содержат `nameserver 192.168.1.10`.
4.  После внесения изменений выполните команды `resolvconf -u` и `systemctl restart network`.

Корректно сконфигурированные `/etc/resolv.conf` на машинах должны содержать только `search au-team.irpo` и `nameserver 192.168.1.10`. Убедитесь, что вы учли эту информацию при настройке DNS-клиента на этих машинах ниже, если столкнулись с указанной проблемой.

#### HQ-CLI

Настройка получена по DHCP (Шаг 9).

```bash
# [root@hq-cli ~]# --- Настройка /etc/resolvconf.conf для корректной работы DNS ---
# Добавляем опции в конец /etc/resolvconf.conf, чтобы предотвратить конфликт с 8.8.8.8 и 1.1.1.1 от WAN-интерфейсов.
tee -a /etc/resolvconf.conf > /dev/null <<'EOF'

# Дополнительная настройка для resolvconf
resolv_conf_local_only=NO
EOF
# [Проверка (опционально): конфиг resolvconf]
# tail -n 3 /etc/resolvconf.conf

# [root@hq-rtr ~]# --- Обновляем /etc/resolv.conf немедленно ---
# Применяем настройки resolvconf (DNS клиентские настройки для самого роутера).
resolvconf -u
```
   
#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Настройка DNS-клиента (через vlan100) ---
# Эта настройка заменит временную конфигурацию DNS для ens18 (из Шага 1.2)
# на использование корпоративного DNS-сервера 192.168.1.10 (HQ-SRV).
# Используем интерфейс vlan100 для связи с HQ-SRV.
mkdir -p /etc/net/ifaces/vlan100
cat <<'EOF' > /etc/net/ifaces/vlan100/resolv.conf
# Домен поиска
search au-team.irpo
# Основной DNS-сервер (HQ-SRV)
nameserver 192.168.1.10
EOF
# Обновляем системный resolv.conf и перезапускаем сеть.
resolvconf -u
systemctl restart network
```

Потенциальная проблема с перезаписью `/etc/resolv.conf` уже исправлена (Шаг 9).

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Настройка DNS-клиента (через ens19) ---
# Эта настройка заменит временную конфигурацию DNS для ens18 (из Шага 1.2)
# на использование корпоративного DNS-сервера 192.168.1.10 (HQ-SRV).
# Используем локальный интерфейс ens19; маршрут к DNS будет получен через OSPF (Шаг 7).
mkdir -p /etc/net/ifaces/ens19
cat <<'EOF' > /etc/net/ifaces/ens19/resolv.conf
# Домен поиска
search au-team.irpo
# Основной DNS-сервер (HQ-SRV, доступен через туннель)
nameserver 192.168.1.10
EOF
# Обновляем системный resolv.conf и перезапускаем сеть.
resolvconf -u
systemctl restart network

# [root@br-rtr ~]# --- Настройка /etc/resolvconf.conf для корректной работы DNS ---
# Добавляем опции в конец /etc/resolvconf.conf, чтобы предотвратить конфликт с 8.8.8.8 и 1.1.1.1 от WAN-интерфейсов.
tee -a /etc/resolvconf.conf > /dev/null <<'EOF'

# Дополнительные настройки для resolvconf
resolv_conf_local_only=NO
deny_interfaces="ens18"
EOF
# [Проверка (опционально): конфиг resolvconf]
# tail -n 3 /etc/resolvconf.conf

# [root@hq-rtr ~]# --- Обновляем /etc/resolv.conf немедленно ---
# Применяем настройки resolvconf (DNS клиентские настройки для самого роутера).
resolvconf -u
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Настройка DNS-клиента (через ens18) ---
# Эта настройка заменит временную конфигурацию DNS для ens18 (из Шага 1.2)
# на использование корпоративного DNS-сервера 192.168.1.10 (HQ-SRV).
# Используем основной интерфейс ens18.
mkdir -p /etc/net/ifaces/ens18
cat <<'EOF' > /etc/net/ifaces/ens18/resolv.conf
# Домен поиска
search au-team.irpo
# Основной DNS-сервер (HQ-SRV, доступен через BR-RTR и туннель)
nameserver 192.168.1.10
EOF
# Обновляем системный resolv.conf и перезапускаем сеть.
resolvconf -u
systemctl restart network

# [root@br-rtr ~]# --- Настройка /etc/resolvconf.conf для корректной работы DNS ---
# Добавляем опции в конец /etc/resolvconf.conf, чтобы предотвратить конфликт с 8.8.8.8 и 1.1.1.1 от WAN-интерфейсов.
tee -a /etc/resolvconf.conf > /dev/null <<'EOF'

# Дополнительная настройка для resolvconf
resolv_conf_local_only=NO
EOF
# [Проверка (опционально): конфиг resolvconf]
# tail -n 3 /etc/resolvconf.conf

# [root@hq-rtr ~]# --- Обновляем /etc/resolv.conf немедленно ---
# Применяем настройки resolvconf (DNS клиентские настройки для самого роутера).
resolvconf -u
```

### Проверка шага 10: Настройка DNS

**Цель:** Проверить, что DNS-сервер `dnsmasq` на `HQ-SRV` запущен, отвечает на запросы, корректно разрешает локальные имена (A, PTR, CNAME) и пересылает внешние запросы. Проверить, что все клиенты настроены использовать `HQ-SRV`.

#### DNS-сервер (HQ-SRV)

```bash
# [root@hq-srv ~]# --- Проверка статуса dnsmasq ---
systemctl is-active dnsmasq
# Ожидаемый результат: `active`.
# [root@hq-srv ~]# --- Проверка DNS порта (UDP/TCP 53) ---
ss -ulnp | grep :53
ss -tlnp | grep :53
# Ожидаемый результат: `dnsmasq` слушает порт `53` на `192.168.1.10` и `127.0.0.1`.
```

#### Все DNS-клиенты (HQ-RTR, BR-RTR, HQ-SRV, BR-SRV, HQ-CLI)

```bash
# [user@hostname ~]# --- Проверка /etc/resolv.conf ---
cat /etc/resolv.conf
# Ожидаемый результат:
# - HQ-SRV: `nameserver 127.0.0.1` и `search au-team.irpo`.
# - Остальные (включая HQ-CLI после DHCP): `nameserver 192.168.1.10` и `search au-team.irpo`.
```

#### Проверка разрешения имён (на HQ-CLI)

```bash
# [root@hq-cli ~]# --- Установка утилит (если не установлено) ---
# apt-get update && apt-get install -y bind-utils

# [root@hq-cli ~]# --- Проверка A-записи (hq-srv) ---
dig hq-srv.au-team.irpo +short
# Ожидаемый результат: `192.168.1.10`.

# [root@hq-cli ~]# --- Проверка PTR-записи (192.168.1.10) ---
dig -x 192.168.1.10 +short
# Ожидаемый результат: `hq-srv.au-team.irpo.`.

# [root@hq-cli ~]# --- Проверка A-записи (br-rtr) ---
dig br-rtr.au-team.irpo +short
# Ожидаемый результат: `192.168.3.1`.

# [root@hq-cli ~]# --- Проверка PTR-записи (192.168.3.1) ---
# dig -x 192.168.3.1 +short
# Ожидаемый результат: Запись PTR не настроена (если использовалась `address=`, а не `host-record`). // ВАРИАТИВНО: Если требовалась PTR, она должна разрешаться.

# [root@hq-cli ~]# --- Проверка CNAME (wiki) ---
dig wiki.au-team.irpo +short
# Ожидаемый результат: IP-адрес(а) hq-rtr (192.168.1.1, 192.168.2.1 или 192.168.99.1). // ВАРИАТИВНО: Если тип/цель записи отличались, результат будет другим.
# Важно, что имя разрешилось в IP HQ-RTR (или в IP, указанный в задании).

# [root@hq-cli ~]# --- Проверка CNAME (moodle) ---
dig moodle.au-team.irpo +short
# Ожидаемый результат: IP-адрес(а) hq-rtr. // ВАРИАТИВНО: См. выше.

# [root@hq-cli ~]# --- Проверка внешнего запроса ---
dig www.google.com +short
# Ожидаемый результат: IP-адрес(а) Google.

# [root@hq-cli ~]# --- Проверка разрешения короткого имени ---
ping -c 1 hq-srv
# Ожидаемый результат: Успешный пинг (ответ от 192.168.1.10).
```

---

## Шаг 11: Настройка часового пояса

**Цель:** Установить единый часовой пояс на всех устройствах стенда для синхронизации времени и корректной работы логов и служб.

**Практическое назначение:** Синхронизация времени и использование единого часового пояса критически важны для анализа событий по логам, работы систем аутентификации (Kerberos) и планировщиков.

**Метод:** Использование команды `timedatectl set-timezone Zone/City`. Требуется установленный пакет `tzdata` на ISP (установка была запущена ранее).

### Шаг 11.1: Настройка пояса

#### ISP

```bash
# [root@isp ~]# --- Установка tzdata (если не установлено) ---
apt-get update && apt-get install -y tzdata

# [root@isp ~]# --- Установка часового пояса ---
# Замените 'Asia/Vladivostok' на актуальный часовой пояс вашего региона.
timedatectl set-timezone Asia/Vladivostok
```

#### HQ-RTR, BR-RTR, HQ-SRV, BR-SRV, HQ-CLI

```bash
# [root@hostname ~]# --- Установка часового пояса ---
# Установка tzdata обычно не требуется на этих ОС (уже есть).
# Устанавливаем тот же часовой пояс, что и на ISP.
timedatectl set-timezone Asia/Vladivostok
```

### Проверка шага 11: Настройка часового пояса

**Цель:** Убедиться, что на всех машинах стенда установлен одинаковый, правильный часовой пояс.

#### Каждая машина стенда

```bash
# [user@hostname ~]# --- Проверка настроек timedatectl ---
timedatectl | grep 'Time zone'
# Ожидаемый результат: Строка `Time zone: Asia/Vladivostok (VLAT, +1000)` (или ваш пояс), одинаковая везде.

# [user@hostname ~]# --- Проверка текущей даты и времени ---
date
# Ожидаемый результат: Время должно быть примерно одинаковым на всех машинах
# (в пределах точности ручной установки, до настройки NTP) и соответствовать часовому поясу.
```

---

**Завершение модуля 1:** Базовая настройка сетевой инфраструктуры завершена. Все устройства сконфигурированы согласно топологии и таблице адресации. **Не забудьте подготовить отчёты по шагам 1 (Таблица 1. Итоговая таблица адресации), 4, 6, 7 и 9.**


---

# **Модуль 2:** Организация сетевого администрирования операционных систем

**Введение:** Этот модуль посвящён настройке и администрированию служб операционных систем в рамках созданной сетевой инфраструктуры. Предполагается, что для выполнения заданий Модуля 2 используется **отдельный, преднастроенный стенд**. На этом стенде **уже сконфигурированы** IP-адресация, сетевая трансляция (NAT), IP-туннель, динамическая маршрутизация, созданы базовые пользователи (`sshuser`, `net_admin`, `user`), настроены DHCP и DNS серверы согласно топологии и требованиям, описанным в **Таблице 3. Преднастроенная таблица адресации (Модуль 2)** ниже.

**Рекомендуемый порядок выполнения заданий Модуля 2 (стратегия экзамена):**
Для максимально эффективного использования времени на экзамене, учитывая длительность загрузки некоторых пакетов и зависимости между шагами, рекомендуется следующий порядок **практических действий**:
1.  **(Параллельно!) Запустить установку Яндекс Браузера:** Выполнить команду установки из **Шага 9** на `HQ-CLI` в фоновом режиме (`epm -y install yandex-browser-stable &`) и **сразу перейти** к следующему пункту, пока браузер скачивается.
2.  Выполнить **Шаг 3:** Настройка службы сетевого времени (NTP) (Быстрый шаг, критичен для Шага 1).
3.  Выполнить **Шаг 1:** Настройка доменного контроллера Samba (Требует NTP).
4.  Выполнить **Шаг 2:** Конфигурация файлового хранилища (NFS).
5.  Выполнить **Шаг 4:** Настройка Ansible (Требует пользователей/SSH).
6.  Выполнить **Шаг 5:** Развёртывание MediaWiki (Docker).
7.  Выполнить **Шаг 7:** Запуск Moodle.
8.  Выполнить **Шаг 6:** Настройка статической трансляции портов (DNAT) (Требует запущенных сервисов).
9.  Выполнить **Шаг 8:** Настройка Nginx как обратного прокси (Требует Moodle/Wiki).
10. Проверить завершение установки и выполнить проверку для **Шага 9** (Яндекс Браузер).

**Важно:** Данное пособие **структурировано по шагам 1-9** согласно заданию. Рекомендуемый порядок выше относится к **практической последовательности действий** на экзамене.

**Таблица 3. Преднастроенная таблица адресации (Модуль 2)**

**Внимание!** IP-адресация стенда для Модуля 2 и последующих **отличается по именам интерфейсов** от той, что проектировалась в Модуле 1, хотя сами IP-адреса и маски остаются теми же. Ниже приведена таблица с фактическими именами интерфейсов и IP-адресами, используемыми на преднастроенном оборудовании Модуля 2. Руководствуйтесь этой таблицей при выполнении заданий Модуля 2 и далее.

| Устройство | Интерфейс           | IP-адрес            | Маска             | VLAN  | Подсеть           | Шлюз          | Примечание                  |
| :--------- | :------------------ | :------------------ | :---------------- | :---- | :---------------- | :------------ | :-------------------------- |
| ISP        | ens19 (к Интернету) | DHCP                | DHCP              | -     | DHCP              | DHCP          | Внешний канал               |
|            | ens20 (к HQ-RTR)    | 172.16.4.1          | 255.255.255.240   | -     | 172.16.4.0/28     | -             | WAN к HQ-RTR                |
|            | ens21 (к BR-RTR)    | 172.16.5.1          | 255.255.255.240   | -     | 172.16.5.0/28     | -             | WAN к BR-RTR                |
| HQ-RTR     | ens19 (к ISP)       | 172.16.4.4          | 255.255.255.240   | -     | 172.16.4.0/28     | 172.16.4.1    | WAN к ISP                   |
|            | ens20 (Trunk)       | -                   | -                 | Trunk | -                 | -             | Trunk порт для LAN/DMZ      |
|            | vlan100 (на ens19)  | 192.168.1.1         | 255.255.255.192   | 100   | 192.168.1.0/26    | -             | LAN 1 (Сеть HQ-SRV)         |
|            | vlan200 (на ens19)  | 192.168.2.1         | 255.255.255.240   | 200   | 192.168.2.0/28    | -             | LAN 2 (Сеть HQ-CLI)         |
|            | vlan999 (на ens19)  | 192.168.99.1        | 255.255.255.248   | 999   | 192.168.99.0/29   | -             | Сеть управления (DMZ)       |
|            | gre1 (IP туннель)   | 192.168.5.1         | 255.255.255.252   | -     | 192.168.5.0/30    | -             | GRE туннель к BR-RTR        |
| HQ-SRV     | ens19 (к HQ-RTR)    | 192.168.1.10        | 255.255.255.192   | 100*  | 192.168.1.0/26    | 192.168.1.1   | LAN 1 (Подключён к VLAN100) |
| HQ-CLI     | ens19 (к HQ-RTR)    | DHCP (192.168.2.10) | DHCP (255.255.255.240)   | 200*  | 192.168.2.0/28    | 192.168.2.1   | LAN 2 (Подключён к VLAN200) |
| BR-RTR     | ens19 (к ISP)       | 172.16.5.5          | 255.255.255.240   | -     | 172.16.5.0/28     | 172.16.5.1    | WAN к ISP                   |
|            | ens20 (к BR-SRV)    | 192.168.3.1         | 255.255.255.224   | -     | 192.168.3.0/27    | -             | LAN 3 (Сеть BR-SRV)         |
|            | gre1 (IP туннель)   | 192.168.5.2         | 255.255.255.252   | -     | 192.168.5.0/30    | -             | GRE туннель к HQ-RTR        |
| BR-SRV     | ens19 (к BR-RTR)    | 192.168.3.10        | 255.255.255.224   | -     | 192.168.3.0/27    | 192.168.3.1   | LAN 3 (Сеть BR)             |

**Примечание:** Символ `*` означает, что VLAN настроен на уровне гипервизора. Преднастроенный стенд Модуля 2 использует VLAN ID 100 (HQ-SRV), 200 (HQ-CLI) и 999 (Управление) согласно данной таблице. Имена интерфейсов на ISP, HQ-RTR, BR-RTR, HQ-SRV, BR-SRV, HQ-CLI для Модуля 2 начинаются с `ens19` и далее по инкременту, где это применимо (например, `ens19`, `ens20`, `ens21`).

---

## Шаг 1: Настройте доменный контроллер Samba на машине BR-SRV

**Цель:** Развернуть **доменный контроллер Active Directory (DC)** на базе **Samba** на `BR-SRV` для централизованного управления пользователями и группами. Ввести `HQ-CLI` в домен, настроить `sudo` для доменных пользователей и импортировать пользователей из CSV-файла.

**Практическое назначение:** Samba AD предоставляет совместимую с Windows службу каталогов в Linux-среде, позволяя централизованно управлять аутентификацией и авторизацией.

**Предпосылки:** Стенд Модуля 2 преднастроен. `BR-SRV` имеет FQDN `br-srv.au-team.irpo`, IP `192.168.3.10/27` (на интерфейсе `ens19`), шлюз `192.168.3.1`, DNS настроен на `HQ-SRV` (`192.168.1.10`). `HQ-CLI` (`192.168.2.10` на интерфейсе `ens19`) также имеет доступ к DNS на `HQ-SRV`. NTP настроен (Шаг 3).

### Шаг 1.1: Подготовка BR-SRV

Убедимся в корректности базовых настроек, удалим конфликтующие пакеты (если есть) и установим необходимые пакеты Samba DC.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка FQDN ---
hostname -f
# Ожидаемый результат: br-srv.au-team.irpo
# [root@br-srv ~]# --- Проверка текущего DNS ---
cat /etc/resolv.conf | grep '192.168.1.10'
# Ожидаемый результат: Строка 'nameserver 192.168.1.10' должна присутствовать.

# [root@br-srv ~]# --- Установка Samba DC ---
# Удаляем bind (несовместим с внутренним DNS Samba), устанавливаем task-samba-dc
apt-get update && apt-get remove -y bind && apt-get install -y samba samba-client task-samba-dc
# [Проверка (опционально): пакеты]
# rpm -q task-samba-dc

# [root@br-srv ~]# --- Резервная копия оригинального smb.conf ---
mv /etc/samba/smb.conf /etc/samba/smb.conf.bak
```

### Шаг 1.2: Инициализация (Provisioning) домена

Используем `samba-tool domain provision` для создания структуры домена AD.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Запуск инициализации домена ---
# Указываем Realm, Domain, Server Role, DNS Backend, включаем RFC2307,
# задаём пароль администратора и DNS forwarder.
samba-tool domain provision --realm=AU-TEAM.IRPO --domain=AU-TEAM --server-role=dc --dns-backend=SAMBA_INTERNAL --use-rfc2307 --adminpass='P@ssw0rd' --option="dns forwarder = 192.168.1.10"

# Ожидаемый результат: Отсутствие в конце вывода.
```

### Шаг 1.3: Настройка Kerberos и системного DNS на DC

Копируем сгенерированный Kerberos конфиг и перенастраиваем системный DNS на `BR-SRV`, чтобы он использовал свой собственный DNS-сервер Samba.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Копирование krb5.conf ---
# Используем '\cp -f', чтобы перезаписать файл без запроса.
\cp -f /var/lib/samba/private/krb5.conf /etc/krb5.conf

# [root@br-srv ~]# --- Настройка системного DNS на локальный DC ---
# Записываем конфигурацию в файл resolv.conf основного интерфейса (ens19).
mkdir -p /etc/net/ifaces/ens19
cat <<'EOF' > /etc/net/ifaces/ens19/resolv.conf
search au-team.irpo
nameserver 127.0.0.1
EOF

# [root@br-srv ~]# --- Обновляем /etc/resolv.conf и перезапускаем сеть ---
resolvconf -u
systemctl restart network
# [Проверка (опционально): resolv.conf]
# cat /etc/resolv.conf
# Ожидаемый результат: nameserver 127.0.0.1, search au-team.irpo
```

### Шаг 1.4: Запуск и включение служб Samba

Включаем службу Samba для автозапуска и стартуем её. Добавляем перезапуск сети и Samba в crontab как меру предосторожности.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Включение и запуск службы Samba ---
systemctl enable --now samba
# [Проверка (опционально): статус службы]
# systemctl status samba

# [root@br-srv ~]# --- Добавление задач в crontab (Workaround для стенда) ---
# Примечание: Это обходное решение на случай проблем инициализации сети/Samba после перезагрузки.
cat << EOF > /tmp/crontab.tmp
# Restart network on reboot after a delay
@reboot sleep 45 ; /bin/systemctl restart network
# Restart samba on reboot after a delay
@reboot sleep 60 ; /bin/systemctl restart samba
EOF
# Загружаем crontab из временного файла.
crontab /tmp/crontab.tmp && rm -f /tmp/crontab.tmp
# [Проверка (опционально): crontab]
# crontab -l
```

### Шаг 1.5: Проверка доменного контроллера

Проверяем работоспособность DC: информацию о домене, DNS-записи и аутентификацию Kerberos.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка информации о домене ---
samba-tool domain info 127.0.0.1 | grep 'Domain'
# Ожидаемый результат: Domain name: AU-TEAM

# [root@br-srv ~]# --- Проверка DNS-записей AD ---
# Устанавливаем утилиты DNS, если не установлены
apt-get update && apt-get install -y bind-utils
# Проверяем SRV-запись LDAP
host -t SRV _ldap._tcp.au-team.irpo
# Ожидаемый результат: ... has SRV record 0 100 389 br-srv.au-team.irpo.
# Проверяем A-запись DC
host br-srv.au-team.irpo
# Ожидаемый результат: br-srv.au-team.irpo has address 192.168.3.10

# [root@br-srv ~]# --- Проверка Kerberos ---
# Получаем тикет для администратора
kinit administrator@AU-TEAM.IRPO
# Вводим пароль: P@ssw0rd
# Проверяем тикет
klist
# Ожидаемый результат: Отображение валидного тикета для administrator@AU-TEAM.IRPO.
```

### Шаг 1.6: Создание пользователей и группы HQ

Создаём пользователей `user1.hq` - `user5.hq` и группу `hq` для главного офиса.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Создание пользователей user1.hq - user5.hq ---
# Используем цикл bash для создания пользователей.
bash << 'EOF'
echo ">>> Создание пользователей user1.hq - user5.hq..."
for i in {1..5}; do
  samba-tool user create "user${i}.hq" 'P@ssw0rdHQ' --given-name=User --surname="${i}HQ" || echo "Предупреждение: Ошибка создания user${i}.hq"
done
echo "<<< Пользователи созданы."
EOF

# [root@br-srv ~]# --- Создание группы hq ---
samba-tool group add hq

# [root@br-srv ~]# --- Добавление пользователей в группу hq ---
samba-tool group addmembers hq user1.hq,user2.hq,user3.hq,user4.hq,user5.hq
# [Проверка (опционально): членство в группе]
# samba-tool group listmembers hq
```

### Шаг 1.7: Ввод HQ-CLI в домен

Присоединяем машину `HQ-CLI` к домену `AU-TEAM.IRPO`. Сначала настраиваем условную пересылку DNS на `HQ-SRV`, чтобы `HQ-CLI` мог разрешать AD-записи через него.

#### HQ-SRV (Настройка условной пересылки DNS)

```bash
# [root@hq-srv ~]# --- Добавление правила условной пересылки в dnsmasq ---
# Используем grep -q || tee -a для добавления строки только если она отсутствует.
grep -q 'server=/au-team.irpo/192.168.3.10' /etc/dnsmasq.conf || tee -a /etc/dnsmasq.conf > /dev/null <<'EOF'

# Пересылка запросов для зоны AD на DC BR-SRV
server=/au-team.irpo/192.168.3.10
EOF
# [Проверка (опционально): конфиг dnsmasq]
# tail -n 3 /etc/dnsmasq.conf

# [root@hq-srv ~]# --- Перезапуск dnsmasq ---
systemctl restart dnsmasq
```

#### HQ-CLI (Ввод в домен)

```bash
# [root@hq-cli ~]# --- Проверка разрешения AD DNS (через HQ-SRV) ---
# Теперь HQ-SRV должен пересылать эти запросы на BR-SRV.
host -t SRV _ldap._tcp.au-team.irpo
# Ожидаемый результат: ... has SRV record ... br-srv.au-team.irpo.
host br-srv.au-team.irpo
# Ожидаемый результат: br-srv.au-team.irpo has address 192.168.3.10

# [root@hq-cli ~]# --- Синхронизация времени (важно для Kerberos) ---
# Используем сервер времени Samba (DC BR-SRV). NTP клиент уже должен быть настроен.
# Принудительная синхронизация (если нужно).
chronyc burst 4/10

# [root@hq-cli ~]# --- Установка пакетов для интеграции с AD (если не установлено) ---
# apt-get update && apt-get install -y task-auth-ad-sssd # Должно быть на преднастроенном стенде

# [root@hq-cli ~]# --- Ввод машины в домен с помощью system-auth ---
# Указываем тип 'ad', реалм, имя клиента, NetBIOS домен, имя админа и пароль админа.
system-auth write ad au-team.irpo hq-cli AU-TEAM 'administrator' 'P@ssw0rd'
# Если потребуется, введите пароль 'P@ssw0rd' ещё раз.

# [root@hq-cli ~]# --- Пауза и проверка присоединения ---
sleep 5
realm list
# Ожидаемый результат: Отображение домена AU-TEAM.IRPO со статусом 'configured'.

# [root@hq-cli ~]# --- Перезагрузка для применения всех изменений ---
# Крайне рекомендуется для корректной работы sssd.
reboot
```

#### HQ-CLI (После перезагрузки)

```bash
# [root@hq-cli ~]# --- Проверка разрешения доменного пользователя ---
# Убедимся, что sssd работает и может получить информацию о пользователе из AD.
id user1.hq
# Ожидаемый результат: Вывод UID, GID и групп для user1.hq@au-team.irpo.

# [root@hq-cli ~]# --- Создание домашних каталогов для user1-5.hq ---
# Примечание: В реальных системах используется pam_mkhomedir. Здесь создаём вручную.
# В ALT Linux pam_mkhomedir настроен по умолчанию при использовании task-auth-ad-sssd,
# но создадим явно для надёжности и демонстрации.
bash << 'EOF'
BASE_HOMEDIR="/home/AU-TEAM.IRPO"
DOMAIN_REALM="au-team.irpo"
SKEL_DIR="/etc/skel"
mkdir -p "${BASE_HOMEDIR}" && chmod 755 "${BASE_HOMEDIR}" || exit 1
echo ">>> Создание домашних каталогов user1.hq - user5.hq..."
for i in {1..5}; do
  user_name="user${i}.hq"
  full_user_name="${user_name}@${DOMAIN_REALM}"
  lower_user_name=$(echo "${user_name}" | tr '[:upper:]' '[:lower:]') # SSSD может использовать lowercase
  user_home_dir="${BASE_HOMEDIR}/${lower_user_name}"
  echo -n "Обработка ${full_user_name}... "
  user_uid=$(id -u "${full_user_name}" 2>/dev/null)
  user_gid=$(id -g "${full_user_name}" 2>/dev/null)
  if [ -n "$user_uid" ] && [ -n "$user_gid" ]; then
    if [ ! -d "${user_home_dir}" ]; then
        install -d -o "${user_uid}" -g "${user_gid}" -m 700 "${user_home_dir}" || { echo "Ошибка install."; continue; }
    fi
    # Копируем содержимое /etc/skel и устанавливаем права, если каталог только что создан
    if [ $? -eq 0 ]; then # Проверяем код возврата install
        cp -aT "${SKEL_DIR}/" "${user_home_dir}/" && chown -R "${user_uid}:${user_gid}" "${user_home_dir}" && chmod 700 "${user_home_dir}" && echo "Готово." || echo "Ошибка копирования/прав."
    else
        echo "Каталог уже существовал или ошибка создания."
    fi
  else
    echo "Ошибка: Не удалось получить UID/GID."
  fi
done
echo "<<< Создание домашних каталогов завершено."
exit 0
EOF

# [root@hq-cli ~]# --- Проверка каталога и входа (опционально) ---
ls -al /home/AU-TEAM.IRPO/user1.hq
# Ожидаемый результат: Содержимое каталога с файлами из skel, владелец user1.hq.
# Попытка войти под пользователем
su - user1.hq
# Ожидаемый результат: Успешный вход, промпт [user1.hq@hq-cli ~]$. Введите 'exit'.
exit
```

### Шаг 1.8: Настройка ограниченных прав sudo для группы `hq`

Настраиваем `sudo` на `HQ-CLI`, чтобы члены доменной группы `hq` могли выполнять команды `cat`, `grep`, `id` от имени `root` без пароля.

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Разрешение использования sudo для всех (через control) ---
# В Alt Linux это необходимо, чтобы доменные пользователи могли вызывать sudo.
control sudo public

# [Проверка (опционально): выполнить control sudo и проверить, что в выводе есть разрешение "public"]
# control sudo
# Ожидаемый результат: `public`.

# [root@hq-cli ~]# --- Создание файла sudoers для группы hq ---
# Создаём файл /etc/sudoers.d/hq с правилом для доменной группы hq.
# Используем %hq (sudo должен разрешить это в доменное имя).
cat <<'EOF' > /etc/sudoers.d/hq
# Разрешить членам доменной группы hq выполнять cat, grep, id без пароля
%hq ALL=(ALL) NOPASSWD:/bin/cat, /bin/grep, /usr/bin/id
EOF

# [root@hq-cli ~]# --- Установка прав и проверка синтаксиса ---
# Устанавливаем права 400 (только чтение для root).
chmod 0400 /etc/sudoers.d/hq
# Проверяем синтаксис всех файлов sudoers.
visudo -c
# Ожидаемый результат: `...parsed OK`.

# [root@hq-cli ~]# --- Тестирование sudo (от имени user1.hq) ---
# Переключаемся на пользователя user1.hq
su - user1.hq
# Пытаемся выполнить разрешённую команду
sudo id
# Ожидаемый результат: Вывод команды id от root (uid=0) без запроса пароля.
# Пытаемся выполнить неразрешённую команду
sudo ls /root
# Введите пароль 'P@ssw0rd'
# Ожидаемый результат: Ошибка "Sorry, user user1.hq... is not allowed to execute...".
# Выходим из сессии пользователя
exit
```

### Шаг 1.9: Импорт пользователей из CSV

Создаём и запускаем скрипт на `BR-SRV` для импорта пользователей из файла `/opt/users.csv`. Имя пользователя формируется как `имя.фамилия` (в нижнем регистре), пароль устанавливается `P@ssw0rd1`, и требуется смена пароля при первом входе.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Создание скрипта импорта /root/samba_user_add.sh ---
cat << 'EOF' > /root/samba_user_add.sh
#!/bin/bash
CSV_FILE="/opt/users.csv"
# Проверяем наличие файла
if [ ! -f "$CSV_FILE" ]; then echo "Ошибка: Файл $CSV_FILE не найден!"; exit 1; fi
echo ">>> Начало импорта пользователей из $CSV_FILE..."
# Читаем CSV файл, пропуская заголовок (tail -n +2)
# IFS=';' устанавливает разделитель
tail -n +2 "$CSV_FILE" | while IFS=';' read -r first_name last_name role phone ou street zip city country password; do
    # Очищаем поля от возможных \r
    first_name=$(echo "${first_name}" | tr -d '\r')
    last_name=$(echo "${last_name}" | tr -d '\r')
    # Формируем имя пользователя (в нижнем регистре)
    user_name="${first_name,,}.${last_name,,}"
    # Пропускаем пустые строки или строки без имени/фамилии
    if [[ -z "$user_name" || "$user_name" == "." ]]; then continue; fi
    echo -n "Обработка: $user_name ... "
    # Добавляем пользователя с паролем 'P@ssw0rd1'
    samba-tool user add "$user_name" 'P@ssw0rd1'
    if [ $? -ne 0 ]; then
      echo "Предупреждение: Ошибка добавления $user_name (возможно, уже существует)."
    else
      echo "Добавлен."
    fi
done
echo "<<< Импорт пользователей завершён."
EOF

# [root@br-srv ~]# --- Предоставление прав и запуск ---
chmod +x /root/samba_user_add.sh
# Запускаем скрипт
/root/samba_user_add.sh
```

### Шаг 1.10 (Необязательно): Создание домашних каталогов для импортированных пользователей

Получаем список импортированных пользователей на `BR-SRV` и используем его для создания домашних каталогов на `HQ-CLI`.

#### BR-SRV (Получение списка)

```bash
# [root@br-srv ~]# --- Получение отфильтрованного списка импортированных пользователей ---
# Исключаем системные и уже созданных userN.hq
FILTERED_USER_LIST=$(samba-tool user list | grep -vE '^(Administrator|Guest|krbtgt|user[1-5]\.hq)$')
# Выводим список (скопируйте его для использования на HQ-CLI)
echo "${FILTERED_USER_LIST}"
```

#### HQ-CLI (Создание каталогов)

**Важно:** Скопируйте список пользователей, выведенный предыдущей командой на `BR-SRV`, и вставьте его вместо плейсхолдера в переменной `USERS_TO_CREATE` ниже.

```bash
# [root@hq-cli ~]# --- Скрипт создания каталогов для списка пользователей ---
bash << 'EOF'
# Вставьте список пользователей с BR-SRV сюда (по одному имени на строку)
USERS_TO_CREATE='
lucian.buck
jacob.schneider
britanney.smith
andrew.gillespie
kyra.odom
josephine.vinson
emmanuel.blankenship
wesley.gonzales
jescie.knox
abbot.blackwell
joshua.buchanan
angela.wilkinson
aspen.terrell
camille.roth
emily.schroeder
francesca.rowe
althea.rodgers
lisandra.rutledge
amber.roth
hayley.padilla
nissim.webb
charity.pacheco
gloria.nicholson
noah.house
alden.wright
alexander.valenzuela
rigel.velez
cyrus.talley
berk.vargas
jenna.myers
cyrus.gomez
nissim.blanchard
reuben.houston
lucas.gallagher
lamar.sweeney
eden.sawyer
mark.duke
raymond.hodges
eden.goff
margaret.hardy
tyrone.hewitt
brock.workman
gabriel.mcdowell
barbara.duncan
gray.foley
nadine.griffin
kevin.golden
hyatt.singleton
stone.stout
arthur.tran
dennis.pace
wesley.lawrence
quinn.park
oliver.castaneda
belle.small
raphael.gaines
hu.pena
hoyt.flynn
ralph.hawkins
brett.juarez
dylan.wheeler
barry.hopkins
libby.mcclure
kylee.orr
nathaniel.mullins
ignatius.watts
fay.bush
madeline.bonner
james.macias
alika.barron
ray.delacruz
keefe.becker
genevieve.vega
kaitlin.branch
carolyn.parsons
aubrey.dale
stephen.castro
tiger.copeland
giacomo.dunlap
althea.battle
caldwell.nielsen
maryam.guzman
zachary.dejesus
uriah.spencer
yolanda.ball
quinlan.velasquez
devin.price
cameron.evans
micah.ware
aidan.byrd
wesley.cain
rhea.bolton
hilel.dalton
maxine.carson
coby.gould
mark.blanchard
samantha.wilkins
cassidy.rivers
mira.woodard
helen.nichols
brody.evans
doris.williamson
rahim.grant
raja.dorsey
dean.hess
jason.cole
hop.fletcher
charity.clay
adam.kramer
diana.nelson
leroy.cameron
deirdre.stark
brielle.mccarty
dean.burton
lois.howell
camden.pickett
sophia.rivera
maggy.craig
ignatius.gilliam
rama.duffy
anastasia.silva
bruno.trujillo
risa.wiggins
angelica.mcgee
irma.mcclure
gemma.williams
florence.reynolds
asher.buckley
mary.singleton
cruz.payne
ramona.frye
maxwell.wilder
meredith.arnold
joseph.wise
lucas.holder
maite.griffin
mercedes.buckner
dale.lindsay
kasper.yates
pearl.davidson
drew.mcintyre
oleg.cannon
drew.trevino
kelsie.horton
mark.sutton
vladimir.rivers
armand.greene
malik.larsen
vivian.malone
anjolie.baldwin
armand.glenn
boris.morgan
liberty.mccarthy
gannon.santiago
simon.stevenson
raphael.bird
vivien.mcguire
fleur.wolfe
phoebe.koch
aladdin.hale
gray.randolph
orla.burton
knox.lopez
kay.tillman
hilary.jimenez
september.carroll
brynne.duffy
macey.waters
colt.wooten
wyatt.fleming
brianna.griffin
mia.mercer
karly.peterson
geoffrey.murray
summer.cooper
noelle.burke
nolan.barry
shellie.ruiz
wynne.ashley
comp.cloud storage
dean.hines
nelle.alford
amery.boyer
aiko.lindsey
roary.conley
patricia.lucas
alika.whitley
ashton.shields
maxwell.leblanc
daryl.ward
chloe.simon
justin.cash
chelsea.monroe
baker.christian
darryl.cherry
jermaine.simmons
hamish.gross
sheila.burt
shaine.simmons
isaac.christian
deirdre.bernard
herman.wright
lois.bruce
emi.mcmillan
timothy.reed
hayes.stokes
dorothy.cervantes
addison.wall
nash.hood
joshua.henderson
amal.clements
malachi.alexander
rinah.deleon
ebony.saunders
shafira.cox
rachel.blackburn
fay.gallegos
conan.odonnell
bernard.waller
britanney.campos
james.cummings
iliana.moreno
barry.schultz
kaye.haynes
colleen.butler
ulla.vaughan
rudyard.house
molly.gill
olivia.rasmussen
noah.buck
hasad.ashley
lydia.sears
rae.wiley
ifeoma.gomez
kaye.lott
hanae.morton
september.dotson
dennis.craig
keith.grant
claire.flores
tanisha.luna
bell.wiley
robert.blevins
nomlanga.randall
slade.rowland
xantha.dillard
william.dalton
brielle.floyd
sawyer.burns
hyatt.rosales
hunter.hopkins
cally.gordon
quinn.flynn
craig.gilbert
iris.gonzalez
adria.logan
dean.roy
kendall.ellison
beverly.carson
jessamine.hodge
emmanuel.bond
vielka.nelson
lucius.mendez
leigh.torres
alexa.bradshaw
malcolm.gilliam
mason.cross
illiana.joyner
brett.carr
carl.cohen
clementine.owen
ahmed.montgomery
flynn.ruiz
martina.watkins
colby.wilcox
reuben.sanford
riley.hansen
trevor.gonzalez
grace.davidson
zenia.berg
imelda.harmon
armand.knowles
rigel.tyson
isabelle.newman
drew.lott
octavia.brady
moana.macdonald
evelyn.calderon
donovan.griffith
alan.todd
tanisha.wiggins
stuart.york
amir.frazier
caesar.wagner
quamar.harrington
anastasia.faulkner
stephen.willis
prescott.gay
ashely.cunningham
karly.curry
'
# Переменные и логика создания каталогов (аналогично Шагу 1.7)
BASE_HOMEDIR="/home/AU-TEAM.IRPO"
DOMAIN_REALM="au-team.irpo"
SKEL_DIR="/etc/skel"
mkdir -p "${BASE_HOMEDIR}" && chmod 755 "${BASE_HOMEDIR}" || { echo "Ошибка создания базового каталога"; exit 1; }
echo ">>> Создание дополнительных домашних каталогов..."
# Используем here-string для передачи списка в цикл
echo "${USERS_TO_CREATE}" | while IFS= read -r user_name_mixed_case; do
  # Очищаем возможное имя пользователя от непечатаемых символов (например, \r от Windows)
  user_name_mixed_case=$(echo "${user_name_mixed_case}" | tr -d '\r')
  [[ -z "${user_name_mixed_case// }" ]] && continue # Пропускаем пустые строки
  lower_user_name=$(echo "${user_name_mixed_case}" | tr '[:upper:]' '[:lower:]')
  full_user_name="${user_name_mixed_case}@${DOMAIN_REALM}"
  user_home_dir="${BASE_HOMEDIR}/${lower_user_name}"
  echo -n "Обработка ${full_user_name}... "
  user_uid=$(id -u "${full_user_name}" 2>/dev/null)
  user_gid=$(id -g "${full_user_name}" 2>/dev/null)
  if [ -n "$user_uid" ] && [ -n "$user_gid" ]; then
    if [ ! -d "${user_home_dir}" ]; then
        install -d -o "${user_uid}" -g "${user_gid}" -m 700 "${user_home_dir}" || { echo "Ошибка install."; continue; }
        # Копируем skel только если каталог был только что создан
        cp -aT "${SKEL_DIR}/" "${user_home_dir}/" && chown -R "${user_uid}:${user_gid}" "${user_home_dir}" && echo "Готово." || echo "Ошибка копирования/прав."
    else
        echo "Каталог уже существует."
    fi
  else
    echo "Ошибка: Не удалось получить UID/GID для '${full_user_name}'."
  fi
done
echo "<<< Создание дополнительных домашних каталогов завершено."
exit 0
EOF

# [root@hq-cli ~]# --- Проверка каталога для примера (используем lowercase) ---
ls -ld /home/AU-TEAM.IRPO/stone.stout
# Ожидаемый результат: Каталог существует, владелец stone.stout, права drwx------.
```

### Пример отчёта: Настройка доменного контроллера Samba AD на BR-SRV

**Цель:**
Развернуть на сервере `BR-SRV` контроллер домена Active Directory (DC), совместимый с Windows, с использованием Samba. Обеспечить централизованное управление пользователями и группами, ввести клиентскую машину `HQ-CLI` в созданный домен, настроить ограниченные права `sudo` для доменных пользователей и выполнить импорт пользователей из предоставленного CSV-файла.

**Выбор реализации:**
*   **Контроллер домена:** Использована `Samba` в режиме `Active Directory Domain Controller` на ВМ `br-srv.au-team.irpo` (192.168.3.10).
*   **Параметры домена:**
    *   Realm (Kerberos): `AU-TEAM.IRPO`
    *   NetBIOS Domain: `AU-TEAM`
*   **DNS:** Использован встроенный DNS-сервер Samba (`SAMBA_INTERNAL`). Настроена пересылка DNS-запросов на `HQ-SRV` (192.168.1.10). На DC системный DNS настроен на `127.0.0.1`. На `HQ-SRV` настроена условная пересылка для зоны `au-team.irpo` на `BR-SRV`.
*   **Unix-атрибуты:** Включена поддержка `rfc2307` для интеграции с Linux-клиентами.
*   **Клиентская интеграция:** ВМ `HQ-CLI` введена в домен с помощью `system-auth` и использует `sssd` для аутентификации и получения информации о пользователях/группах.
*   **Права sudo:** Настроены на `HQ-CLI` через файл в `/etc/sudoers.d/`, предоставляя членам доменной группы `%hq` право выполнять команды `/bin/cat`, `/bin/grep`, `/usr/bin/id` без пароля.
*   **Импорт пользователей:** Выполнен с помощью скрипта, использующего `samba-tool user add` для данных из `/opt/users.csv`.

**Основные шаги конфигурации:**

1.  **Подготовка `BR-SRV`:** Установка `task-samba-dc`, проверка зависимостей (NTP, DNS), удаление конфликтующих пакетов (bind).
2.  **Инициализация домена:** Выполнена команда `samba-tool domain provision` с указанием параметров realm, domain, роли, DNS-бэкенда, `rfc2307` и пароля администратора.
3.  **Настройка DC:** Скопирован `krb5.conf`, системный DNS перенастроен на `127.0.0.1`. Служба `samba` включена и запущена.
4.  **Создание объектов AD:** Созданы пользователи `user1.hq` - `user5.hq` и группа `hq`, пользователи добавлены в группу.
5.  **Настройка DNS-пересылки:** На `HQ-SRV` добавлено правило условной пересылки для `au-team.irpo` на `BR-SRV`.
6.  **Ввод клиента в домен:** На `HQ-CLI` выполнена команда `system-auth write ad ...`, машина перезагружена.
7.  **Настройка `sudo`:** На `HQ-CLI` создан файл `/etc/sudoers.d/hq` с правилами для группы `%hq`.
8.  **Импорт пользователей:** На `BR-SRV` запущен скрипт для импорта пользователей из `/opt/users.csv`.

**Ключевые параметры:**
*   **DC:** `br-srv.au-team.irpo` (IP: 192.168.3.10)
*   **Realm:** `AU-TEAM.IRPO`
*   **Клиент:** `hq-cli.au-team.irpo`
*   **Пароль администратора AD:** `P@ssw0rd`
*   **Группа с правами sudo:** `hq`
*   **Разрешённые команды sudo:** `/bin/cat`, `/bin/grep`, `/usr/bin/id`

**Итог:**
На сервере `BR-SRV` успешно развёрнут и настроен контроллер домена Active Directory на базе Samba. Клиентская машина `HQ-CLI` введена в домен. Созданы необходимые пользователи и группы, настроены ограниченные права sudo. Выполнен импорт пользователей. Инфраструктура готова для централизованного управления пользователями и аутентификации.

### Проверка шага 1: Настройка доменного контроллера Samba

**Цель:** Финально убедиться, что все основные компоненты Шага 1 настроены и работают корректно: DC, интеграция клиента, пользователи, группы, `sudo`, импорт.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка службы и DNS ---
systemctl is-active samba
# Ожидаемый результат: `active`.
host -t SRV _ldap._tcp.au-team.irpo
# Ожидаемый результат: Запись SRV указывает на `br-srv.au-team.irpo`.

# [root@br-srv ~]# --- Проверка пользователей и групп ---
samba-tool group listmembers hq | grep 'user1.hq'
# Ожидаемый результат: `user1.hq` присутствует.
samba-tool user list | grep 'stone.stout'
# Ожидаемый результат: `stone.stout` присутствует.
```

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Проверка домена и пользователей ---
realm list
# Ожидаемый результат: Домен `AU-TEAM.IRPO`, статус `configured`.
id user1.hq
# Ожидаемый результат: Информация о пользователе, включая группу `hq`.
id stone.stout
# Ожидаемый результат: Информация о пользователе.

# [root@hq-cli ~]# --- Проверка входа и sudo ---
su - user1.hq -c 'pwd'
# Ожидаемый результат: `/home/AU-TEAM.IRPO/user1.hq` (или lowercase).
su - user1.hq -c 'sudo id' | grep 'uid=0(root)'
# Ожидаемый результат: Вывод команды `id` от `root`.
```

---

## Шаг 2: Сконфигурируйте файловое хранилище

**Цель:** Настроить на сервере `HQ-SRV` программный **RAID 5** (или другой уровень RAID, согласно вашему варианту задания) массив из трёх дополнительных дисков, создать на нём файловую систему `ext4`, организовать сетевой доступ к данным по протоколу **NFS** для сети `HQ-CLI` (`192.168.2.0/28`) и настроить автоматическое монтирование этого сетевого ресурса на клиенте `HQ-CLI`.

**Практическое назначение:** RAID 5 обеспечивает отказоустойчивость хранилища при выходе из строя одного диска. NFS является стандартным протоколом для предоставления доступа к файлам по сети в Linux/Unix средах. Автомонтирование упрощает доступ к ресурсу для пользователей клиента.

**Предпосылки:** К `HQ-SRV` добавлены три дополнительных диска (например, `/dev/sdb`, `/dev/sdc`, `/dev/sdd`). Стенд Модуля 2 преднастроен согласно **Таблице 3**.

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

### Шаг 2.1: Подготовка к созданию RAID на HQ-SRV

Установим необходимые утилиты (`mdadm`, `fdisk`) и определим имена дополнительных дисков.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Установка утилит ---
apt-get update && apt-get install -y mdadm fdisk
# [Проверка (опционально): пакеты]
# rpm -q mdadm ; rpm -q fdisk

# [root@hq-srv ~]# --- Проверка имён дисков ---
# Ищем диски размером около 1G (или другого заданного размера).
# Замените /dev/sd[b-d] в следующей команде на реальные имена!
lsblk | grep '1G'
# Ожидаемый результат: Вывод имён дисков (например, sdb, sdc, sdd).
```

### Шаг 2.2: Создание RAID 5 (или другого уровня) и файловой системы на HQ-SRV

Создаём **RAID 5** (или другой уровень, указанный в вашем задании) массив `/dev/md0`, сохраняем его конфигурацию, создаём раздел на массиве и форматируем его в `ext4`. Соответственно, если уровень RAID другой (например, RAID 0), то и пути монтирования и NFS (например, `/raid0`, `/raid0/nfs`) будут отличаться.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Создание RAID 5 (/dev/md0) ---
# // ВАРИАТИВНО: Уровень RAID (здесь 5) может отличаться (например, 0). Используйте уровень из вашего варианта задания.
# Используйте актуальные имена дисков, определённые на предыдущем шаге!
# Пример для /dev/sdb, /dev/sdc, /dev/sdd:
mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/sdb /dev/sdc /dev/sdd --force
# [Проверка (опционально): статус массива]
# mdadm --detail /dev/md0 | grep State
# Ожидаемый результат: State : active (может быть resyncing).

# [root@hq-srv ~]# --- Сохранение конфигурации RAID ---
# Сохраняем информацию о массиве для автосборки при загрузке.
mkdir -p /etc/mdadm
mdadm --detail --scan --verbose >> /etc/mdadm/mdadm.conf
# Копируем в основной файл, если он используется в системе
cp /etc/mdadm/mdadm.conf /etc/mdadm.conf 2>/dev/null || true
# [Проверка (опционально): конфиг mdadm]
# cat /etc/mdadm.conf | grep '/dev/md0'

# [root@hq-srv ~]# --- Создание раздела на /dev/md0 ---
# Используем fdisk для создания одного раздела на весь массив.
# Пояснение к командам fdisk (вводятся последовательно после запуска fdisk /dev/md0):
# n - создать новый раздел
# p - тип раздела primary (основной)
# 1 - номер раздела 1
# Enter - принять значение по умолчанию для первого сектора (начало диска)
# Enter - принять значение по умолчанию для последнего сектора (конец диска)
# w - записать изменения и выйти
fdisk /dev/md0 <<EOF
n
p
1


w
EOF
# [Проверка (опционально): раздел]
lsblk -o NAME,SIZE,TYPE,MOUNTPOINT /dev/md0
# Ожидаемый результат: Появление устройства md0p1.

# [root@hq-srv ~]# --- Форматирование раздела в ext4 ---
mkfs.ext4 /dev/md0p1
```

### Шаг 2.3: Монтирование и настройка NFS-сервера на HQ-SRV

Настраиваем автомонтирование созданного раздела в `/raid5`, устанавливаем NFS-сервер, создаём экспортируемый каталог, настраиваем экспорт и запускаем службу NFS.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Настройка fstab для автомонтирования /raid5 ---
# // ВАРИАТИВНО: Путь монтирования (здесь /raid5) может отличаться (например, /raid0). Используйте путь из вашего варианта задания.
# Создаём точку монтирования.
mkdir /raid5
# Получаем UUID раздела.
raid_uuid=$(blkid -s UUID -o value /dev/md0p1)
# Добавляем запись в /etc/fstab для монтирования по UUID.
echo "UUID=$raid_uuid /raid5 ext4 defaults 0 2" >> /etc/fstab
# [Проверка (опционально): fstab]
# tail -n 1 /etc/fstab

# [root@hq-srv ~]# --- Монтирование файловой системы ---
# Монтируем все ФС, указанные в /etc/fstab.
mount -a
# [Проверка (опционально): монтирование]
# df -hT /raid5
# Ожидаемый результат: /dev/md0p1 смонтирован в /raid5, тип ext4. // ВАРИАТИВНО: Путь монтирования может отличаться.

# [root@hq-srv ~]# --- Установка NFS-сервера ---
apt-get update && apt-get install -y nfs-server
# [Проверка (опционально): пакет]
# rpm -q nfs-server

# [root@hq-srv ~]# --- Создание каталога NFS и настройка прав ---
# // ВАРИАТИВНО: Путь к NFS ресурсу (здесь /raid5/nfs) может отличаться (например, /raid0/nfs). Используйте путь из вашего варианта задания.
mkdir /raid5/nfs
# Устанавливаем владельца nobody:nogroup и права 777 для простоты доступа.
chown 99:99 /raid5/nfs
chmod 777 /raid5/nfs
# [Проверка (опционально): права]
# ls -ld /raid5/nfs
# Ожидаемый результат: drwxrwxrwx ... nobody nogroup ... /raid5/nfs

# [root@hq-srv ~]# --- Настройка экспорта NFS (/etc/exports) ---
# // ВАРИАТИВНО: Путь к NFS ресурсу (здесь /raid5/nfs) может отличаться.
# Экспортируем /raid5/nfs для сети HQ-CLI (192.168.2.0/28) с правами rw.
cat <<'EOF' > /etc/exports
/raid5/nfs 192.168.2.0/28(rw,sync,no_subtree_check)
EOF
# [Проверка (опционально): файл exports]
# cat /etc/exports

# [root@hq-srv ~]# --- Применение экспорта и запуск NFS ---
# Перечитываем /etc/exports.
exportfs -ra
# [Проверка (опционально): активный экспорт]
# exportfs -v
# Ожидаемый результат: Отображение экспорта /raid5/nfs. // ВАРИАТИВНО: Путь может отличаться.
# Включаем автозапуск и запускаем службу NFS.
systemctl enable --now nfs-server
```

### Шаг 2.4: Настройка NFS-клиента и автомонтирования на HQ-CLI

Устанавливаем NFS-клиент (если необходимо), создаём точку монтирования и настраиваем `/etc/fstab` для автоматического монтирования NFS-ресурса с сервера `HQ-SRV`.

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Установка NFS-клиента (если необходимо) ---
# apt-get update && apt-get install -y nfs-clients

# [root@hq-cli ~]# --- Создание точки монтирования ---
mkdir -p /mnt/nfs

# [root@hq-cli ~]# --- Настройка fstab для автомонтирования NFS ---
# Добавляем запись для монтирования ресурса с HQ-SRV (192.168.1.10).
# // ВАРИАТИВНО: Путь к NFS ресурсу на сервере (здесь /raid5/nfs) может отличаться. Используйте путь из вашего варианта задания.
# Используем опции для сетевых ФС и автомонтирования systemd.
cat <<EOF >> /etc/fstab
192.168.1.10:/raid5/nfs /mnt/nfs nfs intr,soft,_netdev,x-systemd.automount 0 0
EOF
# [Проверка (опционально): fstab]
# tail -n 1 /etc/fstab

# [root@hq-cli ~]# --- Активация автомонтирования ---
# Перечитываем конфигурацию systemd, чтобы он распознал изменения в /etc/fstab.
systemctl daemon-reload
# Монтируем все файловые системы из /etc/fstab, чтобы применить изменения без перезагрузки.
# Это активирует и нашу новую NFS-запись.
mount -a
# Доступ к каталогу также инициирует монтирование (особенно для x-systemd.automount, если 'mount -a'
# по какой-то причине не активировал его полностью, или для проверки). Может занять несколько секунд.
ls /mnt/nfs
# [Проверка (опционально): монтирование]
# df -hT /mnt/nfs
# Ожидаемый результат: 192.168.1.10:/raid5/nfs смонтирован в /mnt/nfs. // ВАРИАТИВНО: Путь на сервере может отличаться.
```

### Пример отчёта: Конфигурация файлового хранилища и NFS на HQ-SRV

**Цель:** Создать отказоустойчивое файловое хранилище на сервере `HQ-SRV` с использованием программного **RAID 5** (или другого уровня RAID, согласно варианту задания) массива и предоставить к нему сетевой доступ по протоколу NFS для клиентской сети `HQ-CLI`.

**Выбор реализации:**
*   **Хранилище:** Был выбран программный **RAID 5** (`mdadm`), так как он обеспечивает баланс между отказоустойчивостью и полезной ёмкостью. Использованы три дополнительных диска. Имя устройства массива - `/dev/md0`. *Примечание: Уровень RAID может отличаться в зависимости от варианта задания (например, RAID 0).*
*   **Файловая система:** Созданный на RAID-массиве раздел `/dev/md0p1` был отформатирован в **ext4**.
*   **Сетевой доступ:** Для предоставления доступа по сети выбран протокол **NFS (Network File System)**.
*   **Автоматическое монтирование:** На сервере (`HQ-SRV`) настроено автомонтирование RAID-раздела через `/etc/fstab` в `/raid5`. На клиенте (`HQ-CLI`) настроено автомонтирование NFS-ресурса через `/etc/fstab` с опцией `x-systemd.automount` в `/mnt/nfs`. *Примечание: Пути монтирования и NFS-ресурса могут отличаться в зависимости от варианта задания (например, `/raid0`, `/raid0/nfs`).*

**Основные шаги конфигурации:**

1.  **На `HQ-SRV`:**
    *   Установлена утилита `mdadm`.   
    *   Создан **RAID 5** (или другой уровень RAID, например, RAID 0, согласно варианту задания) массив `/dev/md0` из дисков `/dev/sdb`, `/dev/sdc`, `/dev/sdd`.
    *   Конфигурация массива сохранена в `/etc/mdadm.conf`.
    *   На массиве создан раздел `/dev/md0p1`.
    *   Раздел отформатирован в `ext4`.
    *   Создана точка монтирования `/raid5` (или другой путь, например `/raid0`, соответствующий уровню RAID из варианта задания), настроено автомонтирование в `/etc/fstab`, раздел смонтирован.    *   Установлен NFS-сервер (`nfs-server`).
    *   Создан каталог для экспорта `/raid5/nfs` (или другой путь, например `/raid0/nfs`, соответствующий варианту задания), установлены права (`nobody:nogroup`, `777`).
    *   Настроен экспорт каталога `/raid5/nfs` для сети `192.168.2.0/28` с опциями `rw,sync,no_subtree_check` в файле `/etc/exports`.
    *   Служба `nfs-server` включена и запущена.
2.  **На `HQ-CLI`:**
    *   Установлен NFS-клиент (`nfs-clients`).
    *   Создана точка монтирования `/mnt/nfs`.
    *   Настроено автомонтирование ресурса `192.168.1.10:/raid5/nfs` (или другой путь к NFS-ресурсу на сервере, например `/raid0/nfs`, согласно варианту задания) в `/mnt/nfs` через `/etc/fstab` с опциями `intr,soft,_netdev,x-systemd.automount`.
    *   Ресурс смонтирован (при попытке доступа).

**Ключевые параметры NFS:**
*   **Экспортируемый каталог:** `/raid5/nfs` *// ВАРИАТИВНО: Путь может отличаться.*
*   **Доступ разрешён сети:** `192.168.2.0/28` *// ВАРИАТИВНО: Маска может отличаться.*
*   **Опции экспорта:** `rw`, `sync`, `no_subtree_check`.

**Итог:**
На сервере `HQ-SRV` сконфигурирован отказоустойчивый **RAID 5** (или другой уровень RAID, согласно варианту задания) массив. Каталог `/raid5/nfs` (или другой соответствующий путь, например `/raid0/nfs`) на этом массиве успешно экспортирован по протоколу NFS и доступен для чтения и записи клиентам из сети `192.168.2.0/28`. На клиенте `HQ-CLI` настроено автоматическое монтирование этого сетевого ресурса в каталог `/mnt/nfs`. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 2: Конфигурация файлового хранилища и NFS

**Цель:** Убедиться в работоспособности RAID, NFS-сервера и клиента, а также в возможности чтения/записи на сетевой ресурс.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Проверка RAID и ФС ---
mdadm --detail /dev/md0 | grep State
# Ожидаемый результат: `State : active`.
df -hT /raid5
# Ожидаемый результат: `/dev/md0p1` смонтирован в `/raid5`, тип `ext4`. // ВАРИАТИВНО: Путь монтирования может отличаться.

# [root@hq-srv ~]# --- Проверка NFS ---
exportfs -v
# Ожидаемый результат: Отображение экспорта `/raid5/nfs` для `192.168.2.0/28`. // ВАРИАТИВНО: Путь и маска могут отличаться.
systemctl is-active nfs-server
# Ожидаемый результат: `active`.
```

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Проверка монтирования NFS ---
df -hT /mnt/nfs
# Ожидаемый результат: `192.168.1.10:/raid5/nfs` смонтирован в `/mnt/nfs`, тип `nfs4` (или `nfs`). // ВАРИАТИВНО: Путь на сервере может отличаться.
mount | grep /mnt/nfs
# Ожидаемый результат: Строка монтирования с опциями `intr,soft,_netdev,...`.

# [root@hq-cli ~]# --- Проверка доступа на чтение/запись ---
touch /mnt/nfs/test.txt && ls -l /mnt/nfs/test.txt && rm /mnt/nfs/test.txt && echo "NFS Write/Read/Delete OK" || echo "NFS Write/Read/Delete FAILED"
# Ожидаемый результат: `NFS Write/Read/Delete OK`.
```

---

## Шаг 3: Настройте службу сетевого времени (NTP)

**Цель:** Обеспечить синхронизацию системного времени на всех устройствах стенда с использованием протокола **NTP (Network Time Protocol)**. Маршрутизатор `HQ-RTR` будет выступать в роли NTP-сервера stratum 5.

**Практическое назначение:** Единое время важно для корректной работы логов, систем аутентификации (Kerberos), безопасности и планирования задач. `chrony` является современной и рекомендуемой реализацией NTP в Linux.

**Метод:** Установка и настройка `chrony` на `HQ-RTR` как сервера, использующего локальные часы (`local stratum 5`) и разрешающего подключения от клиентов (`allow`). Настройка `chrony` на остальных машинах как клиентов, указывающих на `HQ-RTR` в качестве `server`.

### Шаг 3.1: Настройка NTP-сервера `chrony` на HQ-RTR

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Установка chrony ---
apt-get update && apt-get install -y chrony
# [Проверка (опционально): пакет]
# rpm -q chrony

# [root@hq-rtr ~]# --- Настройка chrony как сервера ---
# Создаем конфигурационный файл /etc/chrony.conf
# Используем локальные часы как основной источник и устанавливаем стратум 5 для локального источника
# Разрешаем подключения от клиентов из всех внутренних сетей
cat <<'EOF' > /etc/chrony.conf
# Использовать локальные часы как источник времени stratum 5
local stratum 5

# Разрешить NTP-клиентов из всех внутренних сетей стенда (согласно Таблице 3)
# // ВАРИАТИВНО: Маски сетей могут отличаться. Используйте маски из вашего варианта.
# Сеть HQ-SRV (VLAN 100)
allow 192.168.1.0/26
# Сеть HQ-CLI (VLAN 200)
allow 192.168.2.0/28
# Сеть управления (VLAN 999)
allow 192.168.99.0/29
# Сеть BR (доступна через GRE)
allow 192.168.3.0/27
# Сеть GRE-туннеля
allow 192.168.5.0/30

# Записывать информацию о смещении системных часов
driftfile /var/lib/chrony/drift

# Разрешить "прыжок" времени при большой разнице при первых трех синхронизациях
makestep 1.0 3

# Включить синхронизацию аппаратных часов (RTC)
rtcsync

# Указывать директорию для лог-файлов
logdir /var/log/chrony
EOF
# [Проверка (опционально): конфиг]
# cat /etc/chrony.conf

# [root@hq-rtr ~]# --- Включение и запуск службы chronyd ---
systemctl enable --now chronyd
```

### Шаг 3.2: Настройка NTP-клиентов `chrony`

**Метод:** Установка `chrony`, запись конфигурации в `/etc/chrony.conf` с указанием IP `HQ-RTR` (по доступному маршруту), запуск службы.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Установка и настройка chrony клиента ---
apt-get update && apt-get install -y chrony
# Указываем IP HQ-RTR в сети VLAN 100 (192.168.1.1)
cat <<'EOF' > /etc/chrony.conf
server 192.168.1.1 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

# [root@hq-srv ~]# --- Включение и запуск службы chronyd ---
systemctl enable --now chronyd
```

#### HQ-CLI

```bash
# [root@hq-cli ~]# --- Установка и настройка chrony клиента ---
apt-get update && apt-get install -y chrony
# Указываем IP HQ-RTR в сети VLAN 200 (192.168.2.1)
cat <<'EOF' > /etc/chrony.conf
server 192.168.2.1 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

# [root@hq-cli ~]# --- Включение и запуск службы chronyd ---
systemctl enable --now chronyd
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Установка и настройка chrony клиента ---
apt-get update && apt-get install -y chrony
# Указываем IP HQ-RTR в GRE-туннеле (192.168.5.1)
cat <<'EOF' > /etc/chrony.conf
server 192.168.5.1 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

# [root@br-rtr ~]# --- Включение и запуск службы chronyd ---
systemctl enable --now chronyd
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Установка и настройка chrony клиента ---
apt-get update && apt-get install -y chrony
# Указываем IP HQ-RTR в GRE-туннеле (192.168.5.1), доступный через BR-RTR.
cat <<'EOF' > /etc/chrony.conf
server 192.168.5.1 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

# [root@br-srv ~]# --- Включение и запуск службы chronyd ---
systemctl enable --now chronyd
```

### Пример отчёта: Настройка службы сетевого времени (NTP)

**Цель:** Обеспечить точную и синхронизированную работу системного времени на всех устройствах сетевого стенда. Это достигается путём настройки службы NTP, где маршрутизатор `HQ-RTR` выступает в роли локального NTP-сервера (stratum 5), а остальные машины (`HQ-SRV`, `HQ-CLI`, `BR-RTR`, `BR-SRV`) синхронизируют своё время с ним.

**Выбор реализации:**
*   **Протокол:** NTP (Network Time Protocol).
*   **Реализация:** `chrony` – современный и гибкий клиент/сервер NTP.
*   **NTP-сервер:** `HQ-RTR` настроен как NTP-сервер, использующий свои локальные системные часы в качестве источника времени (`local stratum 5`). Это позволяет ему служить авторитетным источником времени для внутренней сети, даже если у него нет доступа к внешним NTP-серверам.
*   **NTP-клиенты:** Остальные машины стенда (`HQ-SRV`, `HQ-CLI`, `BR-RTR`, `BR-SRV`) настроены как NTP-клиенты, получающие время от `HQ-RTR`. Клиенты используют IP-адрес `HQ-RTR`, доступный им по сети (локальный VLAN или GRE-туннель).
*   **Безопасность:** На сервере `HQ-RTR` настроены директивы `allow`, разрешающие NTP-запросы только от подсетей, используемых на стенде.

**Основные шаги конфигурации:**

1.  **Установка `chrony`:** Пакет `chrony` установлен на все устройства стенда.
2.  **Настройка NTP-сервера (`HQ-RTR`):**
    *   В файле `/etc/chrony.conf` указана директива `local stratum 5` для использования локальных часов.
    *   Добавлены директивы `allow` для всех внутренних подсетей (`192.168.1.0/26`, `192.168.2.0/28`, `192.168.99.0/29`, `192.168.3.0/27`, `192.168.5.0/30`). *Примечание: Маски сетей могут отличаться в зависимости от варианта задания.*
    *   Служба `chronyd` включена и запущена.
3.  **Настройка NTP-клиентов (остальные машины):**
    *   В файле `/etc/chrony.conf` на каждом клиенте добавлена директива `server <IP_HQ-RTR> iburst`, где `<IP_HQ-RTR>` — это IP-адрес `HQ-RTR`, доступный данному клиенту (например, `192.168.1.1` для `HQ-SRV`, `192.168.5.1` для `BR-RTR` и `BR-SRV`).
    *   Служба `chronyd` включена и запущена на каждом клиенте.
4.  **Проверка синхронизации:** С помощью команд `chronyc tracking` и `chronyc sources` проверено, что клиенты успешно синхронизируются с `HQ-RTR` и получают stratum на единицу выше (stratum 6).

**Схема 5. Визуализация работы NTP**

```text
        +-----------------------------------+
        |      HQ-RTR                       |
        |  (NTP-сервер, Stratum 5)          |
        |  IP: 192.168.1.1 (для HQ-SRV/CLI) |
        |  IP: 192.168.5.1 (для BR)         |
        +----------+------------------------+
                   |
      +------------+-------------+-------------+-------------+
      |            |             |             |             |
+-----------+ +-----------+ +-----------+ +-----------+ +-----------+
|  HQ-SRV   | |  HQ-CLI   | |  BR-RTR   | |  BR-SRV   | |   ISP     |
| (Client)  | | (Client)  | | (Client)  | | (Client)  | | (Client)  |
| Stratum 6 | | Stratum 6 | | Stratum 6 | | Stratum 6 | | Stratum 6 |
+--------- -+ +-----------+ +-----------+ +-----------+ +-----------+
```

**Итог:**
На всех устройствах стенда настроена служба сетевого времени `chrony`. Маршрутизатор `HQ-RTR` функционирует как NTP-сервер stratum 5 для внутренней сети. Все остальные машины успешно синхронизируют своё время с `HQ-RTR`, что обеспечивает единство времени на стенде, критически важное для корректной работы логов и служб, зависящих от точного времени (например, Kerberos). **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 3: Настройка службы сетевого времени (NTP)

**Цель:** Убедиться в работе NTP-сервера `HQ-RTR` (использует локальные часы, stratum 5) и синхронизации клиентов с ним (получение stratum 6).

#### HQ-RTR (Сервер)

```bash
# [root@hq-rtr ~]# --- Проверка статуса chronyd ---
systemctl is-active chronyd
# Ожидаемый результат: `active`.
# [root@hq-rtr ~]# --- Проверка статуса отслеживания ---
chronyc tracking | grep 'Stratum\|Ref ID'
# Ожидаемый результат: `Reference ID    : 7F7F0101 (LOCL)`, `Stratum         : 5`.
# [root@hq-rtr ~]# --- Проверка подключённых клиентов ---
# Может потребоваться время для появления клиентов.
chronyc clients
# Ожидаемый результат: Список IP-адресов клиентов (hq-srv.au-team.irpo (192.168.1.10), hq-cli.au-team.irpo (192.168.2.10), 192.168.5.2, 192.168.3.10).
```

#### HQ-SRV (Клиент)

```bash
# [root@hq-srv ~]# --- Проверка синхронизации ---
chronyc sources -v
# Ожидаемый результат: Строка для 192.168.1.1, помеченная *.
chronyc tracking | grep 'Stratum\|Ref ID'
# Ожидаемый результат: `Stratum         : 6`, `Reference ID    : C0A80101` (IP 192.168.1.1).
```

#### HQ-CLI (Клиент)

```bash
# [root@hq-cli ~]# --- Проверка синхронизации ---
chronyc sources -v
# Ожидаемый результат: Строка для 192.168.2.1, помеченная *.
chronyc tracking | grep 'Stratum\|Ref ID'
# Ожидаемый результат: `Stratum         : 6`, `Reference ID    : C0A80201` (IP 192.168.2.1).
```

#### BR-RTR (Клиент)

```bash
# [root@br-rtr ~]# --- Проверка синхронизации ---
chronyc sources -v
# Ожидаемый результат: Строка для 192.168.5.1, помеченная *.
chronyc tracking | grep 'Stratum\|Ref ID'
# Ожидаемый результат: `Stratum         : 6`, `Reference ID    : C0A80501` (IP 192.168.5.1).
```

#### BR-SRV (Клиент)

```bash
# [root@br-srv ~]# --- Проверка синхронизации ---
chronyc sources -v
# Ожидаемый результат: Строка для 192.168.5.1, помеченная *.
chronyc tracking | grep 'Stratum\|Ref ID'
# Ожидаемый результат: `Stratum         : 6`, `Reference ID    : C0A80501` (IP 192.168.5.1).
```

---

## Шаг 4: Сконфигурируйте Ansible на сервере BR-SRV

**Цель:** Установить и настроить **Ansible** на управляющем сервере `BR-SRV` для автоматизации задач администрирования на управляемых узлах (`HQ-SRV`, `HQ-CLI`, `HQ-RTR`, `BR-RTR`). Настроить SSH-доступ по ключам, создать файл инвентаря и проверить базовую работоспособность.

**Практическое назначение:** Ansible позволяет централизованно управлять конфигурациями множества систем без необходимости установки агентов на управляемые узлы, используя стандартный протокол SSH.

**Предпосылки:** Пользователи (`sshuser`, `user`, `net_admin`) созданы на управляемых узлах. Сетевая связность между `BR-SRV` и всеми узлами установлена. Маршрутизаторы и `HQ-CLI` по умолчанию слушают SSH на порту 22.

**Важное замечание о подготовке SSH для Ansible и DNAT на стенде Модуля 2:** 
Перед настройкой Ansible (Шаг 4) и последующей настройкой DNAT (Шаг 6), необходимо привести конфигурацию SSH-серверов в соответствие с требованиями:
*   **Изменение порта SSH на серверах (`HQ-SRV`, `BR-SRV`):**  На преднастроенном стенде Модуля 2 SSH-серверы (`HQ-SRV`, `BR-SRV`) по умолчанию работают на стандартном порту **22**. Однако для выполнения задания по DNAT (Шаг 6), где часто требуется проброс на порт **2024** Порт может отличаться, поэтому используйте порт из вашего варианта задания для DNAT. Мы **изменим порт SSH на этих серверах на 2024 в начале Шага 4** для единообразия. Ansible будет сразу настраиваться на работу с этим новым портом.
*   **SSH на `HQ-CLI`:** По умолчанию SSH-сервер на `HQ-CLI` может быть **выключен**. Его необходимо будет включить в начале Шага 4.
*   **Пароль для `net_admin`:** На преднастроенном стенде Модуля 2 пользователь `net_admin` (на маршрутизаторах `HQ-RTR` и `BR-RTR`) имеет пароль **`P@ssw0rd`**. Пожалуйста, используйте именно этот пароль при выполнении команд, требующих его ввода (например, для `ssh-copy-id` в Шаге 4.2).
Это обеспечит корректную работу Ansible с актуальными портами и подготовит серверы для настройки DNAT.

---

### Шаг 4.1: Подготовка SSH-серверов (Изменение порта, включение службы)

На этом этапе мы изменим порт SSH на серверах `HQ-SRV` и `BR-SRV` на **2024**. Используйте целевой порт для DNAT из вашего задания) и убедимся, что SSH-сервер на `HQ-CLI` запущен.

#### HQ-SRV (Изменение порта SSH)

```bash
# [root@hq-srv ~]# --- Изменение порта SSH на 2024 ---
# Заменяем 'Port 22' (или любую другую активную/закомментированную строку Port) на 'Port 2024'.
# // ВАРИАТИВНО: Целевой порт SSH (здесь 2024) может отличаться.
sed -i 's/^#*[[:space:]]*Port[[:space:]]\+.*/Port 2024/' /etc/openssh/sshd_config
systemctl restart sshd
# [Проверка (опционально): Убедитесь, что sshd слушает новый порт]
# ss -tlpn | grep :2024
```

#### BR-SRV (Изменение порта SSH)

```bash
# [root@br-srv ~]# --- Изменение порта SSH на 2024 ---
# // ВАРИАТИВНО: Целевой порт SSH (здесь 2024) может отличаться.
sed -i 's/^#*[[:space:]]*Port[[:space:]]\+.*/Port 2024/' /etc/openssh/sshd_config
systemctl restart sshd
# [Проверка (опционально): Убедитесь, что sshd слушает новый порт]
# ss -tlpn | grep :2024
```

#### HQ-CLI (Включение SSH-сервера)

```bash
# [root@hq-cli ~]# --- Включение и запуск sshd, если он неактивен ---
# Эта команда включает автозапуск и немедленно запускает службу.
systemctl enable --now sshd
# [Проверка (опционально): Убедитесь, что служба активна]
# systemctl is-active sshd
# Ожидаемый результат: active
```

### Шаг 4.2: Установка Ansible и настройка SSH-доступа по ключам

#### BR-SRV

```bash
# [root@br-srv ~]# --- Установка Ansible ---
apt-get update && apt-get install -y ansible
# [Проверка (опционально): версия]
# ansible --version

# [root@br-srv ~]# --- Генерация SSH ключа для root (если нет) ---
# Генерируем ключ RSA без парольной фразы (-N "").
# Если ключ уже существует, команда выдаст сообщение, но не перезапишет его.
ssh-keygen -t rsa -f /root/.ssh/id_rsa -N "" || echo "Ключ уже существует."
# [Проверка (опционально): ключи]
# ls -l /root/.ssh/id_rsa*

# [root@br-srv ~]# --- Копирование публичного ключа на управляемые узлы ---
# Используем ssh-copy-id. Вводим пароли соответствующих пользователей.
# SSH на HQ-SRV и BR-SRV теперь на порту 2024 (или вариативном).
# SSH на HQ-CLI и маршрутизаторах на порту 22.

# HQ-SRV (sshuser@192.168.1.10, порт 2024) - Пароль: P@ssw0rd
# // ВАРИАТИВНО: Порт 2024 для HQ-SRV может отличаться.
ssh-copy-id -i /root/.ssh/id_rsa.pub -p 2024 sshuser@192.168.1.10
# HQ-CLI (user@192.168.2.10, порт 22) - Пароль: resu
ssh-copy-id -i /root/.ssh/id_rsa.pub user@192.168.2.10
# HQ-RTR (net_admin@172.16.4.4, порт 22) - Пароль: P@ssw0rd
ssh-copy-id -i /root/.ssh/id_rsa.pub net_admin@172.16.4.4
# BR-RTR (net_admin@172.16.5.5, порт 22) - Пароль: P@ssw0rd
ssh-copy-id -i /root/.ssh/id_rsa.pub net_admin@172.16.5.5
# Примечание: BR-SRV (управляющий узел Ansible) сам себе ключ не копирует этим методом.

# [Проверка (опционально): SSH без пароля]
# // ВАРИАТИВНО: Порт 2024 для HQ-SRV может отличаться.
ssh -p 2024 sshuser@192.168.1.10 exit
ssh user@192.168.2.10 exit
ssh net_admin@172.16.4.4 exit
ssh net_admin@172.16.5.5 exit
```

### Шаг 4.3: Создание файлов конфигурации Ansible

#### BR-SRV

```bash
# [root@br-srv ~]# --- Создание рабочего каталога Ansible ---
mkdir -p /etc/ansible

# [root@br-srv ~]# --- Создание файла инвентаря (/etc/ansible/hosts) ---
# Определяем группы хостов [hq] и [br] и параметры подключения.
# Указываем IP-адреса согласно Таблице 3.
# Используем ansible_user для указания пользователя на удалённом хосте.
# SSH на HQ-SRV теперь на порту 2024 (или вариативном).
# // ВАРИАТИВНО: Порт 2024 для HQ-SRV может отличаться.
cat <<'EOF' > /etc/ansible/hosts
[hq]
192.168.1.10 ansible_user=sshuser ansible_port=2024
192.168.2.10 ansible_user=user
172.16.4.4 ansible_user=net_admin

[br]
172.16.5.5 ansible_user=net_admin
192.168.3.10 ansible_user=sshuser ansible_port=2024
EOF

# [root@br-srv ~]# --- Создание файла конфигурации (/etc/ansible/ansible.cfg) ---
# Устанавливаем путь к инвентарю, отключаем проверку ключей хоста
# и подавляем предупреждения об интерпретаторе Python.
cat <<'EOF' > /etc/ansible/ansible.cfg
[defaults]
inventory      = /etc/ansible/hosts
host_key_checking = False
interpreter_python = auto_silent
EOF
```

### Пример отчёта: Конфигурация Ansible на сервере BR-SRV

**Цель:**
Установить и настроить систему управления конфигурациями **Ansible** на сервере `BR-SRV` для обеспечения возможности автоматизированного управления другими узлами стенда (`HQ-SRV`, `HQ-CLI`, `HQ-RTR`, `BR-RTR`). Настроить беспарольный доступ по SSH-ключам и проверить базовую связность с управляемыми узлами.

**Выбор реализации:**
*   **Управляющий узел:** `BR-SRV` (192.168.3.10).
*   **Метод управления:** `Ansible`, использующий `SSH` для подключения к управляемым узлам.
*   **Аутентификация:** Использование `SSH-ключей` (пара ключей `root` пользователя на `BR-SRV`, публичный ключ скопирован на управляемые узлы).
*   **Инвентарь:** Статический файл `/etc/ansible/hosts`, определяющий группы хостов `[hq]` и `[br]` и параметры подключения (`ansible_user`, `ansible_port`).
*   **Конфигурация Ansible:** Файл `/etc/ansible/ansible.cfg` с указанием пути к инвентарю и отключением проверки SSH-ключей хоста (`host_key_checking = False`) для упрощения работы в тестовой среде.
*   **Рабочий каталог:** `/etc/ansible`.

**Основные шаги конфигурации:**

1.  **Подготовка SSH:**
    *   На серверах `HQ-SRV` и `BR-SRV` порт SSH изменен со стандартного 22 на **2024** (// ВАРИАТИВНО: Порт может отличаться согласно заданию для DNAT).
    *   На `HQ-CLI` включен SSH-сервер (порт 22).
2.  **Установка Ansible:** Пакет `ansible` установлен на `BR-SRV` с помощью `apt-get`.
3.  **Настройка SSH-доступа по ключам:**
    *   Сгенерирована пара SSH-ключей для пользователя `root` на `BR-SRV` (если отсутствовала).
    *   Публичный ключ `id_rsa.pub` скопирован на управляемые узлы (`HQ-SRV`, `HQ-CLI`, `HQ-RTR`, `BR-RTR`) в файл `authorized_keys` соответствующих пользователей (`sshuser`, `user`, `net_admin`) с помощью `ssh-copy-id`, с указанием корректных портов (**2024** для `HQ-SRV`, 22 для остальных).
4.  **Создание структуры Ansible:** Создан рабочий каталог `/etc/ansible`.
5.  **Создание инвентаря:** Создан файл `/etc/ansible/hosts` со списком управляемых узлов, сгруппированных по расположению (`[hq]`, `[br]`), и указанием пользователя для подключения (`ansible_user`) и порта (`ansible_port=2024` для `HQ-SRV`).
6.  **Создание файла конфигурации:** Создан файл `/etc/ansible/ansible.cfg` с базовыми настройками (путь к инвентарю, отключение проверки ключей).
7.  **Проверка связи:** Выполнена команда `ansible all -m ping` для проверки успешного подключения и выполнения модуля `ping` на всех управляемых узлах.

**Ключевые параметры:**
*   **Управляющий узел:** `BR-SRV`
*   **Управляемые узлы (с портами для Ansible):**
    *   `HQ-SRV`: 192.168.1.10, порт **2024** (ansible_user=sshuser) // ВАРИАТИВНО: Порт может отличаться.
    *   `HQ-CLI`: 192.168.2.10, порт 22 (ansible_user=user)
    *   `HQ-RTR`: 172.16.4.4, порт 22 (ansible_user=net_admin)
    *   `BR-RTR`: 172.16.5.5, порт 22 (ansible_user=net_admin)
*   **Файл инвентаря:** `/etc/ansible/hosts`
*   **Рабочий каталог:** `/etc/ansible`
*   **Метод аутентификации:** SSH-ключ пользователя `root@BR-SRV`.

**Итог:**
Система управления конфигурациями Ansible успешно установлена и настроена на сервере `BR-SRV`. Порты SSH на серверах приведены в соответствие с требованиями для последующей настройки DNAT. Налажен беспарольный доступ по SSH-ключам ко всем управляемым узлам стенда. Проверка связи с помощью модуля `ping` прошла успешно, подтвердив готовность системы к выполнению задач автоматизации.

### Проверка шага 4: Конфигурация Ansible и проверка связи

**Цель:** Убедиться, что Ansible установлен, настроен инвентарь и конфигурация, SSH-доступ по ключам работает, и все управляемые узлы успешно отвечают на команду `ping` модуля Ansible без ошибок и предупреждений.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Запуск модуля ping для всех хостов ---
# Ansible попытается подключиться ко всем хостам из /etc/ansible/hosts
# с указанными пользователями и портами (используя SSH ключи) и выполнить модуль ping.
ansible all -m ping

# Ожидаемый результат:
# Для КАЖДОГО из 4 хостов (192.168.1.10, 192.168.2.10, 172.16.4.4, 172.16.5.5)
# должен быть вывод SUCCESS с "ping": "pong".
# Пример:
# 192.168.1.10 | SUCCESS => {
#     "ansible_facts": {
#         "discovered_interpreter_python": "/usr/bin/python3"
#     },
#     "changed": false,
#     "ping": "pong"
# }
# ... (аналогично для остальных хостов) ...
#
# Важно: Не должно быть ошибок UNREACHABLE или FAILED, а также предупреждений (warnings).
```

---

## Шаг 5: Развёртывание приложений в Docker на сервере BR-SRV

**Цель:** Развернуть веб-приложение **MediaWiki** с использованием **Docker** и **Docker Compose** на сервере `BR-SRV`. Будет создан стек из двух контейнеров: `wiki` (MediaWiki) и `mariadb` (база данных). Приложение должно быть доступно снаружи по порту 8080 сервера `BR-SRV`.

**Практическое назначение:** Docker и Docker Compose позволяют упаковать приложение и его зависимости в изолированные контейнеры и легко управлять их развёртыванием и жизненным циклом.

**Предпосылки:** `BR-SRV` (`192.168.3.10`) имеет доступ в Интернет. Клиент `HQ-CLI` (`192.168.2.10`) имеет доступ к `BR-SRV`.

### Шаг 5.1: Установка Docker и Docker Compose на BR-SRV

#### BR-SRV

```bash
# [root@br-srv ~]# --- Установка Docker и Docker Compose ---
apt-get update && apt-get install -y docker-engine docker-compose
# [Проверка (опционально): версии]
# docker --version ; docker-compose --version ; docker compose version

# [root@br-srv ~]# --- Включение и запуск службы Docker ---
systemctl enable --now docker

# [root@br-srv ~]# --- Добавление пользователя в группу docker (опционально) ---
# Чтобы выполнять команды docker без sudo. Требует перелогина пользователя.
# usermod -aG docker sshuser
```

### Шаг 5.2: Подготовка конфигурационных файлов MediaWiki

#### BR-SRV

```bash
# [root@br-srv ~]# --- Создание Docker тома для БД ---
# Создаём именованный том для персистентного хранения данных MariaDB.
docker volume create dbvolume
# [Проверка (опционально): том]
# docker volume ls | grep dbvolume

# [root@br-srv ~]# --- Создание файла docker-compose (wiki.yml) в /home/sshuser ---
# Создаём файл wiki.yml.
cat << 'EOF' > /home/sshuser/wiki.yml
version: '3.7'
services:
  mediawiki:
    container_name: wiki
    image: mediawiki
    restart: always
    ports:
      - "8080:80" # Проброс порта 8080 хоста на 80 контейнера
    links:
      - mariadb:mariadb # Связь с БД по имени сервиса
    volumes:
      - images:/var/www/html/images # Том для изображений
      # Строка для LocalSettings.php будет раскомментирована позже
      # - ./LocalSettings.php:/var/www/html/LocalSettings.php
    depends_on:
      - mariadb # Запускать после mariadb
  mariadb:
    container_name: mariadb
    image: mariadb
    restart: always
    environment:
      MYSQL_DATABASE: mediawiki
      MYSQL_USER: wiki
      MYSQL_PASSWORD: WikiP@ssword
      MYSQL_RANDOM_ROOT_PASSWORD: 'yes' # Случайный пароль для root БД
    volumes:
      - dbvolume:/var/lib/mysql # Используем созданный том для данных БД
volumes:
  images: {} # Определяем том images (будет создан docker-compose)
  dbvolume:
    external: true # Указываем, что dbvolume создан заранее
EOF
# Устанавливаем владельца файла.
chown sshuser:sshuser wiki.yml
# [Проверка (опционально): файл]
# ls -l /home/sshuser/wiki.yml ; cat /home/sshuser/wiki.yml
```

### Шаг 5.3: Превентивный перезапуск Docker (Рекомендуется)

Рекомендуется перезапустить службу Docker перед первым запуском контейнеров для сброса возможного некорректного состояния сети Docker.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Перезапуск Docker ---
systemctl stop docker && sleep 5 && systemctl start docker && sleep 10
# [Проверка (опционально): статус]
# systemctl is-active docker
```

### Шаг 5.4: Запуск стека контейнеров

#### BR-SRV

```bash
# [root@br-srv /home/sshuser]# --- Запуск стека ---
# Используем 'docker compose' (с пробелом) и ключ -f для указания файла.
# Ключ -d запускает контейнеры в фоновом режиме.
docker compose -f /home/sshuser/wiki.yml up -d
# Процесс скачает образы, если их нет локально, и запустит контейнеры.
```

### Шаг 5.5: Первоначальная настройка MediaWiki через веб-интерфейс

После первого запуска контейнера `wiki` необходимо выполнить первоначальную настройку **MediaWiki** через веб-интерфейс. Это создаст файл `LocalSettings.php` с конфигурацией.

**Действия:**
1.  На машине **HQ-CLI** (или любой другой с доступом к `BR-SRV`) откройте веб-браузер (Яндекс Браузер) и перейдите по адресу `http://192.168.3.10:8080`.
2.  Нажмите на ссылку **"Please set up the wiki first."**.
3.  **Язык:** Выберите `ru - русский`, нажмите "Continue".
4.  **Проверка окружения:** Должно быть "The environment has been checked. You can install MediaWiki." Нажмите "Continue".
5.  **Подключение к базе данных:**
    *   Тип базы данных: `MariaDB, MySQL, or compatible`.
    *   Хост базы данных: `mariadb` (имя сервиса Docker).
    *   Имя базы данных: `mediawiki` (из `environment` в `wiki.yml`).
    *   Пользователь базы данных: `wiki` (из `environment`).
    *   Пароль пользователя базы данных: `WikiP@ssword` (из `environment`).
    *   Нажмите "Continue".
6.  **Настройки базы данных:**
    *   **Установите флажок** "Использовать ту же учётную запись, что и для установки".
    *   Нажмите "Continue".
7.  **Название и учётная запись:**
    *   Название вики: `Demo-Wiki` (или любое другое).
    *   Имя пользователя (администратора): `wikiadmin`.
    *   Пароль: `WikiP@ssword`.
    *   Адрес email: `admin@example.com` (можно фиктивный).
    *   Выберите "Хватит уже, просто установите вики".
    *   Нажмите "Continue".
8.  **Настройки:** Оставьте настройки по умолчанию (убедитесь, что выбрана тема, например, Vector). Нажмите "Continue".
9.  **Установка:** Нажмите "Continue".
10. **Завершение:** Установка завершена.
11. **Скачайте `LocalSettings.php`:** Нажмите кнопку **"Download LocalSettings.php"** и сохраните файл (обычно в папку `Загрузки`).

### Шаг 5.6: Размещение `LocalSettings.php` и перезапуск стека

Копируем скачанный `LocalSettings.php` на `BR-SRV`, раскомментируем строку монтирования в `wiki.yml` и перезапускаем стек.

#### HQ-CLI -> BR-SRV (Копирование файла)

```bash
# [user@hq-cli ~]$ --- Копирование LocalSettings.php ---
# Замените путь к скачанному файлу, если он отличается.
# // ВАРИАТИВНО: Порт SSH для BR-SRV (здесь 2024) может отличаться. Используйте порт из вашего варианта задания.
# Укажите порт SSH 2024 для BR-SRV.
scp -P 2024 /home/user/Downloads/LocalSettings.php sshuser@192.168.3.10:/home/sshuser/
# Введите пароль пользователя sshuser (P@ssw0rd).
```

#### BR-SRV (Обновление конфига и перезапуск)

```bash
# [root@br-srv ~]# --- Проверка наличия файла /home/sshuser/LocalSettings.php (опционально) ---
# ls -l /home/sshuser/LocalSettings.php
# Ожидаемый результат: Файл LocalSettings.php существует в /home/sshuser.

# [root@br-srv ~]# --- Редактирование /home/sshuser/wiki.yml (раскомментировать volume) ---
# Раскомментируем строку монтирования LocalSettings.php в файле /home/sshuser/wiki.yml.
# Команда sed ищет строку, начинающуюся с пробелов, затем '# - ./LocalSettings.php:...',
# и удаляет '# ' в начале.
sed -i 's/^\([[:space:]]*\)# - \(\.\/LocalSettings\.php:.*\)$/\1- \2/' /home/sshuser/wiki.yml
# [Проверка (опционально): изменение в файле]
grep -- '- \./LocalSettings\.php:' /home/sshuser/wiki.yml
# Ожидаемый результат: Строка '- ./LocalSettings.php:/var/www/html/LocalSettings.php' без '#'.

# [root@br-srv ~]# --- Остановка текущих контейнеров ---
# Используем 'docker compose' (с пробелом) и абсолютный путь к файлу конфигурации.
docker compose -f /home/sshuser/wiki.yml stop

# [root@br-srv ~]# --- Запуск стека с обновлённой конфигурацией ---
# Docker Compose пересоздаст контейнер 'wiki' с подключённым LocalSettings.php.
# Используем 'docker compose' (с пробелом) и абсолютный путь.
docker compose -f /home/sshuser/wiki.yml up -d
```

### Пример отчёта: Развёртывание MediaWiki в Docker на BR-SRV

**Цель:** Развернуть веб-приложение **MediaWiki** на сервере `BR-SRV` с использованием технологий контейнеризации **Docker** и **Docker Compose**. Цель — обеспечить быстрое и изолированное развёртывание приложения вместе с его зависимостью в виде базы данных **MariaDB**, сделав MediaWiki доступной по сети.

**Выбор реализации:**
*   **Контейнеризация:** Использованы `Docker` для запуска изолированных контейнеров и `Docker Compose` для управления многоконтейнерным приложением (стеком).
*   **Образы Docker:**
    *   `mediawiki`: Официальный образ для MediaWiki.
    *   `mariadb`: Официальный образ для СУБД MariaDB.
*   **Сетевая конфигурация:** MediaWiki прослушивает порт `8080` на хост-машине `BR-SRV`, который пробрасывается на порт `80` внутри контейнера `wiki`.
*   **Хранение данных:**
    *   Для базы данных MariaDB используется именованный Docker-том `dbvolume`, созданный заранее, для обеспечения персистентности данных.
    *   Для изображений MediaWiki используется именованный Docker-том `images`, управляемый Docker Compose.
    *   Конфигурационный файл `LocalSettings.php` монтируется в контейнер `wiki` из файловой системы хоста.
*   **Файл конфигурации стека:** `wiki.yml` (в `/home/sshuser/` на `BR-SRV`) описывает сервисы `mediawiki` и `mariadb`, их зависимости, порты, тома и переменные окружения для базы данных.

**Основные шаги конфигурации:**

1.  **Установка Docker и Docker Compose:** Пакеты `docker-engine` и `docker-compose` установлены на `BR-SRV`. Служба `docker` включена и запущена.
2.  **Подготовка томов и конфигурации:**
    *   Создан именованный Docker-том `dbvolume` для данных MariaDB.
    *   Создан файл `docker-compose.yml` (именован `wiki.yml` в пособии) с описанием сервисов `mediawiki` и `mariadb`, их связей, портов (8080:80 для MediaWiki), переменных окружения для БД (`MYSQL_DATABASE`, `MYSQL_USER`, `MYSQL_PASSWORD`) и томов.
3.  **Первый запуск и настройка MediaWiki:**
    *   Стек запущен командой `docker compose -f /home/sshuser/wiki.yml up -d`.
    *   Выполнена первоначальная настройка MediaWiki через веб-интерфейс (доступ по `http://192.168.3.10:8080`), включая указание параметров подключения к БД (`mariadb` как хост, имя БД `mediawiki`, пользователь `wiki`, пароль `WikiP@ssword`).
    *   По завершении установки скачан файл `LocalSettings.php`.
4.  **Интеграция `LocalSettings.php`:**
    *   Скачанный файл `LocalSettings.php` скопирован на `BR-SRV` в каталог `/home/sshuser/`.
    *   В файле `wiki.yml` раскомментирована строка, отвечающая за монтирование `./LocalSettings.php` в `/var/www/html/LocalSettings.php` внутри контейнера `wiki`.
5.  **Перезапуск стека:** Контейнеры остановлены (`docker compose stop`) и запущены снова (`docker compose up -d`) для применения нового монтирования `LocalSettings.php`.

**Схема 6. Визуализация стека MediaWiki на BR-SRV**

```text
+-----------------------------------------------------------+
| Сервер BR-SRV (Хост-система, IP: 192.168.3.10)            |
|-----------------------------------------------------------|
|                                                           |
|   +-----------------+     +---------------------------+   |
|   | Docker Engine   |---->| Docker Compose (wiki.yml) |   |
|   +-----------------+     +---------------------------+   |
|          ^                                                |
|          | (Управляет)                                    |
|          |                                                |
|   HTTP Запросы (Порт 8080)                                |
|          |                                                |
|   +------V---------------------------------------+        |
|   | Контейнер: wiki (Образ: mediawiki)           |        |
|   | - Порт 80 (внутр.) <--- 8080 (хост)          |        |
|   | - /var/www/html/LocalSettings.php (монтир.)  | -------+---> /home/sshuser/LocalSettings.php (хост)
|   | - /var/www/html/images (том: images)         | -------+---> Том 'images'
|   | - Связь с 'mariadb'                          |        |
|   +---------------------^------------------------+        |
|                         | (Подключение к БД)              |
|   +---------------------V------------------------+        |
|   | Контейнер: mariadb (Образ: mariadb)          |        |
|   | - MYSQL_DATABASE=mediawiki                   |        |
|   | - MYSQL_USER=wiki                            |        |
|   | - MYSQL_PASSWORD=WikiP@ssword                |        |
|   | - /var/lib/mysql (том: dbvolume)             | -------+---> Том 'dbvolume'
|   +----------------------------------------------+        |
|                                                           |
+-----------------------------------------------------------+
```

**Итог:** 
Веб-приложение MediaWiki успешно развёрнуто на сервере `BR-SRV` с использованием Docker и Docker Compose. Приложение работает в изолированных контейнерах, его данные (база данных, изображения) хранятся персистентно в Docker-томах. MediaWiki доступна по адресу `http://192.168.3.10:8080`. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 5: Развёртывание приложений в Docker

**Цель:** Убедиться, что контейнеры запущены, **MediaWiki** работает с `LocalSettings.php` и доступна через веб-интерфейс.

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка контейнеров ---
docker ps
# Ожидаемый результат: `wiki` и `mariadb` в статусе `Up`. Время запуска `wiki` должно быть недавним.
# [root@br-srv ~]# --- Проверка монтирования (опционально) ---
# docker exec wiki ls -l /var/www/html/LocalSettings.php
# Ожидаемый результат: Файл LocalSettings.php виден внутри контейнера.
```

#### HQ-CLI (или другая машина)

1.  Откройте веб-браузер (Яндекс Браузер) и перейдите по адресу `http://192.168.3.10:8080`.
2.  **Ожидаемый результат:** Отображается главная страница **MediaWiki** (`Demo-Wiki`). Страницы установки быть не должно.
3.  Попробуйте войти как `wikiadmin` с паролем, заданным при установке (например, `WikiP@ssword`).
4.  **Ожидаемый результат:** Успешный вход.

---

## Шаг 6: Настройка статической трансляции портов (Port Forwarding)

**Цель:** Сконфигурировать правила **статической трансляции сетевых адресов назначения (DNAT)** на маршрутизаторах `HQ-RTR` и `BR-RTR` для обеспечения доступа к внутренним службам (`SSH` на серверах, `wiki` на `BR-SRV`) из внешних сетей через публичные IP-адреса маршрутизаторов.

**Практическое назначение:** DNAT (проброс портов) является стандартным механизмом для публикации внутренних сервисов. Он позволяет внешним клиентам подключаться к внутренним серверам, используя внешний IP маршрутизатора.

**Важное замечание:** 

Задание предполагает настройку DNAT для доступа к SSH на серверах `HQ-SRV` и `BR-SRV` через внешний порт **2024**. Этот порт для проброса может отличаться, например, 3015. Используйте порт из вашего варианта задания.

Для корректной работы DNAT, SSH-серверы на `HQ-SRV` и `BR-SRV` должны прослушивать именно этот порт (в нашем примере, 2024). **Это изменение порта со стандартного 22 на 2024 (или ваш вариативный порт) должно было быть выполнено в Шаге 4.1 "Подготовка SSH-серверов" Модуля 2.**

---

### Шаг 6.1: Проверка конфигурации порта SSH на серверах (`HQ-SRV`, `BR-SRV`)

#### HQ-SRV (Проверка)

```bash
# [root@hq-srv ~]# --- Проверка порта прослушивания SSH ---
# // ВАРИАТИВНО: Проверяйте порт, который вы настроили (например, 2024).
ss -tlpn | grep :2024
# Ожидаемый результат: Вывод должен содержать строку, указывающую, что служба 'sshd'
# прослушивает (LISTEN) указанный порт на всех или конкретном IP-адресе.
# Например: LISTEN 0 128 *:2024 *:* users:(("sshd",pid=...,fd=...))
```

#### BR-SRV

```bash
# [root@br-srv ~]# --- Проверка порта прослушивания SSH ---
# // ВАРИАТИВНО: Проверяйте порт, который вы настроили (например, 2024).
ss -tlpn | grep :2024
# Ожидаемый результат: Аналогично HQ-SRV, sshd должен слушать указанный порт.
```

Если проверка показывает, что серверы не слушают нужный порт, вернитесь к Шагу 4.1 и выполните команды по изменению порта SSH.

---

### Шаг 6.2: Настройка правил DNAT на маршрутизаторах (`HQ-RTR`, `BR-RTR`)

Создать правила `iptables` в таблице `nat` для перенаправления входящего трафика с внешних интерфейсов на соответствующие внутренние серверы и порты.

**Метод:** Использование `iptables` с опциями `-t nat -A PREROUTING -i <внешний_интерфейс> -p tcp --dport <внешний_порт> -j DNAT --to-destination <внутренний_IP>:<внутренний_порт>`. Очистка цепочки `-F PREROUTING` перед добавлением гарантирует отсутствие дубликатов. Правила сохраняются через `iptables-save`.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Очистка правил PREROUTING таблицы nat ---
# Гарантирует отсутствие старых правил DNAT.
iptables -t nat -F PREROUTING

# [root@hq-rtr ~]# --- Добавление правила DNAT для SSH на HQ-SRV ---
# Перенаправление трафика с внешнего интерфейса ens19:2024 на HQ-SRV (192.168.1.10:2024).
# Используем внешний интерфейс ens19 согласно Таблице 3.
# // ВАРИАТИВНО: Порт SSH для проброса (здесь 2024) может отличаться. Укажите нужный внешний и внутренний порт.
iptables -t nat -A PREROUTING -i ens19 -p tcp --dport 2024 -j DNAT --to-destination 192.168.1.10:2024

# [root@hq-rtr ~]# --- Сохранение конфигурации iptables ---
# Сохраняет все активные правила (включая MASQUERADE) в файл.
iptables-save > /etc/sysconfig/iptables
# [Проверка (опционально): Содержимое файла конфигурации]
# cat /etc/sysconfig/iptables | grep -- '-A PREROUTING'

# [root@hq-rtr ~]# --- Перезапуск службы iptables ---
# Применяет сохранённые правила.
systemctl restart iptables
```

#### BR-RTR

```bash
# [root@br-rtr ~]# --- Очистка правил PREROUTING таблицы nat ---
iptables -t nat -F PREROUTING

# [root@br-rtr ~]# --- Добавление правила DNAT для Wiki на BR-SRV ---
# Перенаправление трафика с внешнего интерфейса ens19:80 на BR-SRV (192.168.3.10:8080).
# Используем внешний интерфейс ens19 согласно Таблице 3.
iptables -t nat -A PREROUTING -i ens19 -p tcp --dport 80 -j DNAT --to-destination 192.168.3.10:8080

# [root@br-rtr ~]# --- Добавление правила DNAT для SSH на BR-SRV ---
# Перенаправление трафика с внешнего интерфейса ens19:2024 на BR-SRV (192.168.3.10:2024).
# // ВАРИАТИВНО: Порт SSH для проброса (здесь 2024) может отличаться. Укажите нужный внешний и внутренний порт.
iptables -t nat -A PREROUTING -i ens19 -p tcp --dport 2024 -j DNAT --to-destination 192.168.3.10:2024

# [root@br-rtr ~]# --- Сохранение конфигурации iptables ---
iptables-save > /etc/sysconfig/iptables
# [Проверка (опционально): Содержимое файла конфигурации]
# cat /etc/sysconfig/iptables | grep -- '-A PREROUTING'

# [root@br-rtr ~]# --- Перезапуск службы iptables ---
systemctl restart iptables
```

---

### Пример отчёта: Настройка статической трансляции портов (DNAT)

**Цель:** Обеспечить доступ из внешней сети (со стороны ISP) к внутренним службам, размещённым на серверах `HQ-SRV` и `BR-SRV`, через публичные IP-адреса маршрутизаторов `HQ-RTR` и `BR-RTR`. Это достигается путём настройки правил статической трансляции сетевых адресов назначения (DNAT или Port Forwarding).

**Выбор реализации:**
*   **Технология:** `iptables` – стандартный инструмент межсетевого экрана и NAT в Linux.
*   **Таблица и цепочка:** Правила DNAT добавляются в таблицу `nat`, цепочку `PREROUTING`.
*   **Маршрутизаторы:**
    *   `HQ-RTR` (внешний IP `172.16.4.4` на `ens19`) пробрасывает порт для SSH.
    *   `BR-RTR` (внешний IP `172.16.5.5` на `ens19`) пробрасывает порт для Wiki и SSH.
*   **Пробрасываемые службы и порты:**
    *   **SSH на `HQ-SRV`:** Внешний `HQ-RTR:2024` (TCP) -> внутренний `HQ-SRV (192.168.1.10):2024` (TCP). *Примечание: Внешний и внутренний порт 2024 является вариативным и должен соответствовать заданию.*
    *   **Wiki на `BR-SRV`:** Внешний `BR-RTR:80` (TCP) -> внутренний `BR-SRV (192.168.3.10):8080` (TCP).
    *   **SSH на `BR-SRV`:** Внешний `BR-RTR:2024` (TCP) -> внутренний `BR-SRV (192.168.3.10):2024` (TCP). *Примечание: Внешний и внутренний порт 2024 является вариативным и должен соответствовать заданию.*
*   **Сохранение правил:** Правила `iptables` сохраняются в `/etc/sysconfig/iptables` для автоматической загрузки при старте службы.

**Основные шаги конфигурации:**

1.  **Проверка портов SSH на серверах:** Убедились, что SSH-серверы на `HQ-SRV` и `BR-SRV` прослушивают порт `2024` (или другой порт, указанный в задании для DNAT). Это изменение было сделано в Шаге 4.1.
2.  **Настройка DNAT на `HQ-RTR`:**
    *   Очищена цепочка `PREROUTING` таблицы `nat`.
    *   Добавлено правило `iptables` для перенаправления TCP-трафика с интерфейса `ens19` порта `2024` на `192.168.1.10:2024`.
    *   Правила сохранены (`iptables-save`), служба `iptables` перезапущена.
3.  **Настройка DNAT на `BR-RTR`:**
    *   Очищена цепочка `PREROUTING` таблицы `nat`.
    *   Добавлено правило `iptables` для перенаправления TCP-трафика с интерфейса `ens19` порта `80` на `192.168.3.10:8080` (для Wiki).
    *   Добавлено правило `iptables` для перенаправления TCP-трафика с интерфейса `ens19` порта `2024` на `192.168.3.10:2024` (для SSH).
    *   Правила сохранены (`iptables-save`), служба `iptables` перезапущена.
4.  **Проверка:** Выполнена проверка доступности служб из сети ISP по внешним IP-адресам маршрутизаторов и соответствующим проброшенным портам.

**Схема 7. Визуализация DNAT**

```text
                                          +------------+
                                          |  Интернет  | (например, ISP)
                                          +------------+
                                                |
                    +---------------------------+---------------------------+
                    |                                                       |
  +--------------------------------------------+     +--------------------------------------------+
  |  HQ-RTR (ens19)         WAN IP: 172.16.4.4 |     |  BR-RTR (ens19)         WAN IP: 172.16.5.5 |
  |--------------------------------------------|     |--------------------------------------------|
  |  DNAT:                                     |     |  DNAT:                                     |
  |    TCP 2024 -> 192.168.1.10:2024 (SSH)     |     |    TCP 80   -> | 192.168.3.10:8080 (Wiki)  |
  |                                            |     |    TCP 2024 -> | 192.168.3.10:2024 (SSH)   |
  +--------------------------------------------+     +--------------------------------------------+
                    |                                                       |
                    |                                                       |
  +--------------------------------------------+     +--------------------------------------------+
  |  HQ-SRV                                    |     |  BR-SRV                                    |
  |    SSH: 2024                               |     |    Wiki: 8080                              |
  |                                            |     |    SSH:  2024                              |
  +--------------------------------------------+     +--------------------------------------------+
```

**Итог:**
На маршрутизаторах `HQ-RTR` и `BR-RTR` успешно настроена статическая трансляция портов (DNAT). Службы SSH на `HQ-SRV` и `BR-SRV` (порт `2024` // ВАРИАТИВНО), а также Wiki на `BR-SRV` (порт `80` внешний, `8080` внутренний) теперь доступны из внешней сети (со стороны ISP) через соответствующие внешние IP-адреса и порты маршрутизаторов. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 6: Настройка статической трансляции портов (Port Forwarding)

**Цель:** Убедиться, что SSH-серверы прослушивают порт `2024`, правила DNAT активны и корректны, и доступ к внутренним службам через **внешние** IP-адреса маршрутизаторов из **внешней сети** (ISP) функционирует.

#### Серверы (HQ-SRV, BR-SRV)

```bash
# [root@hq-srv ~]# --- Проверка порта SSH ---
# // ВАРИАТИВНО: Проверяйте нужный порт.
ss -tlpn | grep :2024 
# Ожидаемый результат: Строка с 'sshd' LISTEN на порту 2024.

# [root@br-srv ~]# --- Проверка порта SSH ---
# // ВАРИАТИВНО: Проверяйте нужный порт.
ss -tlpn | grep :2024 
# Ожидаемый результат: Строка с 'sshd' LISTEN на порту 2024.
# [root@br-srv ~]# --- Проверка порта Wiki (Docker) ---
ss -tlpn | grep :8080
# Ожидаемый результат: Строка с 'docker-proxy' LISTEN на порту 8080.
```

#### Маршрутизаторы (HQ-RTR, BR-RTR)

```bash
# [root@hq-rtr ~]# --- Проверка активных правил DNAT ---
iptables -t nat -L PREROUTING -nv --line-numbers
# Ожидаемый результат: Одно правило DNAT для dpt:2024 на 192.168.1.10:2024 для интерфейса ens19. // ВАРИАТИВНО: Порты могут отличаться.

# [root@br-rtr ~]# --- Проверка активных правил DNAT ---
iptables -t nat -L PREROUTING -nv --line-numbers
# Ожидаемый результат: Два правила DNAT для интерфейса ens19:
# 1. dpt:80 на 192.168.3.10:8080
# 2. dpt:2024 на 192.168.3.10:2024
# // ВАРИАТИВНО: Порты могут отличаться.
```

#### ISP (Тестирование доступа из внешней сети)

```bash
# [root@hq-cli ~]# --- Установка NFS-клиента (если необходимо) ---
apt-get update && apt-get install -y curl
# [root@ISP ~]# --- Проверка доступа к Wiki на BR-SRV через внешний IP BR-RTR ---
Используем WAN IP BR-RTR (172.16.5.5) и порт 80.
curl -i http://172.16.5.5 | grep 'HTTP/1.1 301 Moved Permanently'
# Ожидаемый результат: Обнаружение строки 'HTTP/1.1 301 Moved Permanently'.

# [root@ISP ~]# --- Проверка доступа к SSH на HQ-SRV через внешний IP HQ-RTR ---
# Используем WAN IP HQ-RTR (172.16.4.4) и порт 2024. Подключаемся как 'sshuser'.
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh -p 2024 -o ConnectTimeout=10 sshuser@172.16.4.4 'echo HQ_DNAT_OK_FROM_ISP && exit'
# Ожидаемый результат: Запрос пароля пользователя sshuser (P@ssw0rd) или, при настроенных ключах,
# вывод 'HQ_DNAT_OK_FROM_ISP' и успешный выход. Сообщение 'Connection refused' отсутствовать.

# [root@ISP ~]# --- Проверка доступа к SSH на BR-SRV через внешний IP BR-RTR ---
# Используем WAN IP BR-RTR (172.16.5.5) и порт 2024. Подключаемся как 'sshuser'.
# // ВАРИАТИВНО: Используйте правильный порт (-p 2024 или другой).
ssh -p 2024 -o ConnectTimeout=10 sshuser@172.16.5.5 'echo BR_DNAT_OK_FROM_ISP && exit'
# Ожидаемый результат: Запрос пароля пользователя sshuser (P@ssw0rd) или, при настроенных ключах,
# вывод 'BR_DNAT_OK_FROM_ISP' и успешный выход. Сообщение 'Connection refused' отсутствовать.
```

---

## Шаг 7: Запуск сервиса Moodle на сервере HQ-SRV

**Цель:** Развернуть систему управления обучением (LMS) **Moodle** на сервере `HQ-SRV`, используя веб-сервер **Apache**, СУБД **MariaDB** и **PHP**. Настроить базу данных, веб-сервер и выполнить первоначальную установку Moodle через веб-интерфейс.

**Практическое назначение:** Moodle является популярной LMS с открытым исходным кодом. Apache, MariaDB и PHP (LAMP-стек) — стандартное и надёжное окружение для развёртывания веб-приложений на Linux.

**Предпосылки:** Сервер `HQ-SRV` (`192.168.1.10`) настроен согласно **Таблице 3**, имеет доступ в Интернет для установки пакетов и скачивания Moodle. Клиент `HQ-CLI` (`192.168.2.10`) имеет сетевой доступ к `HQ-SRV`.

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

### Шаг 7.1: Установка необходимых пакетов (Apache, MariaDB, PHP)

Установим веб-сервер Apache, СУБД MariaDB, PHP и все необходимые PHP-расширения для Moodle.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Установка Apache, MariaDB, PHP и расширений ---
# Обновляем список пакетов и устанавливаем необходимые компоненты.
# Обратите внимание на версию PHP (8.2) и соответствующие пакеты модулей.
apt-get update && apt-get install -y apache2 mariadb-server php8.2 apache2-mod_php8.2 php8.2-gd php8.2-curl php8.2-intl php8.2-mysqli php8.2-xml php8.2-xmlrpc php8.2-zip php8.2-soap php8.2-mbstring php8.2-opcache php8.2-json php8.2-ldap php8.2-xmlreader php8.2-fileinfo php8.2-sodium unzip
    # Для распаковки Moodle
# [Проверка (опционально): пакеты]
# rpm -q apache2 mariadb-server php8.2
```

### Шаг 7.2: Запуск и базовая настройка служб

Включим автозапуск и запустим службы Apache и MariaDB. Выполним базовую безопасную настройку MariaDB.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Включение и запуск служб ---
# Используем имена служб для ALT Linux: httpd2 для Apache, mysqld для MariaDB.
systemctl enable --now httpd2.service mysqld.service
# [Проверка (опционально): статус служб]
# systemctl is-active httpd2 mysqld

# [root@hq-srv ~]# --- Первоначальная настройка MariaDB ---
# Устанавливаем пароль root для MariaDB ('P@ssw0rd'), удаляем анонимных пользователей,
# запрещаем удалённый вход root, удаляем тестовую базу.
mysql_secure_installation <<EOF

y
P@ssw0rd
P@ssw0rd
y
y
y
y
EOF
# Примечание: Первый Enter обрабатывает запрос на смену пароля Unix Socket пользователя root (оставляем как есть).
```

### Шаг 7.3: Настройка базы данных MariaDB для Moodle

Создадим базу данных `moodledb` и пользователя `moodle` с паролем `P@ssw0rd`, предоставив ему все необходимые права на эту базу данных.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Создание БД и пользователя Moodle ---
# Выполняем SQL-запросы неинтерактивно через mysql -u root -pПАРОЛЬ.
# Используем CREATE DATABASE IF NOT EXISTS и CREATE USER IF NOT EXISTS для идемпотентности.
mysql -u root -p'P@ssw0rd' <<EOF
CREATE DATABASE IF NOT EXISTS moodledb DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
CREATE USER IF NOT EXISTS 'moodle'@'localhost' IDENTIFIED BY 'P@ssw0rd';
GRANT ALL PRIVILEGES ON moodledb.* TO 'moodle'@'localhost';
FLUSH PRIVILEGES;
EOF
# [Проверка (опционально): база данных и пользователь]
# mysql -u root -p'P@ssw0rd' -e "SHOW DATABASES LIKE 'moodledb';"
# mysql -u root -p'P@ssw0rd' -e "SELECT user, host FROM mysql.user WHERE user = 'moodle';"
```

### Шаг 7.4: Скачивание и распаковка Moodle

Скачаем архив Moodle указанной версии (4.5.4 для Moodle 4.5), распакуем его в корневой каталог веб-сервера и установим правильные права доступа.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Скачивание архива Moodle ---
# Загружаем архив Moodle версии 4.5.4 во временный каталог.
# Примечание: Ссылка может устареть, проверьте актуальность на moodle.org.
curl -L https://download.moodle.org/download.php/direct/stable405/moodle-4.5.4.zip -o /tmp/moodle.zip
# [Проверка (опционально): скачанный файл]
# ls -lh /tmp/moodle.zip

# [root@hq-srv ~]# --- Распаковка Moodle ---
# Распаковываем содержимое в /var/www/html.
# Сначала удаляем стандартный index.html Apache, если он есть.
rm -f /var/www/html/index.html
unzip /tmp/moodle.zip -d /var/www/html
# Перемещаем файлы из созданной папки 'moodle' в корень /var/www/html.
mv /var/www/html/moodle/* /var/www/html/
# [Проверка (опционально): содержимое /var/www/html]
# ls -l /var/www/html | grep 'config-dist.php'

# [root@hq-srv ~]# --- Установка прав на каталог Moodle ---
# Устанавливаем владельца apache2:apache2 (пользователь/группа веб-сервера в ALT Linux).
chown -R apache2:apache2 /var/www/html
# [Проверка (опционально): права]
# ls -ld /var/www/html
```

### Шаг 7.5: Создание каталога данных Moodle

Создадим каталог `/var/www/moodledata`, который Moodle будет использовать для хранения загруженных файлов. Этот каталог должен быть вне корневого каталога веб-сервера и доступен для записи веб-серверу.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Создание каталога данных ---
mkdir /var/www/moodledata
# [root@hq-srv ~]# --- Установка прав на каталог данных ---
chown apache2:apache2 /var/www/moodledata
# Даём права группе apache2
chmod 770 /var/www/moodledata 
# [Проверка (опционально): права]
# ls -ld /var/www/moodledata
```

### Шаг 7.6: Настройка PHP

Изменим параметр `max_input_vars` в конфигурации PHP, как рекомендуется для Moodle.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Редактирование php.ini ---
# Используем sed для раскомментирования и изменения значения max_input_vars.
# Файл конфигурации для модуля Apache PHP в ALT Linux.
PHP_INI_FILE="/etc/php/8.2/apache2-mod_php/php.ini"
# Сначала раскомментируем строку (если она закомментирована)
sed -i 's/^[[:space:]]*;[[:space:]]*max_input_vars[[:space:]]*=.*$/max_input_vars = 1000/' "$PHP_INI_FILE"
# Затем устанавливаем нужное значение (5000)
sed -i 's/^[[:space:]]*max_input_vars[[:space:]]*=.*$/max_input_vars = 5000/' "$PHP_INI_FILE"
# [Проверка (опционально): значение в php.ini]
# grep '^[[:space:]]*max_input_vars' "$PHP_INI_FILE"
```

### Шаг 7.7: Перезапуск Apache

Перезапустим Apache, чтобы применить изменения в конфигурации PHP и подхватить файлы Moodle.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Перезапуск Apache ---
systemctl restart httpd2.service
```

      
### Шаг 7.8: Первоначальная настройка Moodle через веб-интерфейс

Теперь необходимо завершить установку Moodle через веб-интерфейс. Эти шаги выполняются на машине `HQ-CLI` (или любой другой машине с графическим интерфейсом и доступом к `HQ-SRV`). Для доступа к веб-интерфейсу Moodle потребуется веб-браузер. Вы можете использовать предустановленный **Mozilla Firefox** или другой доступный браузер.

#### HQ-CLI (Выполнять в веб-браузере)

1.  **Запустите веб-браузер** на `HQ-CLI` (например, Mozilla Firefox из меню приложений или командой `firefox` в терминале от имени обычного пользователя).
2.  **В адресной строке браузера** перейдите по адресу: `http://192.168.1.10/`
    *   **Ожидание:** Открытие страницы установки Moodle. Если нет, попробуйте `http://192.168.1.10/install.php`.
3.  **Язык:**
    *   Выберите **Язык:** `Русский (ru)`.
    *   Нажмите кнопку **"Далее »"**.
4.  **Пути:**
    *   **Проверьте пути:** `Веб-адрес: http://192.168.1.10`, `Каталог Moodle: /var/www/html`, `Каталог данных: /var/www/moodledata`.
    *   Нажмите кнопку **"Далее »"**.
5.  **Драйвер базы данных:**
    *   Выберите **Тип:** `MariaDB («родной»/mariadb)`.
    *   Нажмите кнопку **"Далее »"**.
6.  **Настройки базы данных:**
    *   **Сервер баз данных:** `localhost`
    *   **Название базы данных:** `moodledb`
    *   **Пользователь базы данных:** `moodle`
    *   **Пароль:** `P@ssw0rd`
    *   **Префикс имён таблиц:** `mdl_` (оставить по умолчанию).
    *   **Порт базы данных:** (оставить пустым).
    *   **Сокет Unix:** (оставить пустым).
    *   Нажмите кнопку **"Далее »"**.
7.  **Авторское право:**
    *   Ознакомьтесь с условиями.
    *   Нажмите кнопку **"Продолжить"**.
8.  **Проверка сервера:**
    *   **Просмотрите результаты.** Предупреждения для версии MariaDB или расширения `exif` допустимы. Все обязательные проверки должны быть **OK**.
    *   Прокрутите страницу вниз и нажмите кнопку **"Продолжить"**.
9.  **Установка системы:**
    *   **Дождитесь завершения** процесса установки (отображается лог).
    *   Прокрутите страницу вниз и нажмите кнопку **"Продолжить"**.
10. **Настройка учётной записи администратора:**
    *   **Логин:** `admin`
    *   **Новый пароль:** `P@ssw0rd` (убедитесь, что он соответствует требованиям сложности, если они отображаются).
    *   **Имя:** `Администратор` (или любое другое).
    *   **Фамилия:** `Пользователь` (или любое другое).
    *   **Адрес электронной почты:** `admin@example.com` (или любое другое).
    *   Заполните или оставьте по умолчанию остальные поля (город, страна, часовой пояс).
    *   Нажмите кнопку **"Обновить профиль"**.
11. **Настройки главной страницы:**
    *   **Полное название сайта:** `9` (**ВАЖНО:** Введите номер вашего рабочего места!)
    *   **Краткое название сайта:** `moodle` (или любое другое).
    *   **Описание главной страницы:** (можно оставить пустым).
    *   **Настройки местонахождения -> Часовой пояс по умолчанию:** Выберите ваш актуальный часовой пояс (например, `Азия/Владивосток`).
    *   **Контакты службы поддержки -> Электронная почта техподдержки:** `admin@example.com` (или любое другое).
    *   Прокрутите страницу вниз и нажмите кнопку **"Сохранить изменения"**.
12. **Завершение:**
    *   **Ожидание:** Вы должны быть перенаправлены на главную страницу Moodle. Установка завершена.

### Пример отчёта: Запуск сервиса Moodle на HQ-SRV

**Цель:**
Развернуть систему управления обучением Moodle на сервере `HQ-SRV`, используя стандартный веб-стек (Apache, MariaDB, PHP) и выполнить базовую настройку.

**Выбор реализации:**
*   **Веб-сервер:** `Apache 2` (пакет `apache2` в ALT Linux, служба `httpd2`).
*   **СУБД:** `MariaDB` (пакет `mariadb-server`, служба `mysqld`).
*   **Язык:** `PHP 8.2` с необходимыми для Moodle расширениями.
*   **База данных:** Создана БД `moodledb`.
*   **Пользователь БД:** Создан пользователь `moodle` с паролем `P@ssw0rd`, имеющий полный доступ к `moodledb`.
*   **Файлы Moodle:** Скачана версия Moodle 4.5.4, распакована в `/var/www/html`.
*   **Каталог данных:** Создан каталог `/var/www/moodledata` вне веб-корня.
*   **Права доступа:** Владельцем файлов Moodle и каталога данных назначен пользователь веб-сервера `apache2:apache2`.

**Основные шаги конфигурации:**

1.  **Установка пакетов:** Установлены `apache2`, `mariadb-server`, `php8.2` и требуемые PHP-расширения.
2.  **Настройка служб:** Запущены и включены в автозагрузку службы `httpd2` и `mysqld`. Выполнена базовая безопасная настройка `mariadb-server` (`mysql_secure_installation`).
3.  **Настройка БД:** Созданы база данных `moodledb` и пользователь `moodle` с необходимыми привилегиями.
4.  **Развёртывание Moodle:** Скачан и распакован Moodle, настроены права доступа. Создан каталог данных `/var/www/moodledata`.
5.  **Настройка PHP:** Увеличен параметр `max_input_vars` в `php.ini`.
6.  **Веб-установка:** Выполнена стандартная процедура установки Moodle через веб-интерфейс с использованием созданных параметров БД.
7.  **Настройка администратора и сайта:**
    *   Создан администратор Moodle с логином `admin` и паролем `P@ssw0rd`.
    *   Полное название сайта установлено в **`9`** (номер рабочего места).

**Ключевые параметры:**
*   **URL Moodle:** `http://192.168.1.10/`
*   **Каталог Moodle:** `/var/www/html`
*   **Каталог данных Moodle:** `/var/www/moodledata`
*   **База данных:** `moodledb` на `localhost`
*   **Пользователь БД:** `moodle`
*   **Пароль пользователя БД:** `P@ssw0rd`
*   **Администратор Moodle:** `admin`
*   **Пароль администратора Moodle:** `P@ssw0rd`
*   **Полное название сайта:** `9`

**Итог:**
Система управления обучением Moodle успешно развёрнута на сервере `HQ-SRV` и доступна по адресу `http://192.168.1.10/`. Базовая конфигурация выполнена, создан администратор и задано название сайта согласно требованиям. **`Студентам НЕОБХОДИМО составить подобный отчёт.`**

### Проверка шага 7: Запуск сервиса Moodle

**Цель:** Убедиться, что службы веб-сервера и СУБД работают, и сайт Moodle доступен с правильным названием.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Проверка статуса служб ---
systemctl is-active httpd2.service mysqld.service
# Ожидаемый результат: `active`.

# [root@hq-srv ~]# --- Проверка конфигурации Apache (опционально) ---
# Проверяем, что Apache слушает порт 80.
ss -tlpn | grep ':80.*httpd2'
# Ожидаемый результат: Строка с `httpd2` LISTEN на порту 80.
```

#### HQ-CLI (или другая машина с доступом)

```bash
# [user@hq-cli ~]$ --- Проверка доступности сайта Moodle ---
# Выполняем запрос к главной странице и ищем тег <title>.
curl -s http://192.168.1.10/ | grep '<title>'
# Ожидаемый результат: Строка вида `<title>9</title>` (Название сайта, заданное как номер рабочего места).

# [user@hq-cli ~]$ --- Визуальная проверка ---
# 1. Откройте http://192.168.1.10/ в веб-браузере.
# 2. Убедитесь, что загружается главная страница Moodle.
# 3. Убедитесь, что в заголовке вкладки браузера или на самой странице отображается "9" (или ваш номер рабочего места).
# 4. Попробуйте войти под пользователем 'admin' с паролем 'P@ssw0rd'.
# Ожидаемый результат: Успешный вход на сайт.
```

---

## Шаг 8: Настройте веб-сервер Nginx как обратный прокси-сервер на HQ-RTR

**Цель:** Настроить веб-сервер **Nginx** на маршрутизаторе `HQ-RTR` для работы в режиме **обратного прокси-сервера (Reverse Proxy)**. Это позволит перенаправлять HTTP-запросы, поступающие на `HQ-RTR` по определённым доменным именам, на соответствующие внутренние веб-сервисы (`Moodle` на `HQ-SRV` и `MediaWiki` на `BR-SRV`).

**Практическое назначение:** Nginx является высокопроизводительным веб-сервером, часто используемым в качестве обратного прокси для распределения нагрузки, терминирования SSL и маршрутизации запросов к внутренним приложениям. Использование имён хостов (`moodle.au-team.irpo`, `wiki.au-team.irpo`) позволяет обслуживать несколько сайтов на одном IP-адресе.

**Предпосылки:** 

Стенд Модуля 2 преднастроен согласно **Таблице 3**. DNS настроен так, что имена `moodle.au-team.irpo` и `wiki.au-team.irpo` разрешаются в IP-адрес `HQ-RTR` (`172.16.4.4`). Сервисы Moodle (`HQ-SRV`, `192.168.1.10`, порт 80) и MediaWiki (`BR-SRV`, `192.168.3.10`, порт 8080) запущены и доступны из сети `HQ-RTR`. 

**Местоположение Reverse Proxy (здесь HQ-RTR) может отличаться в задании (например, ISP).** Следуйте указаниям вашего варианта, хотя размещение на ISP для внутренних сервисов нелогично.

### Шаг 8.1: Установка Nginx на HQ-RTR

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Установка Nginx ---
# Обновляем списки пакетов и устанавливаем Nginx.
apt-get update && apt-get install -y nginx
# [Проверка (опционально): пакет]
# rpm -q nginx
```

      
### Шаг 8.2: Настройка виртуальных хостов Nginx на HQ-RTR

Создадим отдельные файлы конфигурации для каждого проксируемого сайта (`moodle` и `wiki`) в стандартной директории `/etc/nginx/sites-available.d/` и активируем их, создав символические ссылки в `/etc/nginx/sites-enabled.d/`.

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Создание конфигурации для moodle.au-team.irpo ---
# Создаём файл /etc/nginx/sites-available.d/moodle.conf.
cat <<'EOF' > /etc/nginx/sites-available.d/moodle.conf
server {
    # Слушать стандартный порт HTTP
    listen 80;
    # Имя хоста, на которое реагирует этот блок server
    server_name moodle.au-team.irpo;

    location / {
        # Проксировать все запросы на сервер Moodle (HQ-SRV)
        proxy_pass http://192.168.1.10:80;

        # Устанавливаем заголовки для корректной работы Moodle за прокси
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
EOF
# [Проверка (опционально): конфиг Moodle]
# cat /etc/nginx/sites-available.d/moodle.conf

# [root@hq-rtr ~]# --- Создание конфигурации для wiki.au-team.irpo ---
# Создаём файл /etc/nginx/sites-available.d/wiki.conf.
cat <<'EOF' > /etc/nginx/sites-available.d/wiki.conf
server {
    # Слушать стандартный порт HTTP
    listen 80;
    # Имя хоста, на которое реагирует этот блок server
    server_name wiki.au-team.irpo;

    location / {
        # Проксировать все запросы на сервер MediaWiki (BR-SRV, порт 8080)
        proxy_pass http://192.168.3.10:8080;

        # Устанавливаем заголовки для корректной работы MediaWiki за прокси
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
EOF
# [Проверка (опционально): конфиг Wiki]
# cat /etc/nginx/sites-available.d/wiki.conf

# [root@hq-rtr ~]# --- Активация конфигураций (создание ссылок) ---
# Создаём символические ссылки в /etc/nginx/sites-enabled.d/, чтобы Nginx загрузил эти конфиги.
# Опция -f перезаписывает ссылку, если она уже существует.
ln -sf /etc/nginx/sites-available.d/moodle.conf /etc/nginx/sites-enabled.d/moodle.conf
ln -sf /etc/nginx/sites-available.d/wiki.conf /etc/nginx/sites-enabled.d/wiki.conf
# [Проверка (опционально): ссылки]
# ls -l /etc/nginx/sites-enabled.d/
```

### Шаг 8.3: Запуск и проверка конфигурации Nginx

#### HQ-RTR

```bash
# [root@hq-rtr ~]# --- Проверка синтаксиса конфигурации Nginx ---
# Эта команда проверит все конфигурационные файлы на наличие ошибок.
nginx -t
# Ожидаемый результат:
# nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
# nginx: configuration file /etc/nginx/nginx.conf test is successful

# [root@hq-rtr ~]# --- Включение автозапуска и перезапуск Nginx ---
# Включаем службу для старта при загрузке и перезапускаем для применения новой конфигурации.
systemctl enable --now nginx
# Используем restart, чтобы гарантированно применить изменения.
systemctl restart nginx
# [Проверка (опционально): статус службы]
# systemctl --no-pager -l status nginx
```

---

### Шаг 8.4: Корректировка конфигурации Moodle для работы через прокси

**Цель:** Исправить конфигурацию Moodle на `HQ-SRV`, чтобы он генерировал ссылки, используя публичное доменное имя (`moodle.au-team.irpo`), а не свой локальный IP-адрес, при доступе через обратный прокси-сервер Nginx.

**Рациональность:** Moodle использует параметр `$CFG->wwwroot` в своём конфигурационном файле `config.php` для построения всех внутренних URL. Если этот параметр содержит локальный IP, то при переходе по ссылкам внутри Moodle пользователя будет перебрасывать на этот IP, минуя прокси, что неверно. Необходимо указать публичный URL.

**Предпосылки:** Moodle установлен на `HQ-SRV` в каталог `/var/www/html/`. Nginx на `HQ-RTR` настроен как обратный прокси для `moodle.au-team.irpo`.

#### HQ-SRV

```bash
# [root@hq-srv ~]# --- Редактирование файла config.php Moodle ---
# Определяем путь к файлу конфигурации Moodle.
MOODLE_CONFIG="/var/www/html/config.php"
# Определяем правильный публичный URL.
PUBLIC_WWWROOT="http://moodle.au-team.irpo"

# [Проверка (опционально): Текущее значение wwwroot]
# grep '\$CFG->wwwroot' "$MOODLE_CONFIG"

# Используем sed для поиска строки, начинающейся с $CFG->wwwroot, и замены её значения
# на правильный публичный URL. Используем # в качестве разделителя из-за слешей в URL.
# Команда ищет строку вида $CFG->wwwroot = 'http://...'; и заменяет её.
sed -i "s#^\$CFG->wwwroot\s*=\s*'.*';#\$CFG->wwwroot = '$PUBLIC_WWWROOT';#" "$MOODLE_CONFIG"

# [Проверка: Новое значение wwwroot]
grep '\$CFG->wwwroot' "$MOODLE_CONFIG"
# Ожидаемый результат: Строка вида $CFG->wwwroot = 'http://moodle.au-team.irpo';

# Перезапуск веб-сервера не требуется, Moodle читает config.php при каждом запросе.
```

---

### Пример отчёта: Настройка Nginx как обратного прокси-сервера на HQ-RTR

**Цель:** Настроить веб-сервер Nginx на маршрутизаторе `HQ-RTR` для работы в качестве обратного прокси, обеспечивающего доступ к внутренним веб-сервисам (Moodle и MediaWiki) по их доменным именам через IP-адрес `HQ-RTR`.

**Выбор реализации:**
*   **Прокси-сервер:** `Nginx`, установленный на `HQ-RTR`. *Примечание: В других вариантах задания местоположение прокси-сервера может отличаться.*
*   **Метод проксирования:** Использование виртуальных хостов (блоков `server` в Nginx), реагирующих на определённые `server_name` (`moodle.au-team.irpo` и `wiki.au-team.irpo`) и проксирующих запросы (`proxy_pass`) на соответствующие бэкенд-серверы.
*   **Бэкенды:**
    *   Moodle: `http://192.168.1.10:80` (`HQ-SRV`)
    *   MediaWiki: `http://192.168.3.10:8080` (`BR-SRV`)
*   **Заголовки:** Для корректной работы приложений за прокси установлены заголовки `Host`, `X-Real-IP`, `X-Forwarded-For`, `X-Forwarded-Proto`.
*   **Конфигурация Moodle:** Файл `config.php` на `HQ-SRV` был скорректирован для использования публичного URL (`$CFG->wwwroot = 'http://moodle.au-team.irpo';`).

**Основные шаги конфигурации:**

1.  **Установка Nginx:** Пакет `nginx` установлен на `HQ-RTR`.
2.  **Настройка виртуальных хостов:**
    *   Созданы файлы конфигурации `/etc/nginx/sites-available/moodle.conf` и `/etc/nginx/sites-available/wiki.conf` с директивами `listen 80`, `server_name` и `location / { proxy_pass ...; proxy_set_header ...; }`.
    *   Созданы символические ссылки на эти файлы в `/etc/nginx/sites-enabled/`. Стандартный сайт отключён.
3.  **Запуск и проверка Nginx:** Конфигурация проверена (`nginx -t`), служба `nginx` включена в автозагрузку и перезапущена.
4.  **Корректировка Moodle:** Изменён параметр `$CFG->wwwroot` в `/var/www/html/config.php` на `HQ-SRV`.

**Итог:** Nginx на `HQ-RTR` успешно настроен как обратный прокси-сервер. Запросы, поступающие на `HQ-RTR` по именам `moodle.au-team.irpo` и `wiki.au-team.irpo`, корректно перенаправляются на внутренние серверы Moodle и MediaWiki соответственно. Приложения работают корректно через прокси, включая генерацию внутренних ссылок.

### Проверка шага 8: Настройка веб-сервера Nginx как обратного прокси и корректность ссылок приложений

**Цель:** Убедиться, что Nginx на `HQ-RTR` работает, конфигурация корректна, запросы к `moodle.au-team.irpo` и `wiki.au-team.irpo` успешно проксируются на соответствующие внутренние серверы, и что Moodle после корректировки конфигурации генерирует правильные внутренние URL.

#### HQ-RTR (Прокси-сервер)

```bash
# [root@hq-rtr ~]# --- Проверка статуса службы Nginx ---
systemctl is-active nginx
# Ожидаемый результат: `active`.

# [root@hq-rtr ~]# --- Повторная проверка синтаксиса ---
nginx -t
# Ожидаемый результат: `... syntax is ok`, `... test is successful`.

# [root@hq-rtr ~]# --- Проверка прослушивания порта 80 ---
ss -tlpn | grep ':80 .*nginx'
# Ожидаемый результат: Строка, показывающая, что `nginx` слушает порт 80.
```

#### HQ-SRV (Бэкенд Moodle)

```bash
# [root@hq-srv ~]# --- Проверка файла конфигурации Moodle (wwwroot) ---
# Убедимся, что Moodle настроен на использование публичного URL.
grep '\$CFG->wwwroot' /var/www/html/config.php
# Ожидаемый результат: $CFG->wwwroot = 'http://moodle.au-team.irpo';
```

#### HQ-CLI (Клиент для тестирования)

```bash
# [root@hq-cli ~]# --- Проверка доступности Moodle через Nginx (заголовки) ---
# Отправляем запрос к moodle.au-team.irpo (который разрешается в IP HQ-RTR).
curl -I http://moodle.au-team.irpo
# Ожидаемый результат: HTTP/1.1 200 OK (или редирект 30x), Server: nginx/...

# [root@hq-cli ~]# --- Проверка доступности Wiki через Nginx (заголовки) ---
# Отправляем запрос к wiki.au-team.irpo.
curl -I http://wiki.au-team.irpo
# Ожидаемый результат: HTTP/1.1 301 Moved Permanently (или 200 OK), Server: nginx/...
```

**Визуальная и функциональная проверка в браузере (выполнять на `HQ-CLI`):**

1.  **Откройте веб-браузер**.
2.  **(Рекомендуется)** Очистите кэш браузера для доменов `moodle.au-team.irpo` и `wiki.au-team.irpo`, чтобы избежать загрузки старых версий страниц.
3.  **Перейдите по адресу:** `http://wiki.au-team.irpo`
    *   **Ожидаемый результат:** Должна отобразиться главная страница **MediaWiki**. Проверьте, что внутренние ссылки (например, на случайные статьи, историю правок) работают корректно и не ведут на IP-адрес внутреннего сервера.
4.  **Перейдите по адресу:** `http://moodle.au-team.irpo`
    *   **Ожидаемый результат:** Должна отобразиться главная страница **Moodle**.
5.  **Войдите в Moodle** (например, под пользователем `admin` с паролем `P@ssw0rd`).
6.  **Проверьте навигацию:** Попробуйте перейти по различным разделам и ссылкам внутри Moodle (например, "Администрирование", "Мои курсы", настройки профиля и т.д.).
7.  **Проверка URL ссылок:** Наведите курсор на любую внутреннюю ссылку на странице Moodle.
    *   **Ожидаемый результат:** Всплывающая подсказка браузера (или строка состояния) должна показывать, что ссылка начинается с `http://moodle.au-team.irpo/...`. При осуществлении переходов по этим ссылкам, адресная строка браузера **не должна** изменяться на внутренний IP-адрес сервера Moodle (например, `http://192.168.1.10/...`). Moodle должен быть полностью функционален при работе через обратный прокси-сервер.

---

## Шаг 9: Установка Яндекс Браузера для организаций на HQ-CLI

### **Введение:**

**Цель:** Установить на клиентскую машину **HQ-CLI** веб-браузер **Яндекс Браузер**, предназначенный для использования в организациях. Это обеспечит пользователей стандартным и управляемым инструментом для доступа к веб-ресурсам. Установка будет производиться с использованием пакетного менеджера `epm`, рекомендованного для ALT Linux.

**Примечание:** Команда установки этого шага рекомендуется к запуску в **самом начале** выполнения Модуля 2 (см. Рекомендуемый порядок) для экономии времени, пока пакет скачивается. Проверка выполняется после завершения установки.

**`Студентам НЕОБХОДИМО составить отчёт по этому шагу.`**

---

### Установка пакета

#### **HQ-CLI**

**Цель:** Обновить списки пакетов и установить пакет `yandex-browser-stable` с помощью `epm`.

```bash
# Обновляем списки пакетов в репозиториях.
# epm (Etersoft Package Manager) - это рекомендуемый инструмент для управления пакетами в ALT Linux.
epm update

# Устанавливаем Яндекс Браузер.
# Пакет yandex-browser-stable предоставляет актуальную стабильную версию,
# которая включает функционал "для организаций".
# Флаг -y используется для неинтерактивной установки.
# Если запускаете в фоне в начале модуля, добавьте '&' в конец:
# epm -y install yandex-browser-stable &
epm -y install yandex-browser-stable

# После установки пакет будет доступен в системе.
```

---

### Проверка шага 9:

**Цель:** Убедиться, что **Яндекс Браузер для организаций** успешно установлен и доступен на `HQ-CLI`.

#### **HQ-CLI**

1.  **Проверка наличия пакета в системе:**

```bash
# Проверяем, что пакет yandex-browser-stable установлен в системе
# Используем стандартную утилиту rpm для запроса информации о пакете
rpm -q yandex-browser-stable
# Ожидаемый результат:
# Вывод должен показать имя пакета и его установленную версию, например:
# yandex-browser-stable-24.7.6.1010-alt1.x86_64
# (версия может отличаться)
```

2.  **Проверка доступности в графическом интерфейсе и версии "для организаций":**
    *   **Действие:** Запустите браузер через графическое меню рабочего стола.
    *   **Путь:** `Пуск` -> `Интернет` -> `Яндекс Браузер`. (Путь может немного отличаться в зависимости от используемой графической оболочки).
    *   **Действие:** Дождитесь запуска браузера.
    *   **Действие:** В открывшемся браузере нажмите на значок меню (три горизонтальные полоски) в правом верхнем углу.
    *   **Проверка:** Убедитесь, что внизу открывшегося меню присутствует надпись **"Яндекс.Браузер для организаций"**, как показано на скриншоте в задании. Это подтверждает, что установлена корректная версия.

---

### Пример отчёта: Установка Яндекс Браузера для организаций на HQ-CLI

**(Составляется после проверки)**

**Цель:** Установка приложения **Яндекс Браузер для организаций** на ВМ **HQ-CLI**.

**Выполненные действия:**
*   Подключился к ВМ **HQ-CLI** с использованием учётной записи `user` и переключился на суперпользователя `root`.
*   Обновил списки пакетов с помощью команды `epm update`.
*   Запустил установку пакета `yandex-browser-stable` командой `epm -y install yandex-browser-stable &` в фоновом режиме в начале выполнения модуля (или `epm -y install yandex-browser-stable` при выполнении по порядку).
*   После завершения установки проверил её успешность командой `rpm -q yandex-browser-stable`, которая подтвердила наличие пакета в системе.
*   Проверил наличие браузера в меню приложений (`Пуск` -> `Интернет` -> `Яндекс Браузер`).
*   Запустил браузер и в меню настроек убедился в наличии отметки **"Яндекс.Браузер для организаций"**.

**Итог:** **Яндекс Браузер для организаций** успешно установлен на ВМ **HQ-CLI**.

---

**Завершение модуля 2:** Настройка служб операционных систем и сетевого администрирования завершена. Основные сервисы (Samba AD, NFS, NTP, Ansible, Docker/MediaWiki, Moodle, Nginx Proxy) развёрнуты и сконфигурированы, а также установлен **Яндекс Браузер** согласно заданию. **Не забудьте подготовить отчёты по шагам 2, 7 и 9.** (Отчёты по шагам 1, 4 и 8 также предоставлены в пособии, но могут не входить в обязательный список).

---

# Итоги и возможные вариации

Данное пособие представляет собой решение одного из возможных вариантов задания демонстрационного экзамена. Важно помнить, что на реальном экзамене вам может достаться **другой вариант**, в котором некоторые параметры будут отличаться.

**Основные параметры, которые могут быть вариативны:**

*   **VLAN ID:**
    *   **Для сетей HQ-SRV и HQ-CLI:** Идентификаторы VLAN **фиксированы** как `100` и `200` соответственно, из-за особенностей преднастроенного стенда (см. "Критическое исключение" в начале Модуля 1). **Не меняйте их, даже если в вашем варианте задания указаны другие значения для этих двух сетей.**
    *   **Для сети Управления:** Идентификатор VLAN (в пособии пример `999`) **является вариативным** и должен быть взят из вашего варианта задания.*   **Размеры подсетей:** Маски для сетей HQ-SRV, HQ-CLI, Управления, BR-SRV (например, /27 вместо /26). Внимательно рассчитывайте и используйте IP-адреса и маски из вашего задания.
*   **UID пользователей:** Идентификаторы пользователей `sshuser` и `net_admin` (например, 1015 вместо 1010).
*   **Порты служб:**
    *   Порт SSH для удалённого доступа (`sshd`) на серверах (например, 3015 вместо 2024).
    *   Порт для статической трансляции (DNAT) на маршрутизаторах для доступа к SSH (например, 3015 вместо 2024).
*   **Конфигурация DNS:** Типы записей (A/PTR/CNAME) и цели (особенно для `moodle` и `wiki`) могут отличаться.
*   **Конфигурация RAID:** Уровень RAID массива на HQ-SRV (например, RAID 0 вместо RAID 5). Соответственно изменятся пути монтирования (`/raid0` вместо `/raid5`) и путь к NFS-ресурсу (`/raid0/nfs` вместо `/raid5/nfs`).
*   **Местоположение Reverse Proxy:** Сервер для Nginx может быть указан другой (например, ISP вместо HQ-RTR).
*   **DHCP Client ID:** Идентификатор клиента, используемый для статической аренды IP для HQ-CLI, может отличаться от `hq-cli-exam-id`.

**Ваша главная задача на экзамене – внимательно прочитать ваш вариант задания и следовать указанным в нём параметрам, за исключением VLAN ID.**